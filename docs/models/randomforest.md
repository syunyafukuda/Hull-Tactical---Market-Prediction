# RandomForest ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ä»•æ§˜æ›¸

æœ€çµ‚æ›´æ–°: 2025-12-12

## å®Ÿè£…ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹

**Status**: â¬œ **æœªç€æ‰‹**

### å®Ÿè£…äºˆå®š
- â¬œ `src/models/randomforest/train_randomforest.py`: å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- â¬œ `configs/models/randomforest.yaml`: YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
- â¬œ Unit tests: `tests/models/test_randomforest.py`

### æˆæœç‰©
- â¬œ `artifacts/models/randomforest/inference_bundle.pkl`
- â¬œ `artifacts/models/randomforest/oof_predictions.csv`
- â¬œ `artifacts/models/randomforest/cv_fold_logs.csv`
- â¬œ `artifacts/models/randomforest/model_meta.json`
- â¬œ `artifacts/models/randomforest/feature_importances.csv` ï¼ˆç‰¹å¾´é‡é‡è¦åº¦ï¼‰
- â¬œ `artifacts/models/randomforest/submission.csv`

---

## 1. ç›®çš„ã¨ä½ç½®ã¥ã‘

### 1.1 ãƒ¢ãƒ‡ãƒ«é¸å®šãƒ•ã‚§ãƒ¼ã‚ºã§ã®å½¹å‰²

- **ç›®çš„**: ãƒã‚®ãƒ³ã‚°ç³»ãƒ„ãƒªãƒ¼ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€GBDTã¨ã¯ç•°ãªã‚‹å¤šæ§˜æ€§ã‚’å°å…¥
- **æœŸå¾…åŠ¹æœ**: 
  - ExtraTreesã¨åŒæ§˜ã€GBDTã¨ã¯ã€Œç™ºæƒ³ãŒé•ã†ã€ãƒ¢ãƒ‡ãƒ«
  - LGBMã¨ã®äºˆæ¸¬ç›¸é–¢ãŒ0.85-0.92ç¨‹åº¦ï¼ˆé«˜ã„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¾¡å€¤ï¼‰
  - å®‰å®šã—ãŸäºˆæ¸¬ï¼ˆãƒã‚®ãƒ³ã‚°åŠ¹æœï¼‰
- **æ¯”è¼ƒå¯¾è±¡**: LGBM ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆOOF RMSE: 0.012164, LB: 0.681ï¼‰

### 1.2 RandomForestã®ç‰¹å¾´

- **ãƒã‚®ãƒ³ã‚° + ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ–ã‚¹ãƒšãƒ¼ã‚¹**: å„æœ¨ãŒã‚µãƒ³ãƒ—ãƒ«ãƒ»ç‰¹å¾´é‡ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§å­¦ç¿’
- **æœ€é©åˆ†å‰²ç‚¹æ¢ç´¢**: ExtraTreesã¨ç•°ãªã‚Šã€åˆ†å‰²ç‚¹ã¯æœ€é©å€¤ã‚’æ¢ç´¢
- **OOB (Out-of-Bag) è©•ä¾¡**: ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã§ä½¿ã‚ãªã‹ã£ãŸã‚µãƒ³ãƒ—ãƒ«ã§è©•ä¾¡å¯èƒ½
- **ExtraTreesã¨ã®é•ã„**: ã‚ˆã‚Šã€ŒçœŸé¢ç›®ãªã€åˆ†å‰²ã‚’è¡Œã†

### 1.3 å‰ææ¡ä»¶

- **ç‰¹å¾´ã‚»ãƒƒãƒˆ**: FS_compactï¼ˆ116åˆ—ï¼‰ã‚’å›ºå®šï¼ˆFeature Selection Phase ã§ã®çµè«–ã¨æ•´åˆï¼‰
- **CVè¨­å®š**: TimeSeriesSplit, n_splits=5, gap=0ï¼ˆLGBMã¨åŒä¸€ï¼‰
- **è©•ä¾¡æŒ‡æ¨™**:
  - **ä¸»æŒ‡æ¨™**: OOF RMSEï¼ˆé¸å®šãƒ•ã‚§ãƒ¼ã‚ºã®æœ€é‡è¦æŒ‡æ¨™ï¼‰
  - **è£œåŠ©æŒ‡æ¨™**: äºˆæ¸¬ç›¸é–¢ï¼ˆvs LGBMï¼‰ã€OOF MSRï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‰è¦³ç‚¹ã§ã®ç›£è¦–ï¼‰
- **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: ä¸è¦ï¼ˆãƒ„ãƒªãƒ¼ç³»ãƒ¢ãƒ‡ãƒ«ï¼‰

---

## 2. æŠ€è¡“ä»•æ§˜

### 2.1 å…¥å‡ºåŠ›

| é …ç›® | ä»•æ§˜ |
|------|------|
| å…¥åŠ› | `data/raw/train.csv`, `data/raw/test.csv` |
| ç‰¹å¾´é‡ç”Ÿæˆ | SU1 + SU5 â†’ tier3é™¤å¤– â†’ 116åˆ— |
| å‡ºåŠ› | `artifacts/models/randomforest/` é…ä¸‹ã«æˆæœç‰© |

### 2.2 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹é€ 

```
ç”Ÿãƒ‡ãƒ¼ã‚¿ (94åˆ—)
    â†“
[SU1FeatureAugmenter + SU5FeatureAugmenter]  # æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†åˆ©ç”¨
    â†“
åˆè¨ˆ 577åˆ—
    â†“
[tier3 feature exclusion]  # configs/feature_selection/tier3/excluded.json
    â†“
116åˆ— (FS_compact)
    â†“
[GroupImputers: M/E/I/P/S]  # æ—¢å­˜å‰å‡¦ç†å†åˆ©ç”¨
    â†“
[SimpleImputer]  # æ®‹ä½™NaNå‡¦ç†
    â†“
[RandomForestRegressor]  # â˜… ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸è¦
```

### 2.3 åˆæœŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```python
randomforest_params = {
    "n_estimators": 500,         # æœ¨ã®æ•°
    "max_depth": 15,             # æœ¨ã®æ·±ã•åˆ¶é™
    "min_samples_split": 10,     # åˆ†å‰²ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
    "min_samples_leaf": 5,       # è‘‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
    "max_features": 0.7,         # å„åˆ†å‰²ã§ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡å‰²åˆ
    "bootstrap": True,           # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    "oob_score": True,           # OOBã‚¹ã‚³ã‚¢è¨ˆç®—
    "random_state": 42,
    "n_jobs": -1,
}
```

### 2.4 ExtraTreesã¨ã®æ¯”è¼ƒ

| é …ç›® | RandomForest | ExtraTrees |
|------|--------------|------------|
| åˆ†å‰²ç‚¹é¸æŠ | æœ€é©ç‚¹ã‚’æ¢ç´¢ | ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ |
| ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ— | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆTrue | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆFalse |
| è¨ˆç®—ã‚³ã‚¹ãƒˆ | ã‚„ã‚„é«˜ã„ | ä½ã„ |
| ãƒãƒªã‚¢ãƒ³ã‚¹ | ã‚„ã‚„ä½ã„ | é«˜ã„ |
| ãƒã‚¤ã‚¢ã‚¹ | ã‚„ã‚„é«˜ã„ | ä½ã„ |

> **ğŸ“Œ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¦³ç‚¹ã§ã®ä½¿ã„åˆ†ã‘**
>
> - **ExtraTreeså„ªå…ˆ**: ã¾ãšExtraTreesã‚’å®Ÿè£…ãƒ»è©•ä¾¡
> - **RandomForest**: ExtraTreesã®çµæœã‚’è¦‹ã¦ã‹ã‚‰æ¡å¦ã‚’æ±ºå®š
> - **ä¸¡è€…ã®ç›¸é–¢ãŒé«˜ã„å ´åˆ**ï¼ˆ> 0.95ï¼‰: ã‚ˆã‚Šè‰¯ã„RMSEã‚’ç¤ºã™æ–¹ã®ã¿ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«æ¡ç”¨

---

## 3. å®Ÿè£…è©³ç´°

### 3.1 ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
src/models/randomforest/
â”œâ”€â”€ __init__.py              # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆæœŸåŒ–
â””â”€â”€ train_randomforest.py    # ãƒ¡ã‚¤ãƒ³å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

configs/models/
â””â”€â”€ randomforest.yaml        # YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

tests/models/
â””â”€â”€ test_randomforest.py     # ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
```

### 3.2 train_randomforest.py ã®å®Ÿè£…è¦ä»¶

#### 3.2.1 å¿…é ˆæ©Ÿèƒ½

1. **å¼•æ•°ãƒ‘ãƒ¼ã‚¹**: `argparse`ã§ä»¥ä¸‹ã‚’å—ã‘ä»˜ã‘ã‚‹
   - `--data-dir`: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `data/raw`ï¼‰
   - `--out-dir`: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `artifacts/models/randomforest`ï¼‰
   - `--config-path`: feature_generation.yaml ãƒ‘ã‚¹
   - `--preprocess-config`: preprocess.yaml ãƒ‘ã‚¹
   - `--feature-tier`: ä½¿ç”¨ã™ã‚‹tierï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `tier3`ï¼‰
   - `--n-splits`, `--gap`: CVè¨­å®š
   - RandomForestãƒã‚¤ãƒ‘ãƒ©: `--n-estimators`, `--max-depth`, `--min-samples-leaf`, `--max-features`

2. **ç‰¹å¾´é‡ç”Ÿæˆ**: æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†åˆ©ç”¨
3. **å‰å‡¦ç†**: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸è¦
4. **OOBã‚¹ã‚³ã‚¢**: å„foldã®OOBã‚¹ã‚³ã‚¢ã‚‚è¨˜éŒ²

### 3.3 ã‚³ãƒ¼ãƒ‰ã‚¹ã‚±ãƒ«ãƒˆãƒ³

```python
"""RandomForest training script with unified CV framework."""

from __future__ import annotations

import argparse
import json
import pickle
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.model_selection import TimeSeriesSplit
from sklearn.pipeline import Pipeline

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.feature_generation.su5.train_su5 import (
    SU5FeatureAugmenter,
    _prepare_features,
    load_preprocess_policies,
    load_su1_config,
    load_su5_config,
)
from src.models.common.cv_utils import (
    CVConfig,
    aggregate_fold_results,
    compute_fold_metrics,
    create_cv_splits,
)
from src.models.common.feature_loader import get_excluded_features


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train RandomForest model")
    parser.add_argument("--data-dir", type=str, default="data/raw")
    parser.add_argument("--out-dir", type=str, default="artifacts/models/randomforest")
    parser.add_argument(
        "--config-path", type=str, default="configs/feature_generation.yaml"
    )
    parser.add_argument("--preprocess-config", type=str, default="configs/preprocess.yaml")
    parser.add_argument("--feature-tier", type=str, default="tier3")
    parser.add_argument("--n-splits", type=int, default=5)
    parser.add_argument("--gap", type=int, default=0)
    # RandomForest specific
    parser.add_argument("--n-estimators", type=int, default=500)
    parser.add_argument("--max-depth", type=int, default=15)
    parser.add_argument("--min-samples-leaf", type=int, default=5)
    parser.add_argument("--max-features", type=float, default=0.7)
    return parser.parse_args()


def build_randomforest_pipeline(
    n_estimators: int = 500,
    max_depth: int = 15,
    min_samples_leaf: int = 5,
    max_features: float = 0.7,
) -> Pipeline:
    """Build RandomForest pipeline (no scaling needed)."""
    return Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("model", RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=10,
            min_samples_leaf=min_samples_leaf,
            max_features=max_features,
            bootstrap=True,
            oob_score=True,
            random_state=42,
            n_jobs=-1,
        )),
    ])


def train_fold(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_val: pd.DataFrame,
    pipeline_params: dict[str, Any],
) -> tuple[Pipeline, np.ndarray, float | None]:
    """Train a single fold, return pipeline, predictions, and OOB score."""
    pipeline = build_randomforest_pipeline(**pipeline_params)
    pipeline.fit(X_train, y_train)
    val_pred = pipeline.predict(X_val)
    
    # Get OOB score if available
    oob_score = None
    if hasattr(pipeline.named_steps["model"], "oob_score_"):
        oob_score = pipeline.named_steps["model"].oob_score_
    
    return pipeline, val_pred, oob_score


def main() -> None:
    args = parse_args()
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»ç‰¹å¾´é‡ç”Ÿæˆï¼ˆæ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    train_df = pd.read_csv(Path(args.data_dir) / "train.csv")
    test_df = pd.read_csv(Path(args.data_dir) / "test.csv")

    # SU1 + SU5 ç‰¹å¾´é‡ç”Ÿæˆ
    su1_cfg = load_su1_config(args.config_path)
    su5_cfg = load_su5_config(args.config_path)
    preprocess_policies = load_preprocess_policies(args.preprocess_config)
    augmenter = SU5FeatureAugmenter(su1_cfg, su5_cfg, preprocess_policies)

    train_aug = augmenter.fit_transform(train_df.copy())
    test_aug = augmenter.transform(test_df.copy())

    # tieré™¤å¤–
    excluded = get_excluded_features(args.feature_tier)
    feature_cols = [c for c in train_aug.columns if c not in excluded and c not in ["Date", "TARGET"]]
    
    X = train_aug[feature_cols]
    y = train_aug["TARGET"]
    X_test = test_aug[feature_cols]

    # CVè¨­å®š
    cv_config = CVConfig(n_splits=args.n_splits, gap=args.gap)
    splits = create_cv_splits(X, cv_config)

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    pipeline_params = {
        "n_estimators": args.n_estimators,
        "max_depth": args.max_depth,
        "min_samples_leaf": args.min_samples_leaf,
        "max_features": args.max_features,
    }

    # CVå­¦ç¿’
    fold_results = []
    oob_scores = []
    oof_preds = np.zeros(len(X))
    models = []

    for fold_idx, (train_idx, val_idx) in enumerate(splits):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        pipeline, val_pred, oob_score = train_fold(X_train, y_train, X_val, pipeline_params)
        oof_preds[val_idx] = val_pred
        models.append(pipeline)
        oob_scores.append(oob_score)

        metrics = compute_fold_metrics(y_val.values, val_pred, fold_idx)
        fold_results.append(metrics)
        oob_str = f", OOB RÂ²={oob_score:.4f}" if oob_score else ""
        print(f"Fold {fold_idx}: RMSE={metrics.rmse:.6f}{oob_str}")

    # é›†è¨ˆ
    summary = aggregate_fold_results(fold_results)
    print(f"\nOOF RMSE: {summary['oof_rmse']:.6f}")
    print(f"OOF MSR: {summary['oof_msr']:.6f}")
    if oob_scores[0] is not None:
        print(f"Mean OOB RÂ²: {np.mean([s for s in oob_scores if s]):.4f}")

    # æˆæœç‰©ä¿å­˜
    # 1. inference_bundle.pkl
    test_preds = np.mean([m.predict(X_test) for m in models], axis=0)
    bundle = {
        "models": models,
        "augmenter": augmenter,
        "feature_cols": feature_cols,
        "excluded_features": excluded,
    }
    with open(out_dir / "inference_bundle.pkl", "wb") as f:
        pickle.dump(bundle, f)

    # 2. oof_predictions.csv
    oof_df = pd.DataFrame({
        "Date": train_aug["Date"],
        "TARGET": y,
        "prediction": oof_preds,
    })
    oof_df.to_csv(out_dir / "oof_predictions.csv", index=False)

    # 3. cv_fold_logs.csv
    fold_logs = pd.DataFrame([
        {
            "fold": r.fold_idx,
            "rmse": r.rmse,
            "msr": r.msr,
            "n_samples": r.n_samples,
            "oob_score": oob_scores[i],
        }
        for i, r in enumerate(fold_results)
    ])
    fold_logs.to_csv(out_dir / "cv_fold_logs.csv", index=False)

    # 4. model_meta.json
    meta = {
        "model_type": "RandomForestRegressor",
        "feature_tier": args.feature_tier,
        "n_features": len(feature_cols),
        "cv_config": {"n_splits": args.n_splits, "gap": args.gap},
        "hyperparameters": pipeline_params,
        "oof_rmse": summary["oof_rmse"],
        "oof_msr": summary["oof_msr"],
        "mean_oob_r2": float(np.mean([s for s in oob_scores if s])) if oob_scores[0] else None,
    }
    with open(out_dir / "model_meta.json", "w") as f:
        json.dump(meta, f, indent=2)

    # 5. submission.csv
    submission = pd.DataFrame({
        "Date": test_aug["Date"],
        "TARGET": test_preds,
    })
    submission.to_csv(out_dir / "submission.csv", index=False)

    print(f"\nArtifacts saved to {out_dir}")


if __name__ == "__main__":
    main()
```

---

## 4. ãƒ†ã‚¹ãƒˆè¦ä»¶

### 4.1 ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ

```python
# tests/models/test_randomforest.py

import numpy as np
import pandas as pd
import pytest
from sklearn.ensemble import RandomForestRegressor


class TestRandomForestPipeline:
    """Test RandomForest pipeline components."""

    def test_randomforest_basic_fit(self, sample_train_data: pd.DataFrame) -> None:
        """RandomForestRegressor can fit and predict."""
        X = sample_train_data.drop(columns=["Date", "TARGET"])
        y = sample_train_data["TARGET"]

        model = RandomForestRegressor(n_estimators=10, random_state=42, n_jobs=-1)
        model.fit(X, y)
        preds = model.predict(X)

        assert len(preds) == len(y)
        assert not np.isnan(preds).any()

    def test_randomforest_oob_score(self, sample_train_data: pd.DataFrame) -> None:
        """RandomForest provides OOB score."""
        X = sample_train_data.drop(columns=["Date", "TARGET"])
        y = sample_train_data["TARGET"]

        model = RandomForestRegressor(
            n_estimators=50,
            bootstrap=True,
            oob_score=True,
            random_state=42,
        )
        model.fit(X, y)

        assert hasattr(model, "oob_score_")
        assert -1 <= model.oob_score_ <= 1  # RÂ² score

    def test_feature_importance_available(self, sample_train_data: pd.DataFrame) -> None:
        """RandomForest provides feature importances."""
        X = sample_train_data.drop(columns=["Date", "TARGET"])
        y = sample_train_data["TARGET"]

        model = RandomForestRegressor(n_estimators=10, random_state=42)
        model.fit(X, y)

        importances = model.feature_importances_
        assert len(importances) == X.shape[1]
        assert np.isclose(importances.sum(), 1.0)

    def test_randomforest_vs_extratrees(self, sample_train_data: pd.DataFrame) -> None:
        """RandomForest and ExtraTrees give different predictions."""
        from sklearn.ensemble import ExtraTreesRegressor

        X = sample_train_data.drop(columns=["Date", "TARGET"])
        y = sample_train_data["TARGET"]

        rf = RandomForestRegressor(n_estimators=50, random_state=42)
        et = ExtraTreesRegressor(n_estimators=50, random_state=42)

        rf.fit(X, y)
        et.fit(X, y)

        rf_pred = rf.predict(X)
        et_pred = et.predict(X)

        # Predictions should be different (not identical)
        assert not np.allclose(rf_pred, et_pred)
```

---

## 5. æˆåŠŸåŸºæº–

### 5.1 å®šé‡åŸºæº–

| å„ªå…ˆåº¦ | æŒ‡æ¨™ | é–¾å€¤ | ç†ç”± |
|--------|------|------|------|
| **ä¸»æŒ‡æ¨™** | OOF RMSE | â‰¤ 0.0130 | LGBMï¼ˆ0.01216ï¼‰ã‚ˆã‚Šå¤šå°‘åŠ£ã£ã¦ã‚‚è¨±å®¹ |
| è£œåŠ© | äºˆæ¸¬ç›¸é–¢ï¼ˆvs LGBMï¼‰ | < 0.92 | ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¤šæ§˜æ€§ã®ç¢ºä¿ |
| è£œåŠ© | OOF MSR | > 0ï¼ˆç›£è¦–ã®ã¿ï¼‰ | ãƒˆãƒ¬ãƒ¼ãƒ‰è¦³ç‚¹ã§ã®å¥å…¨æ€§ç¢ºèª |
| å‚è€ƒ | OOB RÂ² | è¨˜éŒ² | ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—å¤–ã‚µãƒ³ãƒ—ãƒ«ã§ã®æ±åŒ–æŒ‡æ¨™ |

> **ğŸ“Œ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å€™è£œã¨ã—ã¦ã®åˆ¤æ–­åŸºæº–**
>
> æ„æ€æ±ºå®šã¯RMSEã§è¡Œã„ã€MSRã¨ç›¸é–¢ã¯è£œåŠ©çš„ãªè¨ºæ–­æŒ‡æ¨™ã¨ã—ã¾ã™ã€‚
> OOB RÂ²ã¯ã‚ãã¾ã§è£œåŠ©çš„æŒ‡æ¨™ã§ã‚ã‚Šã€æœ€çµ‚çš„ãªæ„æ€æ±ºå®šã¯OOF RMSEã§è¡Œã„ã¾ã™ã€‚
>
> **ExtraTreesã¨ã®å„ªå…ˆãƒ«ãƒ¼ãƒ«**: ã¾ãšExtraTreesã‚’è©¦ã—ã€RandomForestã¯OOF RMSEã¨ç›¸é–¢ã‚’è¦‹ã¦æ¡å¦ã‚’æ±ºã‚ã¾ã™ã€‚
> ä¸¡è€…ã®ç›¸é–¢ãŒé«˜ã„å ´åˆï¼ˆ> 0.95ï¼‰ã¯ã€ã‚ˆã‚Šè‰¯ã„RMSEã‚’ç¤ºã™æ–¹ã®ã¿ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«æ¡ç”¨ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¾ã™ã€‚

### 5.2 å®šæ€§åŸºæº–

- [ ] å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ï¼‰
- [ ] æˆæœç‰©ãŒå…¨ã¦ç”Ÿæˆã•ã‚Œã‚‹
- [ ] å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆruff, pyrightï¼‰ã‚’ãƒ‘ã‚¹
- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãŒå…¨ã¦ãƒ‘ã‚¹

---

## 6. å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰

### 6.1 å­¦ç¿’å®Ÿè¡Œ

```bash
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
python -m src.models.randomforest.train_randomforest

# ãƒã‚¤ãƒ‘ãƒ©æŒ‡å®š
python -m src.models.randomforest.train_randomforest \
    --n-estimators 700 \
    --max-depth 20 \
    --min-samples-leaf 3 \
    --max-features 0.8
```

### 6.2 ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
pytest tests/models/test_randomforest.py -v
```

### 6.3 å“è³ªãƒã‚§ãƒƒã‚¯

```bash
ruff check src/models/randomforest/
ruff format src/models/randomforest/
pyright src/models/randomforest/
```

---

## 7. å‚è€ƒãƒªãƒ³ã‚¯

- [scikit-learn RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
- [ExtraTreesä»•æ§˜æ›¸](extratrees.md)
- [LGBMå®Ÿè£…](../models/lgbm/train_lgbm.py)
- [CVå…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«](../../src/models/common/cv_utils.py)
