# ExtraTrees ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ä»•æ§˜æ›¸

æœ€çµ‚æ›´æ–°: 2025-12-13

## å®Ÿè£…ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹

**Status**: âœ… **å®Ÿè£…å®Œäº†**

### å®Ÿè£…æ¸ˆã¿
- âœ… `src/models/extratrees/train_extratrees.py`: å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- âœ… `src/models/extratrees/predict_extratrees.py`: æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- âœ… `configs/models/extratrees.yaml`: YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
- âœ… Unit tests: `tests/models/test_extratrees.py` (10ãƒ†ã‚¹ãƒˆ ALL PASS)

### æˆæœç‰©
- âœ… `artifacts/models/extratrees/inference_bundle.pkl`
- âœ… `artifacts/models/extratrees/oof_predictions.csv`
- âœ… `artifacts/models/extratrees/cv_fold_logs.csv`
- âœ… `artifacts/models/extratrees/model_meta.json`
- âœ… `artifacts/models/extratrees/feature_list.json`
- âœ… `artifacts/models/extratrees/feature_importances.csv` ï¼ˆç‰¹å¾´é‡é‡è¦åº¦ï¼‰
- âœ… `artifacts/models/extratrees/submission.csv`

**Note**: å‡ºåŠ›ä»•æ§˜ã®è©³ç´°ã¯ [README.md](README.md#æˆæœç‰©å‡ºåŠ›ä»•æ§˜kaggle-nbç”¨) ã‚’å‚ç…§ã€‚

### LBæ¤œè¨¼çµæœ âŒ

**LB Score: 0.500** - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¾¡å€¤ãªã—ã€**éæ¡ç”¨**

| æŒ‡æ¨™ | ExtraTrees | LGBM | å·®åˆ† |
|------|------------|------|------|
| **OOF RMSE** | 0.011347 | 0.012164 | **-6.7%** âœ… |
| **LB Score** | 0.500 | 0.681 | **-26.6%** âŒ |

**çµè«–**: ExtraTreesã¯OOFã§ã¯è‰¯å¥½ã ãŒLBã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åŒç­‰ï¼ˆ0.500ï¼‰ã€‚
åˆ†å‰²ç‚¹ã®ãƒ©ãƒ³ãƒ€ãƒ é¸æŠãŒé‡‘èãƒ‡ãƒ¼ã‚¿ã®å¾®å¼±ã‚·ã‚°ãƒŠãƒ«æ¤œå‡ºã«ä¸é©åˆã€‚
å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆLGBM/XGBoost/CatBoostï¼‰ã¨ã¯ç•°ãªã‚Šã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¾¡å€¤ãªã—ã€‚

---

## 1. ç›®çš„ã¨ä½ç½®ã¥ã‘

### 1.1 ãƒ¢ãƒ‡ãƒ«é¸å®šãƒ•ã‚§ãƒ¼ã‚ºã§ã®å½¹å‰²

- **ç›®çš„**: ãƒã‚®ãƒ³ã‚°ç³»ãƒ„ãƒªãƒ¼ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€GBDTã¨ã¯ç•°ãªã‚‹å¤šæ§˜æ€§ã‚’å°å…¥
- **æœŸå¾…åŠ¹æœ**: 
  - å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã¨ã¯ã€Œæºã‚Œæ–¹ã€ãŒç•°ãªã‚‹äºˆæ¸¬
  - LGBMã¨ã®äºˆæ¸¬ç›¸é–¢ãŒ0.85-0.92ç¨‹åº¦ï¼ˆé«˜ã„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¾¡å€¤ï¼‰
  - éå­¦ç¿’ã—ã«ãã„æ€§è³ª
- **æ¯”è¼ƒå¯¾è±¡**: LGBM ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆOOF RMSE: 0.012164, LB: 0.681ï¼‰

### 1.2 ExtraTreesã®ç‰¹å¾´

- **Extremely Randomized Trees**: åˆ†å‰²ç‚¹ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
- **ãƒã‚®ãƒ³ã‚°**: å„æœ¨ãŒç‹¬ç«‹ã«å­¦ç¿’ï¼ˆãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®ã‚ˆã†ãªé€æ¬¡å­¦ç¿’ã§ã¯ãªã„ï¼‰
- **é«˜ã„ãƒãƒªã‚¢ãƒ³ã‚¹å‰Šæ¸›**: å¤šæ•°ã®å¼±å­¦ç¿’å™¨ã®å¹³å‡ã§å®‰å®šã—ãŸäºˆæ¸¬
- **é«˜é€Ÿ**: æœ€é©åˆ†å‰²ç‚¹ã‚’æ¢ç´¢ã—ãªã„ãŸã‚ã€RandomForestã‚ˆã‚Šé«˜é€Ÿ

### 1.3 å‰ææ¡ä»¶

- **ç‰¹å¾´ã‚»ãƒƒãƒˆ**: FS_compactï¼ˆ116åˆ—ï¼‰ã‚’å›ºå®šï¼ˆFeature Selection Phase ã§ã®çµè«–ã¨æ•´åˆï¼‰
- **CVè¨­å®š**: TimeSeriesSplit, n_splits=5, gap=0ï¼ˆLGBMã¨åŒä¸€ï¼‰
- **è©•ä¾¡æŒ‡æ¨™**:
  - **ä¸»æŒ‡æ¨™**: OOF RMSEï¼ˆé¸å®šãƒ•ã‚§ãƒ¼ã‚ºã®æœ€é‡è¦æŒ‡æ¨™ï¼‰
  - **è£œåŠ©æŒ‡æ¨™**: äºˆæ¸¬ç›¸é–¢ï¼ˆvs LGBMï¼‰ã€OOF MSRï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‰è¦³ç‚¹ã§ã®ç›£è¦–ï¼‰
- **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: ä¸è¦ï¼ˆãƒ„ãƒªãƒ¼ç³»ãƒ¢ãƒ‡ãƒ«ï¼‰

---

## 2. æŠ€è¡“ä»•æ§˜

### 2.1 å…¥å‡ºåŠ›

| é …ç›® | ä»•æ§˜ |
|------|------|
| å…¥åŠ› | `data/raw/train.csv`, `data/raw/test.csv` |
| ç‰¹å¾´é‡ç”Ÿæˆ | SU1 + SU5 â†’ tier3é™¤å¤– â†’ 116åˆ— |
| å‡ºåŠ› | `artifacts/models/extratrees/` é…ä¸‹ã«æˆæœç‰© |

### 2.2 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹é€ 

```
ç”Ÿãƒ‡ãƒ¼ã‚¿ (94åˆ—)
    â†“
[SU1FeatureAugmenter + SU5FeatureAugmenter]  # æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†åˆ©ç”¨
    â†“
åˆè¨ˆ 577åˆ—
    â†“
[tier3 feature exclusion]  # configs/feature_selection/tier3/excluded.json
    â†“
116åˆ— (FS_compact)
    â†“
[GroupImputers: M/E/I/P/S]  # æ—¢å­˜å‰å‡¦ç†å†åˆ©ç”¨
    â†“
[SimpleImputer]  # æ®‹ä½™NaNå‡¦ç†
    â†“
[ExtraTreesRegressor]  # â˜… ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸è¦
```

### 2.3 åˆæœŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```python
extratrees_params = {
    "n_estimators": 500,         # æœ¨ã®æ•°ï¼ˆå¤šã„ã»ã©å®‰å®šï¼‰
    "max_depth": 15,             # æœ¨ã®æ·±ã•åˆ¶é™
    "min_samples_split": 10,     # åˆ†å‰²ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
    "min_samples_leaf": 5,       # è‘‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
    "max_features": 0.7,         # å„åˆ†å‰²ã§ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡å‰²åˆ
    "bootstrap": False,          # ExtraTreesã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    "random_state": 42,
    "n_jobs": -1,
}
```

### 2.4 GBDTã¨ã®ä¸»ãªé•ã„

| é …ç›® | ExtraTrees | LGBM/XGBoost |
|------|------------|--------------|
| å­¦ç¿’æ–¹å¼ | ãƒã‚®ãƒ³ã‚°ï¼ˆä¸¦åˆ—ï¼‰ | ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆé€æ¬¡ï¼‰ |
| åˆ†å‰²ç‚¹é¸æŠ | ãƒ©ãƒ³ãƒ€ãƒ  | æœ€é©ç‚¹ã‚’æ¢ç´¢ |
| äºˆæ¸¬ã®æºã‚Œ | ç©ã‚„ã‹ | æ€¥å³» |
| éå­¦ç¿’å‚¾å‘ | ä½ã„ | ä¸­ç¨‹åº¦ |
| ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° | ä¸è¦ | ä¸è¦ |

> **ğŸ“Œ RandomForestã¨ã®å½¹å‰²åˆ†æ‹…**
>
> ExtraTreesã¯ã€Œã‚ˆã‚Šãƒ©ãƒ³ãƒ€ãƒ ã§ãƒãƒªã‚¢ãƒ³ã‚¹é«˜ã‚ã€ã€RandomForestã¯ã€Œã‚‚ã†å°‘ã—è½ã¡ç€ã„ãŸãƒ©ãƒ³ãƒ€ãƒ æ€§ã€ã¨ã„ã†ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚
> ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¦³ç‚¹ã§ã¯ã€ExtraTreesã¯ã€Œå¤‰ãªæºã‚Œæ–¹ã€ã‚’ã•ã›ãŸã„ã¨ãã«æ¡ç”¨ã—ã¾ã™ã€‚
> **ã¾ãšExtraTreesã‚’è©¦ã—ã€RandomForestã¯OOF RMSEã¨ç›¸é–¢ã‚’è¦‹ã¦æ¡å¦ã‚’æ±ºå®š**ã™ã‚‹æ–¹é‡ã§ã™ã€‚

---

## 3. å®Ÿè£…è©³ç´°

### 3.1 ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
src/models/extratrees/
â”œâ”€â”€ __init__.py           # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆæœŸåŒ–
â””â”€â”€ train_extratrees.py   # ãƒ¡ã‚¤ãƒ³å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

configs/models/
â””â”€â”€ extratrees.yaml       # YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

tests/models/
â””â”€â”€ test_extratrees.py    # ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
```

### 3.2 train_extratrees.py ã®å®Ÿè£…è¦ä»¶

#### 3.2.1 å¿…é ˆæ©Ÿèƒ½

1. **å¼•æ•°ãƒ‘ãƒ¼ã‚¹**: `argparse`ã§ä»¥ä¸‹ã‚’å—ã‘ä»˜ã‘ã‚‹
   - `--data-dir`: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `data/raw`ï¼‰
   - `--out-dir`: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `artifacts/models/extratrees`ï¼‰
   - `--config-path`: feature_generation.yaml ãƒ‘ã‚¹
   - `--preprocess-config`: preprocess.yaml ãƒ‘ã‚¹
   - `--feature-tier`: ä½¿ç”¨ã™ã‚‹tierï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `tier3`ï¼‰
   - `--n-splits`, `--gap`: CVè¨­å®š
   - ExtraTreesãƒã‚¤ãƒ‘ãƒ©: `--n-estimators`, `--max-depth`, `--min-samples-leaf`, `--max-features`

2. **ç‰¹å¾´é‡ç”Ÿæˆ**: æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†åˆ©ç”¨
   ```python
   from src.feature_generation.su5.train_su5 import (
       load_su1_config,
       load_su5_config,
       load_preprocess_policies,
       SU5FeatureAugmenter,
       _prepare_features,
   )
   from src.models.common.feature_loader import get_excluded_features
   ```

3. **å‰å‡¦ç†**: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸è¦
   ```python
   from sklearn.impute import SimpleImputer
   # StandardScalerã¯ä¸è¦
   ```

4. **ãƒ¢ãƒ‡ãƒ«å­¦ç¿’**:
   ```python
   from sklearn.ensemble import ExtraTreesRegressor
   ```

5. **æˆæœç‰©å‡ºåŠ›**:
   - `inference_bundle.pkl`: å…¨fold ã®ãƒ¢ãƒ‡ãƒ« + å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
   - `oof_predictions.csv`: OOFäºˆæ¸¬å€¤
   - `cv_fold_logs.csv`: foldåˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
   - `model_meta.json`: è¨­å®šãƒ»è©•ä¾¡ã‚µãƒãƒª

### 3.3 ã‚³ãƒ¼ãƒ‰ã‚¹ã‚±ãƒ«ãƒˆãƒ³

```python
"""ExtraTrees training script with unified CV framework."""

from __future__ import annotations

import argparse
import json
import pickle
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.impute import SimpleImputer
from sklearn.model_selection import TimeSeriesSplit
from sklearn.pipeline import Pipeline

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.feature_generation.su5.train_su5 import (
    SU5FeatureAugmenter,
    _prepare_features,
    load_preprocess_policies,
    load_su1_config,
    load_su5_config,
)
from src.models.common.cv_utils import (
    CVConfig,
    aggregate_fold_results,
    compute_fold_metrics,
    create_cv_splits,
)
from src.models.common.feature_loader import get_excluded_features


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train ExtraTrees model")
    parser.add_argument("--data-dir", type=str, default="data/raw")
    parser.add_argument("--out-dir", type=str, default="artifacts/models/extratrees")
    parser.add_argument(
        "--config-path", type=str, default="configs/feature_generation.yaml"
    )
    parser.add_argument("--preprocess-config", type=str, default="configs/preprocess.yaml")
    parser.add_argument("--feature-tier", type=str, default="tier3")
    parser.add_argument("--n-splits", type=int, default=5)
    parser.add_argument("--gap", type=int, default=0)
    # ExtraTrees specific
    parser.add_argument("--n-estimators", type=int, default=500)
    parser.add_argument("--max-depth", type=int, default=15)
    parser.add_argument("--min-samples-leaf", type=int, default=5)
    parser.add_argument("--max-features", type=float, default=0.7)
    return parser.parse_args()


def build_extratrees_pipeline(
    n_estimators: int = 500,
    max_depth: int = 15,
    min_samples_leaf: int = 5,
    max_features: float = 0.7,
) -> Pipeline:
    """Build ExtraTrees pipeline (no scaling needed)."""
    return Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("model", ExtraTreesRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=10,
            min_samples_leaf=min_samples_leaf,
            max_features=max_features,
            bootstrap=False,
            random_state=42,
            n_jobs=-1,
        )),
    ])


def train_fold(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_val: pd.DataFrame,
    pipeline_params: dict[str, Any],
) -> tuple[Pipeline, np.ndarray]:
    """Train a single fold."""
    pipeline = build_extratrees_pipeline(**pipeline_params)
    pipeline.fit(X_train, y_train)
    val_pred = pipeline.predict(X_val)
    return pipeline, val_pred


def main() -> None:
    args = parse_args()
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»ç‰¹å¾´é‡ç”Ÿæˆï¼ˆæ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    train_df = pd.read_csv(Path(args.data_dir) / "train.csv")
    test_df = pd.read_csv(Path(args.data_dir) / "test.csv")

    # SU1 + SU5 ç‰¹å¾´é‡ç”Ÿæˆ
    su1_cfg = load_su1_config(args.config_path)
    su5_cfg = load_su5_config(args.config_path)
    preprocess_policies = load_preprocess_policies(args.preprocess_config)
    augmenter = SU5FeatureAugmenter(su1_cfg, su5_cfg, preprocess_policies)

    train_aug = augmenter.fit_transform(train_df.copy())
    test_aug = augmenter.transform(test_df.copy())

    # tieré™¤å¤–
    excluded = get_excluded_features(args.feature_tier)
    feature_cols = [c for c in train_aug.columns if c not in excluded and c not in ["Date", "TARGET"]]
    
    X = train_aug[feature_cols]
    y = train_aug["TARGET"]
    X_test = test_aug[feature_cols]

    # CVè¨­å®š
    cv_config = CVConfig(n_splits=args.n_splits, gap=args.gap)
    splits = create_cv_splits(X, cv_config)

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    pipeline_params = {
        "n_estimators": args.n_estimators,
        "max_depth": args.max_depth,
        "min_samples_leaf": args.min_samples_leaf,
        "max_features": args.max_features,
    }

    # CVå­¦ç¿’
    fold_results = []
    oof_preds = np.zeros(len(X))
    models = []

    for fold_idx, (train_idx, val_idx) in enumerate(splits):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        pipeline, val_pred = train_fold(X_train, y_train, X_val, pipeline_params)
        oof_preds[val_idx] = val_pred
        models.append(pipeline)

        metrics = compute_fold_metrics(y_val.values, val_pred, fold_idx)
        fold_results.append(metrics)
        print(f"Fold {fold_idx}: RMSE={metrics.rmse:.6f}")

    # é›†è¨ˆ
    summary = aggregate_fold_results(fold_results)
    print(f"\nOOF RMSE: {summary['oof_rmse']:.6f}")
    print(f"OOF MSR: {summary['oof_msr']:.6f}")

    # æˆæœç‰©ä¿å­˜
    # 1. inference_bundle.pkl
    test_preds = np.mean([m.predict(X_test) for m in models], axis=0)
    bundle = {
        "models": models,
        "augmenter": augmenter,
        "feature_cols": feature_cols,
        "excluded_features": excluded,
    }
    with open(out_dir / "inference_bundle.pkl", "wb") as f:
        pickle.dump(bundle, f)

    # 2. oof_predictions.csv
    oof_df = pd.DataFrame({
        "Date": train_aug["Date"],
        "TARGET": y,
        "prediction": oof_preds,
    })
    oof_df.to_csv(out_dir / "oof_predictions.csv", index=False)

    # 3. cv_fold_logs.csv
    fold_logs = pd.DataFrame([
        {"fold": r.fold_idx, "rmse": r.rmse, "msr": r.msr, "n_samples": r.n_samples}
        for r in fold_results
    ])
    fold_logs.to_csv(out_dir / "cv_fold_logs.csv", index=False)

    # 4. model_meta.json
    meta = {
        "model_type": "ExtraTreesRegressor",
        "feature_tier": args.feature_tier,
        "n_features": len(feature_cols),
        "cv_config": {"n_splits": args.n_splits, "gap": args.gap},
        "hyperparameters": pipeline_params,
        "oof_rmse": summary["oof_rmse"],
        "oof_msr": summary["oof_msr"],
    }
    with open(out_dir / "model_meta.json", "w") as f:
        json.dump(meta, f, indent=2)

    # 5. submission.csv
    submission = pd.DataFrame({
        "Date": test_aug["Date"],
        "TARGET": test_preds,
    })
    submission.to_csv(out_dir / "submission.csv", index=False)

    print(f"\nArtifacts saved to {out_dir}")


if __name__ == "__main__":
    main()
```

---

## 4. ãƒ†ã‚¹ãƒˆè¦ä»¶

### 4.1 ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ

```python
# tests/models/test_extratrees.py

import numpy as np
import pandas as pd
import pytest
from sklearn.ensemble import ExtraTreesRegressor


class TestExtraTreesPipeline:
    """Test ExtraTrees pipeline components."""

    def test_extratrees_basic_fit(self, sample_train_data: pd.DataFrame) -> None:
        """ExtraTreesRegressor can fit and predict."""
        X = sample_train_data.drop(columns=["Date", "TARGET"])
        y = sample_train_data["TARGET"]

        model = ExtraTreesRegressor(n_estimators=10, random_state=42, n_jobs=-1)
        model.fit(X, y)
        preds = model.predict(X)

        assert len(preds) == len(y)
        assert not np.isnan(preds).any()

    def test_extratrees_no_scaling_needed(self, sample_train_data: pd.DataFrame) -> None:
        """ExtraTrees produces same results regardless of scaling."""
        X = sample_train_data.drop(columns=["Date", "TARGET"])
        y = sample_train_data["TARGET"]

        # Without scaling
        model1 = ExtraTreesRegressor(n_estimators=10, random_state=42)
        model1.fit(X, y)
        pred1 = model1.predict(X)

        # With scaling (should give same results for tree models)
        from sklearn.preprocessing import StandardScaler
        X_scaled = pd.DataFrame(
            StandardScaler().fit_transform(X),
            columns=X.columns
        )
        model2 = ExtraTreesRegressor(n_estimators=10, random_state=42)
        model2.fit(X_scaled, y)
        pred2 = model2.predict(X_scaled)

        # Tree models are scale-invariant (results should be identical)
        np.testing.assert_array_almost_equal(pred1, pred2)

    def test_feature_importance_available(self, sample_train_data: pd.DataFrame) -> None:
        """ExtraTrees provides feature importances."""
        X = sample_train_data.drop(columns=["Date", "TARGET"])
        y = sample_train_data["TARGET"]

        model = ExtraTreesRegressor(n_estimators=10, random_state=42)
        model.fit(X, y)

        importances = model.feature_importances_
        assert len(importances) == X.shape[1]
        assert np.isclose(importances.sum(), 1.0)
```

### 4.2 çµ±åˆãƒ†ã‚¹ãƒˆ

```python
def test_extratrees_cv_pipeline(sample_train_data: pd.DataFrame) -> None:
    """Test full CV pipeline for ExtraTrees."""
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.impute import SimpleImputer
    from sklearn.pipeline import Pipeline

    X = sample_train_data.drop(columns=["Date", "TARGET"])
    y = sample_train_data["TARGET"]

    tscv = TimeSeriesSplit(n_splits=3)
    oof_preds = np.zeros(len(X))

    for train_idx, val_idx in tscv.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train = y.iloc[train_idx]

        pipeline = Pipeline([
            ("imputer", SimpleImputer(strategy="median")),
            ("model", ExtraTreesRegressor(n_estimators=10, random_state=42)),
        ])
        pipeline.fit(X_train, y_train)
        oof_preds[val_idx] = pipeline.predict(X_val)

    # Check OOF predictions are valid
    assert not np.isnan(oof_preds).any()
    assert oof_preds.std() > 0  # Not all same value
```

---

## 5. æˆåŠŸåŸºæº–

### 5.1 å®šé‡åŸºæº–

| å„ªå…ˆåº¦ | æŒ‡æ¨™ | é–¾å€¤ | ç†ç”± |
|--------|------|------|------|
| **ä¸»æŒ‡æ¨™** | OOF RMSE | â‰¤ 0.0130 | LGBMï¼ˆ0.01216ï¼‰ã‚ˆã‚Šå¤šå°‘åŠ£ã£ã¦ã‚‚è¨±å®¹ |
| è£œåŠ© | äºˆæ¸¬ç›¸é–¢ï¼ˆvs LGBMï¼‰ | < 0.92 | ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¤šæ§˜æ€§ã®ç¢ºä¿ |
| è£œåŠ© | OOF MSR | > 0ï¼ˆç›£è¦–ã®ã¿ï¼‰ | ãƒˆãƒ¬ãƒ¼ãƒ‰è¦³ç‚¹ã§ã®å¥å…¨æ€§ç¢ºèª |

> **ğŸ“Œ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å€™è£œã¨ã—ã¦ã®åˆ¤æ–­åŸºæº–**
>
> æ„æ€æ±ºå®šã¯RMSEã§è¡Œã„ã€MSRã¨ç›¸é–¢ã¯è£œåŠ©çš„ãªè¨ºæ–­æŒ‡æ¨™ã¨ã—ã¾ã™ã€‚
> **RMSEã¯å¤šå°‘æ‚ªãã¦ã‚‚ï¼ˆâ‰¤ 0.0130ï¼‰ã€å¤šæ§˜æ€§ï¼ˆç›¸é–¢ < 0.92ï¼‰ãŒç¢ºä¿ã§ãã‚Œã°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å€™è£œã¨ã—ã¦æ®‹ã™**æ–¹é‡ã§ã™ã€‚
> ExtraTreesã¯ã€ŒGBDTã¨ã¯ç•°ãªã‚‹æºã‚Œæ–¹ã€ã‚’æœŸå¾…ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ç²¾åº¦ã‚ˆã‚Šã‚‚å¤šæ§˜æ€§ã‚’é‡è¦–ã—ã¾ã™ã€‚

### 5.2 å®šæ€§åŸºæº–

- [ ] å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ï¼‰
- [ ] æˆæœç‰©ãŒå…¨ã¦ç”Ÿæˆã•ã‚Œã‚‹
- [ ] å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆruff, pyrightï¼‰ã‚’ãƒ‘ã‚¹
- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãŒå…¨ã¦ãƒ‘ã‚¹

---

## 6. å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰

### 6.1 å­¦ç¿’å®Ÿè¡Œ

```bash
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
python -m src.models.extratrees.train_extratrees

# ãƒã‚¤ãƒ‘ãƒ©æŒ‡å®š
python -m src.models.extratrees.train_extratrees \
    --n-estimators 700 \
    --max-depth 20 \
    --min-samples-leaf 3 \
    --max-features 0.8
```

### 6.2 ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
pytest tests/models/test_extratrees.py -v
```

### 6.3 å“è³ªãƒã‚§ãƒƒã‚¯

```bash
ruff check src/models/extratrees/
ruff format src/models/extratrees/
pyright src/models/extratrees/
```

---

## 7. å‚è€ƒãƒªãƒ³ã‚¯

- [scikit-learn ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)
- [Extremely Randomized Trees è«–æ–‡](https://link.springer.com/article/10.1007/s10994-006-6226-1)
- [LGBMå®Ÿè£…](../models/lgbm/train_lgbm.py)
- [CVå…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«](../../src/models/common/cv_utils.py)

---

## 8. æ³¨æ„äº‹é …ï¼ˆXGBoostå®Ÿè£…ã‹ã‚‰å¾—ãŸå…±é€šæ•™è¨“ï¼‰

### 8.1 ãƒ†ã‚¹ãƒˆäºˆæ¸¬æ™‚ã®featureãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯å­¦ç¿’æ™‚ã«å­˜åœ¨ã—ãªã„ã‚«ãƒ©ãƒ ï¼ˆ`is_scored`, `lagged_*`ç­‰ï¼‰ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚‹ã€‚
**å­¦ç¿’æ™‚ã®feature_colsã®ã¿ã‚’æŠ½å‡º**ã—ã¦ã‹ã‚‰äºˆæ¸¬ã‚’å®Ÿè¡Œï¼š
```python
test_features = test_df[feature_cols].copy()
test_pred = final_pipeline.predict(test_features)
```

### 8.2 submission.csv ã®ã‚·ã‚°ãƒŠãƒ«å¤‰æ›

ç”Ÿã®äºˆæ¸¬å€¤ï¼ˆexcess returnsï¼‰ã§ã¯ãªãã€**ç«¶æŠ€ã‚·ã‚°ãƒŠãƒ«å½¢å¼**ã«å¤‰æ›ã—ã¦å‡ºåŠ›ï¼š
```python
# ã‚·ã‚°ãƒŠãƒ«å¤‰æ›: pred * mult + 1.0, clipped to [0.9, 1.1]
signal_mult = 1.0
signal_pred = np.clip(test_pred * signal_mult + 1.0, 0.9, 1.1)

# ã‚«ãƒ©ãƒ åã¯ "prediction"ï¼ˆtargetå¤‰æ•°åã§ã¯ãªã„ï¼‰
submission_df = pd.DataFrame({
    "date_id": id_values,
    "prediction": signal_pred,
})
```

### 8.3 is_scored ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

submission.csvã«ã¯`is_scored==True`ã®è¡Œã®ã¿ã‚’å«ã‚ã‚‹ï¼ˆç«¶æŠ€è¦ä»¶ï¼‰ã€‚
