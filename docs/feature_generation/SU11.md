# SU11: メタモデル・予測値を用いた 2 段階特徴（スタッキング系）

最終更新: 2025-12-06

---

## 0. ステータス

| 項目 | 状態 |
|------|------|
| 実装状況 | 未実装（設計フェーズ） |
| 採否ステータス | `status: planned`, `decision: pending` |
| 位置づけ | SU ライン由来の「メタ特徴束」 |

**コンセプト**: Level-1 モデルの予測値（OOF / test）を入力とする Level-2 スタッキングを導入し、Sharpe 向上を狙う。

---

## 1. 目的と背景

### 1.1 目的

Hull Tactical Market Prediction において、既に確立済みの安定ライン

- **ベースライン**: SU1 + SU5 + GroupImputers + StandardScaler + LGBM（LB 0.681）

を **Level-1 モデル** とみなし、その予測値を新しい特徴量として用いる **2 段階構造（スタッキング）** を導入する。

#### 狙い

1. Level-1 モデルが既存の全特徴（生+SU1+SU5+前処理）から抽出した情報を、**1 次元の潜在表現 `y_pred_base`** として扱う。

2. Level-2 モデルは、
   - **入力**: `y_pred_base`（+ 必要に応じてごく少数の追加特徴）
   - **出力**: 最終予測値 `y_pred_final`
   
   とし、「ベースモデルの残差・微調整」に特化することで、**過学習リスクを抑えつつ Sharpe の改善**を狙う。

### 1.2 背景（これまでの SU1〜SU10 の教訓）

#### 既存 SU ラインの状況（抜粋）

| SU | 状態 | LB スコア | 備考 |
|----|------|-----------|------|
| SU1 | **採用** | 0.674 | 欠損構造一次特徴 |
| SU5 | **採用** | 0.681 | 共欠損構造特徴（現行ベスト） |
| SU2 | 非採用 | 0.597 | 高次欠損特徴 → 過学習 |
| SU3 | 非採用 | 0.461 | 遷移・再出現パターン → コンセプト不適合 |
| SU7 | 非採用 | 0.476 | モメンタム/RSI 系 → OOF 改善も LB 大幅悪化 |
| SU8 | 非採用 | 0.624 | ボラティリティ/レジーム → OOF/LB 共に悪化 |
| SU9 | 非採用 | 0.679 | カレンダー/季節性 → OOF 改善も LB 微悪化 |
| SU10 | 非採用 | 0.597 | 外部レジーム（SPY） → LB -12.3% 悪化 |

**教訓**: 高次特徴や外部軸を直接追加するアプローチは、**OOF が良くても LB で崩れやすい**という傾向が確認されている。

### 1.3 SU11 のコンセプト

そこで SU11 では、

1. 「大量の新特徴を追加する」のではなく、
2. **既に安定している Level-1 モデルの予測値を 1 列のメタ特徴として利用**し、
3. それに対して **より強く正則化された Level-2 モデルで微修正だけを学習**する、

というアーキテクチャにより、

- **自由度を抑えつつ**
- **既存ラインの情報を最大限活かす**

ことを狙う。

---

## 2. SU シリーズ内での位置づけ

| SU | 軸 | 概要 | 状態 |
|----|-----|------|------|
| SU1 | 欠損構造 | 単列レベルの欠損フラグ・run | **採用** (LB 0.674) |
| SU5 | 欠損構造 | 共欠損構造特徴 | **採用** (LB 0.681) ← ベスト |
| SU7 | モメンタム | diff/lag/RSI 等 | 非採用 |
| SU8 | ボラティリティ | 内部ボラ・レジーム | 非採用 |
| SU9 | カレンダー | 曜日・月・祝日 | 非採用 |
| SU10 | 外部レジーム | SPY レジーム特徴 | 非採用 |
| **SU11** | **メタモデル** | **Level-1 予測値由来の 2 段階特徴** | **計画中** |

他の SU は「入力特徴の変形・拡張」だが、**SU11 は「モデル出力を再利用するメタ特徴」**という性格を持つ。

実装方針としては、`src/feature_generation/su11/` 配下に実装する前提で記述する。

---

## 3. パイプライン全体像

### 3.1 現行ライン（Level-1 モデル）

現行の標準ラインは概ね次のとおり:

```
train.csv / test.csv 読み込み
    ↓
SU1 FeatureAugmenter
    ↓
SU5 FeatureAugmenter
    ↓
GroupImputers (M/E/I/P/S)
    ↓
前処理 (StandardScaler + OneHot)
    ↓
LGBMRegressor (Level-1 モデル)
```

### 3.2 SU11 導入後の概念図

```
[Step 1: Level-1 学習 & 予測値生成]
  train → (既存ライン: SU1+SU5+Imputers+Preprocess+LGBM) → Level-1 モデル
  train → OOF 予測値 y_pred_L1_oof
  test  → 予測値 y_pred_L1_test

[Step 2: Level-2 用データセット構築]
  Level-2 train データ:
    - 特徴: y_pred_L1_oof (+ 必要ならごく少数の補助特徴)
    - 目的変数: y (元の目的変数)
  Level-2 test データ:
    - 特徴: y_pred_L1_test (+ 同じ補助特徴)

[Step 3: Level-2 学習 & 最終予測]
  Level-2 モデル (LGBM / Ridge / 小さめNN 等) を学習
  Level-2 で y_pred_L2_oof, y_pred_L2_test を生成
  → y_pred_L2 系を最終 submit 用の予測値として利用
```

Level-2 は `y_pred_L1` を説明変数とする追加モデルであり、

- 単純な線形補正（Ridge）から
- 非線形な微調整（浅い LGBM / 小規模 NN）

まで柔軟に検討可能とする。

---

## 4. インターフェースとファイル構成（想定）

### 4.1 ディレクトリ構成（案）

```
src/
  feature_generation/
    su11/
      __init__.py
      feature_su11.py      # SU11Config, SU11MetaFeatureBuilder など
      train_su11.py        # Level-1/Level-2 学習 & OOF 生成パイプライン
      predict_su11.py      # 本番推論 (Level-1 + Level-2 結合)

tests/
  feature_generation/
    test_su11.py           # 単体テスト

artifacts/
  SU11/
    inference_bundle.pkl   # Level-1 + Level-2 統合バンドル
    model_meta.json
    feature_list.json
    cv_fold_logs.csv
    oof_predictions.csv
    submission.csv
```

### 4.2 主要クラス（概念）

```python
@dataclass
class SU11Config:
    level1_artifacts_dir: str       # 既存 SU5 ラインなどの artifacts へのパス
    level2_model_type: str          # "ridge" | "lgbm" | "mlp" 等
    use_extra_features: bool        # y_pred 以外を使うか
    extra_feature_names: list[str]  # 使用する追加特徴名（例: ごく一部の SU7/SU9/SU10 等）
    n_splits: int = 5               # CV fold 数
    random_state: int = 42


class SU11MetaFeatureBuilder:
    """
    Level-1 の OOF/test 予測値から Level-2 用データセットを構築する責務。
    """
    def build_level2_train(
        self,
        oof_pred: pd.Series,
        y: pd.Series,
        X_extra: pd.DataFrame | None = None
    ) -> pd.DataFrame:
        ...

    def build_level2_test(
        self,
        test_pred: pd.Series,
        X_extra: pd.DataFrame | None = None
    ) -> pd.DataFrame:
        ...
```

Level-2 モデルそのものは、既存の `inference_bundle` 形式の中に

- `"level2_model"`
- `"level2_meta"`（使用した特徴名等）

として格納する想定。

---

## 5. データフロー詳細

### 5.1 Level-1: 既存ラインの OOF / test 予測

1. 既存 SU5 ラインの `inference_bundle.pkl` を利用するか、SU11 用に再学習する。

2. 時系列 CV（既存と同じ fold 設計）で OOF 予測を生成:
   - `y_pred_L1_oof`（train と同じ長さの Series）

3. 同じバンドルで test に対して予測:
   - `y_pred_L1_test`（test 行数分の Series）

4. 既存ログ（`submissions.md` 等）にある SU5 の fold / OOF ロジックを再利用し、fold 定義・評価ロジックを完全に共通化する。

### 5.2 Level-2: 入力特徴の構成

#### 最小構成（必須）

- `x0 = y_pred_L1` の 1 列のみ

#### 拡張構成（任意）

- `x0 = y_pred_L1`
- `x1..xk`: 以下のいずれか（※「ごく少数」に限定）
  - SU7/SU8/SU9/SU10 で「概念的には有効そう」だった特徴を 1〜数列だけ再利用
    - 例: `su9_month_flag_december`, `su8_vol_level` のような代表列
  - `date_id` ベースの簡単なメタ情報
    - 例: `days_since_start`（時系列の位置）、`is_late_phase` など

> ⚠️ **注意**: SU7〜SU10 はいずれも LB 悪化しており、多く取り込みすぎると SU11 でも同じ罠にハマる可能性が高い。
> 
> 初期段階では **「純粋に `y_pred_L1` のみ」** からスタートし、必要なら 1〜2 列追加する方針とする。

### 5.3 Level-2 モデル候補

| モデル | 特徴 | 優先度 |
|--------|------|--------|
| **Ridge Regression** | 実装容易・安定・過学習しにくい | 第1候補 |
| shallow LightGBM | 非線形補正可能、ただし正則化必須 | 第2候補 |
| シンプル MLP | 入力数が少ないため小さな NN でも可 | 第3候補 |

**第一スプリント**では、

- 実装容易性・安定性から **Ridge Regression を第一候補**とし、
- 2nd 候補として **shallow LightGBM** を検証する。

---

## 6. 評価方針

### 6.1 CV 設計

- Level-1 と同じ fold 分割をそのまま Level-2 にも用いる。
- Level-2 の OOF 予測は、「Level-1 OOF を説明変数とし、同じ fold で学習・評価」を行うことで取得。

### 6.2 指標

#### 主指標

- **OOF RMSE**（Level-1 vs Level-2 の比較）
- **OOF MSR / vMSR**（既存ラインと同様）

#### 副指標

- **Public LB**（SU11 ライン submit 後）

### 6.3 採用条件の目安

1. OOF RMSE・MSR で、SU5 ベースラインと比べて **明確に改善**（統計的に有意な範囲）していること。
2. 1〜2 回の Kaggle submit で、Public LB でも SU5 の **0.681 を上回る or 同等以上** であること。

上記を満たさない場合、SU11 は「実装済みだが非採用（`not_adopted`）」として扱う。

---

## 7. Kaggle Notebook / 推論時の扱い

### 7.1 InferenceBundle の構造（案）

既存の `inference_bundle.pkl` に、Level-2 要素を追加:

```python
bundle = {
    "pipeline_L1": <既存 SU5 ラインの前処理 + LGBM>,
    "meta_L1": {...},  # fold情報, feature_list 等
    "model_L2": <Ridge or LGBM など>,
    "meta_L2": {
        "input_features": ["y_pred_L1"] or ["y_pred_L1", "extra1", ...],
        "model_type": "ridge",
        ...
    },
}
```

### 7.2 `predict(test: pl.DataFrame) -> float` の流れ

1. `test`（polars.DataFrame）を pandas に変換。

2. Level-1 パイプライン `pipeline_L1` で特徴生成 + 前処理 + 予測:
   ```python
   y_pred_L1 = pipeline_L1.predict(X_test)
   ```

3. 必要に応じて extra features を生成:
   - 例: `date_id` から簡単なフラグを作る等（リークに注意）

4. Level-2 入力特徴を組み立て:
   ```python
   X_L2 = pd.DataFrame({"y_pred_L1": y_pred_L1, ...})
   ```

5. `model_L2.predict(X_L2)` で最終予測値を出力。

6. Post-process（mult/lo/hi など）は SU5 ラインのポリシーに従いつつ、Level-2 用に過度な最適化は行わない設定を検討。

---

## 8. リスクと制約

### 8.1 リスク

| リスク | 説明 |
|--------|------|
| **OOF vs LB の乖離** | SU7〜SU10 と同様、OOF 改善が必ずしも LB 改善に繋がらないリスク。特に MSR proxy 最適化と組み合わせると、レジームに依存する方向性の賭けになりやすい。 |
| **複雑性の増加** | Level-1 / Level-2 の 2 段階管理になるため、アーティファクト管理・Notebook 実装が複雑化する。 |
| **過剰な extra features** | SU7〜SU10 由来の特徴を多く入れすぎると、結局「特徴爆発 + 過学習」に逆戻りする可能性。 |

### 8.2 制約

- 外部データ・カレンダーデータ等を Level-2 で用いる場合も、SU10 同様に**規約順守・リーク防止が必要**。
- SU11 は **既存 SU5 ラインの「上に乗る」構造**のため、SU5 のアーティファクト形式・API に強く依存する。

---

## 9. 実装スプリント案

### Sprint 1: 最小構成 PoC

| 項目 | 内容 |
|------|------|
| Level-1 | 現行 SU1+SU5 ライン（既存 artifacts を再利用） |
| Level-2 入力 | `y_pred_L1` のみ |
| Level-2 モデル | Ridge Regression |
| 成果物 | `inference_bundle_su11_poc.pkl`, `model_meta_su11_poc.json` |
| 評価 | OOF RMSE/MSR 比較、Public LB 1 回 submit |

### Sprint 2: 軽微な拡張

- extra features を 1〜2 列追加したバリアントを試す
- Level-2 モデルに shallow LightGBM を追加検証
- OOF / LB でベスト構成を 1 つ選定

### Sprint 3: 採否判断・ドキュメント反映

SU11 を

- **採用**: `configs/feature_generation.yaml` / `submissions.md` に反映
- **非採用**: `status: not_adopted` とし、コード・成果物は参考実装として残す

---

## 10. メタ情報（configs への追記イメージ）

`configs/feature_generation.yaml` への追記案:

```yaml
su11:
  enabled: false        # 初期は false（PoC段階）
  mode: "stacking"
  level1_artifacts_dir: "artifacts/SU5"
  level2_model_type: "ridge"
  use_extra_features: false
  extra_feature_names: []
  
  # Level-2 モデルパラメータ
  level2_params:
    ridge:
      alpha: 1.0
    lgbm:
      n_estimators: 50
      max_depth: 3
      learning_rate: 0.05
      reg_alpha: 1.0
      reg_lambda: 1.0
  
  metadata:
    artifacts_dir: "artifacts/SU11"
    depends_on: [SU1, SU5, GroupImputers]
    expected_usage: "Meta-model using Level-1 predictions as features (stacking)"
    numpy_version: 1.26.4
    status: "planned"
    decision: "pending"
    notes: |
      Level-1 モデル（SU5ライン）の OOF 予測値を特徴として Level-2 モデルを学習。
      過学習リスクを抑えつつ、既存ラインの情報を最大限活かすことを狙う。
      初期は Ridge Regression からスタートし、必要に応じて shallow LGBM を検証。
```

---

## 11. 次のステップ

この段階ではあくまで「大枠仕様」としておき、次のステップで以下を詰める:

1. **API 仕様**: 関数シグネチャの詳細定義
2. **OOF 生成の詳細**: fold 分割ロジックの共通化
3. **inference_bundle のキー構造**: Level-1 / Level-2 の統合方法
4. **Kaggle Notebook テンプレート**: 推論コードのプロトタイプ

---

## 参考リンク

- [SU5 仕様書](./SU5.md) - 現行ベースライン
- [submissions.md](../submissions.md) - 提出履歴
- [feature_generation.yaml](../../configs/feature_generation.yaml) - 設定ファイル
