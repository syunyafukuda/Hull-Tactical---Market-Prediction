{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03133c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"X does not have valid feature names, but LGBMRegressor was fitted with feature names\",\n",
    "    category=UserWarning,\n",
    "    module=\"sklearn.utils.validation\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*elapsed before server startup.*\",\n",
    "    category=RuntimeWarning,\n",
    "    module=\"kaggle_evaluation.core.templates\",\n",
    ")\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "INPUT_ROOT = Path(\"/kaggle/input\")\n",
    "if not INPUT_ROOT.exists():\n",
    "    raise FileNotFoundError(\"/kaggle/input is not mounted.\")\n",
    "\n",
    "ARTIFACT_FILENAMES = (\n",
    "    \"inference_bundle.pkl\",\n",
    "    \"model_meta.json\",\n",
    "    \"feature_list.json\",\n",
    ")\n",
    "\n",
    "\n",
    "def _discover_artifact_root() -> Path:\n",
    "    candidates: list[Path] = []\n",
    "    for child in sorted(INPUT_ROOT.iterdir()):\n",
    "        if not child.is_dir():\n",
    "            continue\n",
    "        if all((child / name).exists() for name in ARTIFACT_FILENAMES):\n",
    "            candidates.append(child)\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            \"No input dataset contains the required two-head artifacts \"\n",
    "            \"(inference_bundle.pkl, model_meta.json, feature_list.json).\"\n",
    "        )\n",
    "    if len(candidates) > 1:\n",
    "        print(\"[warn] multiple artifact datasets detected; using\", candidates[0])\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "ARTIFACT_ROOT = _discover_artifact_root()\n",
    "print(\"artifact root:\", ARTIFACT_ROOT)\n",
    "\n",
    "\n",
    "def _find_wheel(prefix: str) -> Path | None:\n",
    "    pattern = f\"{prefix}-*.whl\"\n",
    "    matches = sorted(ARTIFACT_ROOT.glob(pattern))\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    matches = sorted(INPUT_ROOT.glob(f\"**/{pattern}\"))\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "SKLEARN_WHEEL = _find_wheel(\"scikit_learn\")\n",
    "if SKLEARN_WHEEL is None:\n",
    "    raise FileNotFoundError(\"Required scikit-learn wheel not found alongside SU1 artifacts.\")\n",
    "\n",
    "py_tag = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n",
    "if py_tag not in SKLEARN_WHEEL.name:\n",
    "    raise RuntimeError(f\"Wheel tag mismatch: expected runtime tag '{py_tag}' in {SKLEARN_WHEEL.name}\")\n",
    "\n",
    "subprocess.run(\n",
    "    [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"uninstall\",\n",
    "        \"-y\",\n",
    "        \"scikit-learn\",\n",
    "        \"sklearn-compat\",\n",
    "        \"category-encoders\",\n",
    "    ],\n",
    "    check=False,\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "\n",
    "try:\n",
    "    import numpy as _np\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"numpy must be preinstalled in the Kaggle image.\") from exc\n",
    "else:\n",
    "    print(\"numpy:\", _np.__version__)\n",
    "\n",
    "    def _ensure_numpy_bitgenerator_aliases() -> None:\n",
    "        try:\n",
    "            import numpy.random._pickle as _np_random_pickle  # type: ignore[attr-defined]\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "        generator_cls = getattr(_np.random, \"MT19937\", None)\n",
    "        if generator_cls is None:\n",
    "            return\n",
    "\n",
    "        aliases = (\n",
    "            (\"<class 'numpy.random._mt19937.MT19937'>\", generator_cls),\n",
    "            (\"numpy.random._mt19937.MT19937\", generator_cls),\n",
    "        )\n",
    "        bit_generators = getattr(_np_random_pickle, \"BitGenerators\", {})\n",
    "        for key, cls in aliases:\n",
    "            if key not in bit_generators:\n",
    "                bit_generators[key] = cls\n",
    "\n",
    "    _ensure_numpy_bitgenerator_aliases()\n",
    "\n",
    "subprocess.check_call(\n",
    "    [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"install\",\n",
    "        \"--no-index\",\n",
    "        \"--no-deps\",\n",
    "        \"--force-reinstall\",\n",
    "        \"--upgrade\",\n",
    "        str(SKLEARN_WHEEL),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for key in list(sys.modules):\n",
    "    if key == \"sklearn\" or key.startswith(\"sklearn.\"):\n",
    "        del sys.modules[key]\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "import sklearn\n",
    "print(\"scikit-learn:\", sklearn.__version__, \"@\", sklearn.__file__)\n",
    "if not sklearn.__version__.startswith(\"1.7.\"):\n",
    "    raise RuntimeError(\"scikit-learn version mismatch: expected 1.7.x\")\n",
    "\n",
    "STATSMODELS_WHEEL = _find_wheel(\"statsmodels\")\n",
    "if STATSMODELS_WHEEL is not None:\n",
    "    try:\n",
    "        subprocess.check_call(\n",
    "            [\n",
    "                sys.executable,\n",
    "                \"-m\",\n",
    "                \"pip\",\n",
    "                \"install\",\n",
    "                \"--no-index\",\n",
    "                \"--no-deps\",\n",
    "                \"--force-reinstall\",\n",
    "                \"--upgrade\",\n",
    "                str(STATSMODELS_WHEEL),\n",
    "            ]\n",
    "        )\n",
    "    except subprocess.CalledProcessError as exc:\n",
    "        print(\"[warn] failed to install statsmodels wheel:\", STATSMODELS_WHEEL.name, exc)\n",
    "    else:\n",
    "        for key in list(sys.modules):\n",
    "            if key == \"statsmodels\" or key.startswith(\"statsmodels.\"):\n",
    "                del sys.modules[key]\n",
    "        importlib.invalidate_caches()\n",
    "        try:\n",
    "            import statsmodels as _sm\n",
    "        except ImportError:\n",
    "            print(\"[warn] statsmodels wheel installed but module not importable\")\n",
    "        else:\n",
    "            print(\"statsmodels:\", _sm.__version__)\n",
    "\n",
    "for pkg_name in (\"joblib\", \"pandas\", \"polars\", \"pyarrow\"):\n",
    "    module = __import__(pkg_name)\n",
    "    version = getattr(module, \"__version__\", \"unknown\")\n",
    "    print(f\"{pkg_name}: {version}\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import types\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Iterable, Mapping, Sequence, Tuple, cast\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import kaggle_evaluation.default_inference_server as kies\n",
    "\n",
    "# === Module: preprocess.M_group.m_group ===\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from typing import Any, Deque, Dict, Hashable, Iterable, List, Mapping, Sequence, Tuple, cast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "try:  # optional dependency\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "\n",
    "except ImportError:  # pragma: no cover - statsmodels optional at runtime\n",
    "    ARIMA = None  # type: ignore[assignment]\n",
    "    UnobservedComponents = None  # type: ignore[assignment]\n",
    "\n",
    "class MGroupImputer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\u6642\u9593\u60c5\u5831\u3092\u8003\u616e\u3057\u3066 M \u7cfb\u7279\u5fb4\u91cf\u306e\u6b20\u640d\u3092\u88dc\u5b8c\u3059\u308b\u63a8\u5b9a\u5668\u3002\n",
    "    Parameters\n",
    "    ----------\n",
    "    columns:\n",
    "        \u5bfe\u8c61\u3068\u3059\u308b\u5217\u3002\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u306f ``fit`` \u6642\u306b ``\"M\"`` \u3067\u59cb\u307e\u308b\u5217\u3092\u81ea\u52d5\u9078\u629e\u3002\n",
    "    policy:\n",
    "        \u63a1\u7528\u3059\u308b\u88dc\u5b8c\u30dd\u30ea\u30b7\u30fc\u3002 ``SUPPORTED_POLICIES`` \u3092\u53c2\u7167\u3002\n",
    "    rolling_window:\n",
    "        \u30ed\u30fc\u30ea\u30f3\u30b0\u7cfb\u30dd\u30ea\u30b7\u30fc\u3067\u7528\u3044\u308b\u7a93\u5e45\u3002\n",
    "    ema_alpha:\n",
    "        \u6307\u6570\u79fb\u52d5\u5e73\u5747\u30dd\u30ea\u30b7\u30fc\u3067\u7528\u3044\u308b\u5e73\u6ed1\u5316\u4fc2\u6570\u3002\n",
    "    calendar_column:\n",
    "        \u66dc\u65e5\u3084\u6708\u6b21\u306a\u3069\u5b63\u7bc0\u6027\u30dd\u30ea\u30b7\u30fc\u304c\u53c2\u7167\u3059\u308b\u65e5\u6642\u5217\u540d\u3002\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u306f\u30dd\u30ea\u30b7\u30fc\u5185\u3067\u81ea\u52d5\u63a2\u7d22\u3002\n",
    "    policy_params:\n",
    "        \u30dd\u30ea\u30b7\u30fc\u56fa\u6709\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6307\u5b9a\u3059\u308b\u8f9e\u66f8\u3002\u30ad\u30fc\u306f\u6587\u5b57\u5217\u3001\u5024\u306f\u6570\u5024\u307e\u305f\u306f\u6587\u5b57\u5217\u3002\n",
    "    random_state:\n",
    "        \u591a\u5909\u91cf\u30e2\u30c7\u30eb\u3092\u5229\u7528\u3059\u308b\u30dd\u30ea\u30b7\u30fc\u306e\u4e71\u6570\u30b7\u30fc\u30c9\u3002\n",
    "    Notes\n",
    "    -----\n",
    "        - **\u91cd\u8981:** ``ffill_bfill`` \u306f\u5b66\u7fd2\u6642\u306e\u307f\u5f8c\u65b9\u88dc\u5b8c\u3092\u4f75\u7528\u3057\u3001 ``transform`` \u3067\u306f\u524d\u65b9\u65b9\u5411\u306e\u5024\u3060\u3051\u3067\u88dc\u5b8c\u3057\u307e\u3059\u3002\n",
    "            \u30a8\u30a4\u30ea\u30a2\u30b9 ``ffill_train_bfill_in_fit`` \u3067\u3082\u6307\u5b9a\u53ef\u80fd\u3067\u3001\u63a8\u8ad6\u6642\u306e\u672a\u6765\u53c2\u7167\u3092\u9632\u304e\u3064\u3064\u5b66\u7fd2\u6642\u306b\u306f\u672b\u5c3e\u5024\u3092\u30a6\u30a9\u30fc\u30e0\u30b9\u30bf\u30fc\u30c8\u3067\u304d\u307e\u3059\u3002\n",
    "        - ``kalman_*`` \u3068 ``arima_auto`` \u30dd\u30ea\u30b7\u30fc\u306f statsmodels \u306e ``fittedvalues``\uff08\u30d5\u30a3\u30eb\u30bf\u306b\u3088\u308b\u4e00\u6b69\u5148\u63a8\u5b9a\uff09\u3060\u3051\u3092\u4f7f\u7528\u3057\u3001\u5c06\u6765\u306e\u5e73\u6ed1\u5024\u3092\u53c2\u7167\u3057\u307e\u305b\u3093\u3002\n",
    "    \"\"\"\n",
    "    _BASE_POLICIES: Tuple[str, ...] = (\n",
    "        \"ffill_bfill\",\n",
    "        \"ffill_only\",\n",
    "        \"rolling_median_k\",\n",
    "        \"rolling_mean_k\",\n",
    "        \"ema_alpha\",\n",
    "        \"linear_interp\",\n",
    "        \"spline_interp_deg\",\n",
    "        \"time_interp\",\n",
    "        \"backfill_robust\",\n",
    "        \"winsorized_median_k\",\n",
    "        \"quantile_fill\",\n",
    "        \"dow_median\",\n",
    "        \"dom_median\",\n",
    "        \"month_median\",\n",
    "        \"holiday_bridge\",\n",
    "        \"knn_k\",\n",
    "        \"pca_reconstruct_r\",\n",
    "        \"mice\",\n",
    "        \"missforest\",\n",
    "        \"ridge_stack\",\n",
    "        \"kalman_local_level\",\n",
    "        \"arima_auto\",\n",
    "        \"state_space_custom\",\n",
    "        \"mask_plus_mean\",\n",
    "        \"two_stage\",\n",
    "    )\n",
    "    POLICY_ALIASES: Dict[str, str] = {\n",
    "        \"ffill_train_bfill_in_fit\": \"ffill_bfill\",\n",
    "    }\n",
    "    SUPPORTED_POLICIES: Tuple[str, ...] = _BASE_POLICIES + tuple(POLICY_ALIASES.keys())\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        columns: Iterable[Hashable] | None = None,\n",
    "        policy: str = \"ffill_bfill\",\n",
    "        rolling_window: int = 5,\n",
    "        ema_alpha: float = 0.3,\n",
    "        calendar_column: str | None = None,\n",
    "        policy_params: Mapping[str, Any] | None = None,\n",
    "        random_state: int = 42,\n",
    "    ) -> None:\n",
    "        self.columns = columns\n",
    "        resolved_policy = self.POLICY_ALIASES.get(policy, policy)\n",
    "        self.policy_requested = policy\n",
    "        self.policy = resolved_policy\n",
    "        self.rolling_window = int(rolling_window)\n",
    "        self.ema_alpha = float(ema_alpha)\n",
    "        self.calendar_column = calendar_column\n",
    "        self.policy_params = policy_params\n",
    "        self._policy_params: Dict[str, Any] = dict(policy_params) if policy_params is not None else {}\n",
    "        self.random_state = int(random_state)\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):  # type: ignore[override]\n",
    "        X_df = self._ensure_dataframe(X).copy()\n",
    "        if self.columns is None:\n",
    "            cols: List[str] = [c for c in X_df.columns if isinstance(c, str) and c.startswith(\"M\")]\n",
    "        else:\n",
    "            cols = [c for c in self.columns if isinstance(c, str) and c in X_df.columns]\n",
    "        self.columns_ = cols\n",
    "        self.extra_columns_: List[str] = []\n",
    "        if not self.columns_:\n",
    "            self._train_index = X_df.index.copy()\n",
    "            self._train_filled_ = pd.DataFrame(index=X_df.index.copy())\n",
    "            self._state_ = {}\n",
    "            self._calendar_fit_values_ = None\n",
    "            self._output_columns_ = []\n",
    "            self._medians_dict_ = {}\n",
    "            return self\n",
    "        if self.policy not in self.SUPPORTED_POLICIES:\n",
    "            raise ValueError(f\"Unsupported policy '{self.policy}'. Supported: {list(self.SUPPORTED_POLICIES)}\")\n",
    "        if self.rolling_window <= 0:\n",
    "            raise ValueError(\"rolling_window must be positive\")\n",
    "        if not (0.0 < self.ema_alpha <= 1.0):\n",
    "            raise ValueError(\"ema_alpha must be in (0, 1]\")\n",
    "        calendar_series = self._extract_calendar_series(X_df)\n",
    "        self._calendar_column_name_ = calendar_series.name if calendar_series is not None else None\n",
    "        self._calendar_fit_values_ = calendar_series.copy() if calendar_series is not None else None\n",
    "        data = cast(pd.DataFrame, X_df.loc[:, self.columns_].copy())\n",
    "        medians_series = data.median(numeric_only=True)\n",
    "        if not isinstance(medians_series, pd.Series):  # pragma: no cover - defensive\n",
    "            raise TypeError(\"Expected pandas Series from DataFrame.median().\")\n",
    "        medians_series = medians_series.reindex(self.columns_)\n",
    "        medians_series = medians_series.fillna(0.0)\n",
    "        medians_dict: Dict[Hashable, float] = {}\n",
    "        for idx_label in medians_series.index:\n",
    "            idx_position = cast(int, medians_series.index.get_loc(idx_label))\n",
    "            medians_dict[idx_label] = float(cast(float, medians_series.iloc[idx_position]))\n",
    "        self._medians_dict_ = medians_dict\n",
    "        if self.policy == \"ffill_bfill\":\n",
    "            filled, state = self._fit_ffill(data, medians_dict, use_bfill=True)\n",
    "        elif self.policy == \"ffill_only\":\n",
    "            filled, state = self._fit_ffill(data, medians_dict, use_bfill=False)\n",
    "        elif self.policy in {\"rolling_median_k\", \"rolling_mean_k\"}:\n",
    "            filled, state = self._fit_rolling(\n",
    "                data,\n",
    "                use_median=self.policy == \"rolling_median_k\",\n",
    "                medians_lookup=medians_dict,\n",
    "            )\n",
    "            state[\"medians\"] = medians_dict\n",
    "        elif self.policy == \"ema_alpha\":\n",
    "            filled, state = self._fit_ema(data, medians_lookup=medians_dict)\n",
    "        elif self.policy == \"linear_interp\":\n",
    "            filled, state = self._fit_linear_interp(data, medians_dict)\n",
    "        elif self.policy == \"spline_interp_deg\":\n",
    "            filled, state = self._fit_spline_interp(data, medians_dict)\n",
    "        elif self.policy == \"time_interp\":\n",
    "            filled, state = self._fit_time_interp(data, medians_dict, calendar_series)\n",
    "        elif self.policy == \"backfill_robust\":\n",
    "            filled, state = self._fit_backfill_robust(data, medians_dict)\n",
    "        elif self.policy == \"winsorized_median_k\":\n",
    "            filled, state = self._fit_winsorized_median(data, medians_dict)\n",
    "        elif self.policy == \"quantile_fill\":\n",
    "            filled, state = self._fit_quantile_fill(data, medians_dict)\n",
    "        elif self.policy in {\"dow_median\", \"dom_median\", \"month_median\"}:\n",
    "            filled, state = self._fit_seasonal_median(data, medians_dict, calendar_series)\n",
    "        elif self.policy == \"holiday_bridge\":\n",
    "            filled, state = self._fit_holiday_bridge(data, medians_dict, calendar_series)\n",
    "        elif self.policy == \"knn_k\":\n",
    "            filled, state = self._fit_knn(data)\n",
    "        elif self.policy == \"pca_reconstruct_r\":\n",
    "            filled, state = self._fit_pca_reconstruct(data, medians_dict)\n",
    "        elif self.policy == \"mice\":\n",
    "            filled, state = self._fit_mice(data)\n",
    "        elif self.policy == \"missforest\":\n",
    "            filled, state = self._fit_missforest(data)\n",
    "        elif self.policy == \"ridge_stack\":\n",
    "            filled, state = self._fit_ridge_stack(data, medians_dict)\n",
    "        elif self.policy == \"kalman_local_level\":\n",
    "            filled, state = self._fit_kalman(data, medians_dict, level_only=True)\n",
    "        elif self.policy == \"arima_auto\":\n",
    "            filled, state = self._fit_arima_auto(data, medians_dict)\n",
    "        elif self.policy == \"state_space_custom\":\n",
    "            filled, state = self._fit_kalman(data, medians_dict, level_only=False)\n",
    "        elif self.policy == \"mask_plus_mean\":\n",
    "            filled, state = self._fit_mask_plus_mean(data, medians_dict)\n",
    "        elif self.policy == \"two_stage\":\n",
    "            filled, state = self._fit_two_stage(data, medians_dict)\n",
    "        else:  # pragma: no cover - safeguarded above\n",
    "            raise ValueError(self.policy)\n",
    "        state = self._prepare_state(state)\n",
    "        self.extra_columns_ = [c for c in filled.columns if c not in self.columns_]\n",
    "        self._train_index = X_df.index.copy()\n",
    "        self._train_filled_ = filled\n",
    "        self._state_ = state\n",
    "        self._output_columns_ = list(filled.columns)\n",
    "        return self\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):  # type: ignore[override]\n",
    "        self._validate_fitted()\n",
    "        X_df = self._ensure_dataframe(X)\n",
    "        if not self.columns_:\n",
    "            return X_df\n",
    "        df = X_df.copy()\n",
    "        if df.index.equals(self._train_index):\n",
    "            for col in self._output_columns_:\n",
    "                if col in df.columns:\n",
    "                    df.loc[:, col] = self._train_filled_.loc[:, col].values\n",
    "                else:\n",
    "                    df[col] = self._train_filled_.loc[:, col].values\n",
    "            return df\n",
    "        subset = cast(pd.DataFrame, df.loc[:, self.columns_])\n",
    "        expect_calendar = bool(getattr(self, \"_calendar_column_name_\", None))\n",
    "        calendar_series = self._extract_calendar_series(df, expect_existing=expect_calendar)\n",
    "        if self.policy == \"ffill_bfill\":\n",
    "            filled = self._transform_ffill(subset, use_bfill=True)\n",
    "        elif self.policy == \"ffill_only\":\n",
    "            filled = self._transform_ffill(subset, use_bfill=False)\n",
    "        elif self.policy in {\"rolling_median_k\", \"rolling_mean_k\"}:\n",
    "            filled = self._transform_rolling(subset, use_median=self.policy == \"rolling_median_k\")\n",
    "        elif self.policy == \"ema_alpha\":\n",
    "            filled = self._transform_ema(subset)\n",
    "        elif self.policy == \"linear_interp\":\n",
    "            filled = self._transform_linear_interp(subset)\n",
    "        elif self.policy == \"spline_interp_deg\":\n",
    "            filled = self._transform_spline_interp(subset)\n",
    "        elif self.policy == \"time_interp\":\n",
    "            filled = self._transform_time_interp(subset, calendar_series)\n",
    "        elif self.policy == \"backfill_robust\":\n",
    "            filled = self._transform_backfill_robust(subset)\n",
    "        elif self.policy == \"winsorized_median_k\":\n",
    "            filled = self._transform_winsorized_median(subset)\n",
    "        elif self.policy == \"quantile_fill\":\n",
    "            filled = self._transform_quantile_fill(subset)\n",
    "        elif self.policy in {\"dow_median\", \"dom_median\", \"month_median\"}:\n",
    "            filled = self._transform_seasonal_median(subset, calendar_series)\n",
    "        elif self.policy == \"holiday_bridge\":\n",
    "            filled = self._transform_holiday_bridge(subset, calendar_series)\n",
    "        elif self.policy == \"knn_k\":\n",
    "            filled = self._transform_knn(subset)\n",
    "        elif self.policy == \"pca_reconstruct_r\":\n",
    "            filled = self._transform_pca_reconstruct(subset)\n",
    "        elif self.policy == \"mice\":\n",
    "            filled = self._transform_mice(subset)\n",
    "        elif self.policy == \"missforest\":\n",
    "            filled = self._transform_missforest(subset)\n",
    "        elif self.policy == \"ridge_stack\":\n",
    "            filled = self._transform_ridge_stack(subset)\n",
    "        elif self.policy == \"kalman_local_level\":\n",
    "            filled = self._transform_kalman(subset, calendar_series)\n",
    "        elif self.policy == \"arima_auto\":\n",
    "            filled = self._transform_arima_auto(subset)\n",
    "        elif self.policy == \"state_space_custom\":\n",
    "            filled = self._transform_kalman(subset, calendar_series)\n",
    "        elif self.policy == \"mask_plus_mean\":\n",
    "            filled = self._transform_mask_plus_mean(subset)\n",
    "        elif self.policy == \"two_stage\":\n",
    "            filled = self._transform_two_stage(subset)\n",
    "        else:  # pragma: no cover - safeguarded in fit\n",
    "            raise ValueError(self.policy)\n",
    "        for col in self.columns_:\n",
    "            df.loc[:, col] = filled.loc[:, col].values\n",
    "        for col in self.extra_columns_:\n",
    "            df.loc[:, col] = filled.loc[:, col].values\n",
    "        return df\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is not None:\n",
    "            return np.array(input_features)\n",
    "        if hasattr(self, \"_output_columns_\") and self._output_columns_:\n",
    "            return np.array(self._output_columns_)\n",
    "        return np.array(self.columns_)\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _ensure_dataframe(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X\n",
    "        raise TypeError(\"MGroupImputer expects a pandas DataFrame as input.\")\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def _ensure_warning_list(state: Dict[str, Any]) -> List[str]:\n",
    "        warnings = state.get(\"warnings\")\n",
    "        if isinstance(warnings, list):\n",
    "            return warnings\n",
    "        warnings_list: List[str] = []\n",
    "        state[\"warnings\"] = warnings_list\n",
    "        return warnings_list\n",
    "\n",
    "    def _prepare_state(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        self._ensure_warning_list(state)\n",
    "        return state\n",
    "\n",
    "    def _record_warning(self, message: str) -> None:\n",
    "        if not hasattr(self, \"_state_\"):\n",
    "            return\n",
    "        warnings = self._state_.setdefault(\"warnings\", [])\n",
    "        if isinstance(warnings, list):\n",
    "            warnings.append(message)\n",
    "        else:  # pragma: no cover - defensive\n",
    "            self._state_[\"warnings\"] = [message]\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def _effective_history(frame: pd.DataFrame, requested_len: int) -> tuple[int, pd.DataFrame]:\n",
    "        if requested_len <= 0 or frame.empty:\n",
    "            return 0, frame.iloc[0:0].copy()\n",
    "        effective = min(requested_len, len(frame))\n",
    "        return effective, frame.tail(effective).copy()\n",
    "\n",
    "    def _empty_tail_frame(self) -> pd.DataFrame:\n",
    "        if not hasattr(self, \"columns_\") or not self.columns_:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame({col: pd.Series(dtype=float) for col in self.columns_})\n",
    "\n",
    "    def _state_tail_frame(self) -> pd.DataFrame:\n",
    "        stored = self._state_.get(\"tail\") if hasattr(self, \"_state_\") else None\n",
    "        if isinstance(stored, pd.DataFrame):\n",
    "            return stored\n",
    "        return self._empty_tail_frame()\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def _deque_median(values: Sequence[float]) -> float:\n",
    "        seq = list(values)\n",
    "        if not seq:\n",
    "            return float(\"nan\")\n",
    "        arr = np.asarray(seq, dtype=float)\n",
    "        return float(np.median(arr))\n",
    "\n",
    "    def _validate_fitted(self):\n",
    "        if not hasattr(self, \"_state_\"):\n",
    "            raise AttributeError(\"MGroupImputer is not fitted.\")\n",
    "    # \u30ed\u30fc\u30ea\u30f3\u30b0\u88dc\u5b8c\u7528\u306e\u5185\u90e8\u51e6\u7406 -------------------------------------------\n",
    "\n",
    "    def _fit_rolling(self, data: pd.DataFrame, *, use_median: bool, medians_lookup: dict[Hashable, float]):\n",
    "        filled = data.copy()\n",
    "        deques: dict[str, Deque[float]] = {col: deque(maxlen=self.rolling_window) for col in self.columns_}\n",
    "        for index_label in filled.index:\n",
    "            for col in self.columns_:\n",
    "                original_val = data.at[index_label, col]\n",
    "                if pd.isna(original_val):\n",
    "                    dq = deques[col]\n",
    "                    if dq:\n",
    "                        if use_median:\n",
    "                            fill_value = self._deque_median(dq)\n",
    "                        else:\n",
    "                            fill_value = float(np.mean(dq))\n",
    "                            if np.isfinite(fill_value):\n",
    "                                dq.append(fill_value)\n",
    "                    else:\n",
    "                        lookup = medians_lookup.get(col)\n",
    "                        fill_value = float(lookup) if lookup is not None else float(\"nan\")\n",
    "                        if not use_median and np.isfinite(fill_value):\n",
    "                            dq.append(fill_value)\n",
    "                    filled.at[index_label, col] = fill_value\n",
    "                else:\n",
    "                    fill_value = float(cast(float, original_val))\n",
    "                    filled.at[index_label, col] = fill_value\n",
    "                    deques[col].append(fill_value)\n",
    "        state = {\n",
    "            \"deques\": {col: deque(deques[col], maxlen=self.rolling_window) for col in self.columns_},\n",
    "            \"use_median\": use_median,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_rolling(self, data: pd.DataFrame, *, use_median: bool):\n",
    "        filled = data.copy()\n",
    "        state = cast(dict[str, Deque[float]], deepcopy(self._state_[\"deques\"]))\n",
    "        medians_lookup = cast(dict[Hashable, float], self._state_[\"medians\"])\n",
    "        for index_label in filled.index:\n",
    "            for col in self.columns_:\n",
    "                original_val = data.at[index_label, col]\n",
    "                dq = state[col]\n",
    "                if pd.isna(original_val):\n",
    "                    if dq:\n",
    "                        fill_value = self._deque_median(dq) if use_median else float(np.mean(dq))\n",
    "                    else:\n",
    "                        lookup = medians_lookup.get(col)\n",
    "                        fill_value = float(lookup) if lookup is not None else float(\"nan\")\n",
    "                    filled.at[index_label, col] = fill_value\n",
    "                    if not use_median:\n",
    "                        dq.append(fill_value)\n",
    "                else:\n",
    "                    fill_value = float(cast(float, original_val))\n",
    "                    filled.at[index_label, col] = fill_value\n",
    "                    dq.append(fill_value)\n",
    "        return filled\n",
    "    # EMA \u88dc\u5b8c\u7528\u306e\u5185\u90e8\u51e6\u7406 ------------------------------------------------\n",
    "\n",
    "    def _fit_ema(self, data: pd.DataFrame, *, medians_lookup: dict[Hashable, float]):\n",
    "        filled = data.copy()\n",
    "        ema_state: dict[str, float | None] = {col: None for col in self.columns_}\n",
    "        for index_label in filled.index:\n",
    "            for col in self.columns_:\n",
    "                val = filled.at[index_label, col]\n",
    "                ema_val = ema_state[col]\n",
    "                if pd.isna(val):\n",
    "                    if ema_val is not None:\n",
    "                        fill_value = ema_val\n",
    "                    else:\n",
    "                        lookup = medians_lookup.get(col)\n",
    "                        fill_value = float(lookup) if lookup is not None else float(\"nan\")\n",
    "                    filled.at[index_label, col] = fill_value\n",
    "                    ema_val = float(fill_value)\n",
    "                else:\n",
    "                    val_float = float(cast(float, val))\n",
    "                    ema_val = val_float if ema_val is None else self.ema_alpha * val_float + (1 - self.ema_alpha) * ema_val\n",
    "                    filled.at[index_label, col] = val_float\n",
    "                ema_state[col] = ema_val\n",
    "        state = {\n",
    "            \"ema\": ema_state,\n",
    "            \"medians\": medians_lookup.copy(),\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_ema(self, data: pd.DataFrame):\n",
    "        filled = data.copy()\n",
    "        ema_state = cast(dict[str, float | None], deepcopy(self._state_[\"ema\"]))\n",
    "        medians_lookup = cast(dict[Hashable, float], self._state_[\"medians\"])\n",
    "        for index_label in filled.index:\n",
    "            for col in self.columns_:\n",
    "                val = filled.at[index_label, col]\n",
    "                ema_val = ema_state[col]\n",
    "                if pd.isna(val):\n",
    "                    if ema_val is not None:\n",
    "                        fill_value = ema_val\n",
    "                    else:\n",
    "                        lookup = medians_lookup.get(col)\n",
    "                        fill_value = float(lookup) if lookup is not None else float(\"nan\")\n",
    "                    filled.at[index_label, col] = fill_value\n",
    "                    ema_val = float(fill_value)\n",
    "                else:\n",
    "                    val_float = float(cast(float, val))\n",
    "                    ema_val = val_float if ema_val is None else self.ema_alpha * val_float + (1 - self.ema_alpha) * ema_val\n",
    "                    filled.at[index_label, col] = val_float\n",
    "                ema_state[col] = ema_val\n",
    "        return filled\n",
    "    # \u5171\u901a\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3 --------------------------------------------------\n",
    "\n",
    "    def _extract_calendar_series(self, X_df: pd.DataFrame, expect_existing: bool = False) -> pd.Series | None:\n",
    "        \"\"\"\u30dd\u30ea\u30b7\u30fc\u304c\u5fc5\u8981\u3068\u3059\u308b\u5834\u5408\u306b\u65e5\u6642\u5217\u3092\u62bd\u51fa\u3059\u308b\u3002\"\"\"\n",
    "        col_name = getattr(self, \"_calendar_column_name_\", None)\n",
    "        if col_name is not None and col_name in X_df.columns:\n",
    "            return cast(pd.Series, X_df[col_name])\n",
    "        if col_name is not None and col_name not in X_df.columns:\n",
    "            if expect_existing:\n",
    "                raise KeyError(\n",
    "                    f\"Calendar column '{col_name}' required by policy '{self.policy}' is missing in transform input.\"\n",
    "                )\n",
    "            return None\n",
    "        if self.calendar_column and self.calendar_column in X_df.columns:\n",
    "            self._calendar_column_name_ = self.calendar_column\n",
    "            return cast(pd.Series, X_df[self.calendar_column])\n",
    "        if self.calendar_column and self.calendar_column not in X_df.columns:\n",
    "            if expect_existing:\n",
    "                raise KeyError(\n",
    "                    f\"Calendar column '{self.calendar_column}' not found in input DataFrame for policy '{self.policy}'.\"\n",
    "                )\n",
    "            return None\n",
    "        for candidate in (\"date\", \"date_id\", \"timestamp\", \"datetime\", \"evaluation_date\"):\n",
    "            if candidate in X_df.columns:\n",
    "                self._calendar_column_name_ = candidate\n",
    "                return cast(pd.Series, X_df[candidate])\n",
    "        if expect_existing:\n",
    "            raise KeyError(\n",
    "                \"Calendar column is required for the selected policy. Provide calendar_column explicitly or include a compatible column in the input.\"\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    def _get_policy_param(self, key: str, default: Any) -> Any:\n",
    "        return self._policy_params.get(key, default)\n",
    "\n",
    "    def _history_length(self) -> int:\n",
    "        raw = int(self._get_policy_param(\"history_length\", max(self.rolling_window, 2)))\n",
    "        return max(2, raw)\n",
    "\n",
    "    def _compute_scaler_stats(self, data: pd.DataFrame) -> tuple[Dict[str, float], Dict[str, float]]:\n",
    "        means: Dict[str, float] = {}\n",
    "        stds: Dict[str, float] = {}\n",
    "        for col in self.columns_:\n",
    "            series = data[col].astype(float)\n",
    "            values = series.to_numpy()\n",
    "            with np.errstate(all=\"ignore\"):\n",
    "                mean = float(np.nanmean(values))\n",
    "                std = float(np.nanstd(values))\n",
    "            if not np.isfinite(mean):\n",
    "                mean = 0.0\n",
    "            if not np.isfinite(std) or std < 1e-12:\n",
    "                std = 1.0\n",
    "            means[col] = mean\n",
    "            stds[col] = std\n",
    "        return means, stds\n",
    "\n",
    "    def _standardize_with_stats(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        means: Mapping[str, float],\n",
    "        stds: Mapping[str, float],\n",
    "    ) -> pd.DataFrame:\n",
    "        scaled = data.astype(float).copy()\n",
    "        for col in self.columns_:\n",
    "            mean = float(means.get(col, 0.0))\n",
    "            std = float(stds.get(col, 1.0))\n",
    "            if not np.isfinite(std) or std < 1e-12:\n",
    "                std = 1.0\n",
    "            scaled[col] = (scaled[col] - mean) / std\n",
    "        return scaled\n",
    "\n",
    "    def _destandardize_with_stats(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        means: Mapping[str, float],\n",
    "        stds: Mapping[str, float],\n",
    "    ) -> pd.DataFrame:\n",
    "        restored = data.astype(float).copy()\n",
    "        for col in self.columns_:\n",
    "            mean = float(means.get(col, 0.0))\n",
    "            std = float(stds.get(col, 1.0))\n",
    "            restored[col] = restored[col] * std + mean\n",
    "        return restored\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def _series_any(series: pd.Series) -> bool:\n",
    "        \"\"\"Return whether a boolean Series contains any True values.\"\"\"\n",
    "        return bool(np.any(series.to_numpy(dtype=bool, copy=False)))\n",
    "\n",
    "    def _calendar_to_datetime(self, series: pd.Series | None) -> pd.Series | None:\n",
    "        if series is None:\n",
    "            return None\n",
    "        if pd.api.types.is_datetime64_any_dtype(series):\n",
    "            return series\n",
    "        origin = self._get_policy_param(\"calendar_origin\", None)\n",
    "        unit = self._get_policy_param(\"calendar_unit\", \"D\")\n",
    "        try:\n",
    "            if origin is not None:\n",
    "                return pd.to_datetime(series, unit=unit, origin=origin)\n",
    "            return pd.to_datetime(series)\n",
    "        except Exception as exc:  # pragma: no cover - defensive\n",
    "            raise ValueError(\"Failed to convert calendar column to datetime. Consider specifying calendar_origin and calendar_unit.\") from exc\n",
    "    # \u30d5\u30a3\u30eb\u524d\u5f8c\u51e6\u7406 -------------------------------------------------------\n",
    "\n",
    "    def _fit_ffill(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float], *, use_bfill: bool):\n",
    "        filled = data.ffill()\n",
    "        if use_bfill:\n",
    "            filled = filled.bfill()\n",
    "        filled = filled.fillna(pd.Series(medians_lookup))\n",
    "        last_values: Dict[str, float] = {}\n",
    "        for col in self.columns_:\n",
    "            series = filled[col].dropna()\n",
    "            if not series.empty:\n",
    "                last_values[col] = float(series.iloc[-1])\n",
    "        state = {\n",
    "            \"last\": last_values,\n",
    "            \"medians\": medians_lookup,\n",
    "            \"use_bfill\": use_bfill,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_ffill(self, data: pd.DataFrame, *, use_bfill: bool):\n",
    "        \"\"\"Apply forward-only fills during transform (``use_bfill`` kept for compatibility).\"\"\"\n",
    "        filled = data.ffill()\n",
    "        last_values = cast(Dict[str, float], self._state_.get(\"last\", {}))\n",
    "        if last_values:\n",
    "            filled = filled.fillna(last_values)\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        filled = filled.fillna(pd.Series(medians_lookup))\n",
    "        return filled\n",
    "    # \u7dda\u5f62/\u30b9\u30d7\u30e9\u30a4\u30f3/\u6642\u9593\u88dc\u9593 --------------------------------------------\n",
    "\n",
    "    def _fit_linear_interp(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float]):\n",
    "        requested_history = self._history_length()\n",
    "        filled = data.interpolate(method=\"linear\", limit_direction=\"forward\")\n",
    "        filled = filled.ffill()\n",
    "        filled = filled.fillna(pd.Series(medians_lookup))\n",
    "        history_len, tail = self._effective_history(filled, requested_history)\n",
    "        state = {\n",
    "            \"history_len\": history_len,\n",
    "            \"tail\": tail,\n",
    "            \"medians\": medians_lookup,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_linear_interp(self, data: pd.DataFrame):\n",
    "        history_len = cast(int, self._state_.get(\"history_len\", self._history_length()))\n",
    "        tail = self._state_tail_frame()\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        if len(tail) != history_len:\n",
    "            raise RuntimeError(f\"history_len mismatch in linear_interp: tail={len(tail)} expected={history_len}\")\n",
    "        combined = pd.concat([tail, data], axis=0)\n",
    "        combined = combined.interpolate(method=\"linear\", limit_direction=\"forward\")\n",
    "        combined = combined.ffill()\n",
    "        combined = combined.fillna(pd.Series(medians_lookup))\n",
    "        result = combined.iloc[history_len:].copy()\n",
    "        result.index = data.index\n",
    "        new_tail = combined.tail(history_len).copy()\n",
    "        self._state_[\"tail\"] = new_tail\n",
    "        self._state_[\"history_len\"] = len(new_tail)\n",
    "        return result\n",
    "\n",
    "    def _fit_spline_interp(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float]):\n",
    "        order = int(self._get_policy_param(\"spline_degree\", 3))\n",
    "        requested_history = self._history_length()\n",
    "        try:\n",
    "            filled = data.interpolate(method=\"spline\", order=order, limit_direction=\"forward\")\n",
    "        except ValueError as exc:  # pragma: no cover - SciPy \u672a\u5c0e\u5165\u306a\u3069\n",
    "            raise RuntimeError(\"spline_interp_deg policy requires scipy to be installed.\") from exc\n",
    "        filled = filled.ffill()\n",
    "        filled = filled.fillna(pd.Series(medians_lookup))\n",
    "        history_len, tail = self._effective_history(filled, requested_history)\n",
    "        state = {\n",
    "            \"history_len\": history_len,\n",
    "            \"tail\": tail,\n",
    "            \"medians\": medians_lookup,\n",
    "            \"order\": order,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_spline_interp(self, data: pd.DataFrame):\n",
    "        history_len = cast(int, self._state_.get(\"history_len\", self._history_length()))\n",
    "        tail = self._state_tail_frame()\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        order = int(self._state_.get(\"order\", 3))\n",
    "        if len(tail) != history_len:\n",
    "            raise RuntimeError(f\"history_len mismatch in spline_interp: tail={len(tail)} expected={history_len}\")\n",
    "        combined = pd.concat([tail, data], axis=0)\n",
    "        try:\n",
    "            combined = combined.interpolate(method=\"spline\", order=order, limit_direction=\"forward\")\n",
    "        except ValueError as exc:  # pragma: no cover\n",
    "            raise RuntimeError(\"spline_interp_deg policy requires scipy during transform as well.\") from exc\n",
    "        combined = combined.ffill()\n",
    "        combined = combined.fillna(pd.Series(medians_lookup))\n",
    "        result = combined.iloc[history_len:].copy()\n",
    "        result.index = data.index\n",
    "        new_tail = combined.tail(history_len).copy()\n",
    "        self._state_[\"tail\"] = new_tail\n",
    "        self._state_[\"history_len\"] = len(new_tail)\n",
    "        return result\n",
    "\n",
    "    def _fit_time_interp(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        medians_lookup: Dict[Hashable, float],\n",
    "        calendar_series: pd.Series | None,\n",
    "    ):\n",
    "        calendar_dt = self._calendar_to_datetime(calendar_series)\n",
    "        if calendar_dt is None:\n",
    "            raise ValueError(\"time_interp policy requires a datetime-like calendar column.\")\n",
    "        requested_history = self._history_length()\n",
    "        working = data.copy()\n",
    "        working.index = pd.DatetimeIndex(calendar_dt)\n",
    "        filled = working.interpolate(method=\"time\", limit_direction=\"forward\")\n",
    "        filled = filled.ffill()\n",
    "        filled = filled.fillna(pd.Series(medians_lookup))\n",
    "        history_len, tail = self._effective_history(filled, requested_history)\n",
    "        tail_calendar = calendar_dt.tail(history_len).copy() if history_len else calendar_dt.iloc[0:0].copy()\n",
    "        filled_reset = filled.copy()\n",
    "        filled_reset.index = data.index\n",
    "        state = {\n",
    "            \"history_len\": history_len,\n",
    "            \"tail\": tail,\n",
    "            \"tail_calendar\": tail_calendar,\n",
    "            \"medians\": medians_lookup,\n",
    "        }\n",
    "        return filled_reset, state\n",
    "\n",
    "    def _transform_time_interp(self, data: pd.DataFrame, calendar_series: pd.Series | None):\n",
    "        calendar_dt = self._calendar_to_datetime(calendar_series)\n",
    "        if calendar_dt is None:\n",
    "            raise ValueError(\"time_interp policy requires a datetime-like calendar column during transform.\")\n",
    "        history_len = cast(int, self._state_.get(\"history_len\", self._history_length()))\n",
    "        tail = self._state_tail_frame()\n",
    "        tail_calendar = cast(pd.Series, self._state_.get(\"tail_calendar\", pd.Series(dtype=\"datetime64[ns]\")))\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        if len(tail) != history_len:\n",
    "            raise RuntimeError(f\"history_len mismatch in time_interp: tail={len(tail)} expected={history_len}\")\n",
    "        if len(tail_calendar) != history_len:\n",
    "            raise RuntimeError(\n",
    "                f\"history_len mismatch in time_interp calendar: tail_calendar={len(tail_calendar)} expected={history_len}\"\n",
    "            )\n",
    "        combined = pd.concat([tail, data], axis=0)\n",
    "        combined_calendar = pd.concat([tail_calendar, calendar_dt], axis=0)\n",
    "        combined.index = pd.DatetimeIndex(pd.to_datetime(combined_calendar))\n",
    "        combined = combined.interpolate(method=\"time\", limit_direction=\"forward\")\n",
    "        combined = combined.ffill()\n",
    "        combined = combined.fillna(pd.Series(medians_lookup))\n",
    "        result = combined.iloc[history_len:].copy()\n",
    "        result.index = data.index\n",
    "        new_tail = combined.tail(history_len).copy()\n",
    "        new_tail_calendar = (\n",
    "            combined_calendar.tail(history_len).copy() if history_len else combined_calendar.iloc[0:0].copy()\n",
    "        )\n",
    "        self._state_[\"tail\"] = new_tail\n",
    "        self._state_[\"tail_calendar\"] = new_tail_calendar\n",
    "        self._state_[\"history_len\"] = len(new_tail)\n",
    "        return result\n",
    "    # \u30ed\u30d0\u30b9\u30c8/\u5206\u4f4d\u7cfb ----------------------------------------------------\n",
    "\n",
    "    def _fit_backfill_robust(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float]):\n",
    "        deques: Dict[str, Deque[float]] = {col: deque(maxlen=self.rolling_window) for col in self.columns_}\n",
    "        last_valid: Dict[str, float | None] = {col: None for col in self.columns_}\n",
    "        fallback = data.copy()\n",
    "        for index_label in data.index:\n",
    "            for col in self.columns_:\n",
    "                original = data.at[index_label, col]\n",
    "                if pd.isna(original):\n",
    "                    dq = deques[col]\n",
    "                    if dq:\n",
    "                        fallback_val = self._deque_median(dq)\n",
    "                    else:\n",
    "                        fallback_val = float(medians_lookup.get(col, np.nan))\n",
    "                    fallback.at[index_label, col] = fallback_val\n",
    "                else:\n",
    "                    valf = float(cast(float, original))\n",
    "                    fallback.at[index_label, col] = valf\n",
    "                    deques[col].append(valf)\n",
    "                    last_valid[col] = valf\n",
    "        filled = fallback.copy()\n",
    "        requested_history = self._history_length()\n",
    "        history_len, tail = self._effective_history(filled, requested_history)\n",
    "        state = {\n",
    "            \"medians\": medians_lookup,\n",
    "            \"deques\": {col: deque(deques[col], maxlen=self.rolling_window) for col in self.columns_},\n",
    "            \"last_valid\": last_valid,\n",
    "            \"history_len\": history_len,\n",
    "            \"tail\": tail,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_backfill_robust(self, data: pd.DataFrame):\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        stored_deques = cast(Dict[str, Deque[float]], self._state_.get(\"deques\", {}))\n",
    "        base_deques = {col: deque(stored_deques.get(col, deque()), maxlen=self.rolling_window) for col in self.columns_}\n",
    "        last_valid = cast(Dict[str, float | None], self._state_.get(\"last_valid\", {}))\n",
    "        fallback = data.copy()\n",
    "        for col in self.columns_:\n",
    "            last_val = last_valid.get(col)\n",
    "            if last_val is not None:\n",
    "                base_deques[col].append(last_val)\n",
    "        for index_label in data.index:\n",
    "            for col in self.columns_:\n",
    "                original = data.at[index_label, col]\n",
    "                if pd.isna(original):\n",
    "                    dq = base_deques[col]\n",
    "                    if dq:\n",
    "                        fallback_val = self._deque_median(dq)\n",
    "                    else:\n",
    "                        fallback_val = float(medians_lookup.get(col, np.nan))\n",
    "                    fallback.at[index_label, col] = fallback_val\n",
    "                else:\n",
    "                    valf = float(cast(float, original))\n",
    "                    fallback.at[index_label, col] = valf\n",
    "                    base_deques[col].append(valf)\n",
    "        filled = fallback.copy()\n",
    "        for col in self.columns_:\n",
    "            series = filled[col]\n",
    "            if not series.empty:\n",
    "                last_valid[col] = float(series.iloc[-1])\n",
    "        self._state_[\"last_valid\"] = last_valid\n",
    "        self._state_[\"deques\"] = {col: deque(base_deques[col], maxlen=self.rolling_window) for col in self.columns_}\n",
    "        history_len = cast(int, self._state_.get(\"history_len\", self._history_length()))\n",
    "        history_len = min(history_len, len(filled)) if len(filled) else 0\n",
    "        tail_updated = filled.tail(history_len).copy() if history_len else filled.iloc[0:0].copy()\n",
    "        self._state_[\"tail\"] = tail_updated\n",
    "        self._state_[\"history_len\"] = len(tail_updated)\n",
    "        return filled\n",
    "\n",
    "    def _winsorized_stat(self, values: Sequence[float], clip: float) -> float:\n",
    "        arr = np.asarray(list(values), dtype=float)\n",
    "        if arr.size == 0:\n",
    "            return float(\"nan\")\n",
    "        lower = float(np.quantile(arr, clip))\n",
    "        upper = float(np.quantile(arr, 1.0 - clip))\n",
    "        clipped = np.clip(arr, lower, upper)\n",
    "        return float(np.median(clipped))\n",
    "\n",
    "    def _fit_winsorized_median(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float]):\n",
    "        clip = float(self._get_policy_param(\"winsor_clip\", 0.1))\n",
    "        clip = min(max(0.0, clip), 0.49)\n",
    "        deques: Dict[str, Deque[float]] = {col: deque(maxlen=self.rolling_window) for col in self.columns_}\n",
    "        filled = data.copy()\n",
    "        for index_label in data.index:\n",
    "            for col in self.columns_:\n",
    "                original = data.at[index_label, col]\n",
    "                if pd.isna(original):\n",
    "                    dq = deques[col]\n",
    "                    if dq:\n",
    "                        fill_value = self._winsorized_stat(dq, clip)\n",
    "                    else:\n",
    "                        fill_value = float(medians_lookup.get(col, np.nan))\n",
    "                    filled.at[index_label, col] = fill_value\n",
    "                    deques[col].append(fill_value)\n",
    "                else:\n",
    "                    valf = float(cast(float, original))\n",
    "                    filled.at[index_label, col] = valf\n",
    "                    deques[col].append(valf)\n",
    "        state = {\n",
    "            \"medians\": medians_lookup,\n",
    "            \"deques\": {col: deque(deques[col], maxlen=self.rolling_window) for col in self.columns_},\n",
    "            \"clip\": clip,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_winsorized_median(self, data: pd.DataFrame):\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        clip = float(self._state_.get(\"clip\", 0.1))\n",
    "        stored_deques = cast(Dict[str, Deque[float]], self._state_.get(\"deques\", {}))\n",
    "        deques = {col: deque(stored_deques.get(col, deque()), maxlen=self.rolling_window) for col in self.columns_}\n",
    "        filled = data.copy()\n",
    "        for index_label in data.index:\n",
    "            for col in self.columns_:\n",
    "                original = data.at[index_label, col]\n",
    "                if pd.isna(original):\n",
    "                    dq = deques[col]\n",
    "                    if dq:\n",
    "                        fill_value = self._winsorized_stat(dq, clip)\n",
    "                    else:\n",
    "                        fill_value = float(medians_lookup.get(col, np.nan))\n",
    "                    filled.at[index_label, col] = fill_value\n",
    "                    deques[col].append(fill_value)\n",
    "                else:\n",
    "                    valf = float(cast(float, original))\n",
    "                    filled.at[index_label, col] = valf\n",
    "                    deques[col].append(valf)\n",
    "        self._state_[\"deques\"] = {col: deque(deques[col], maxlen=self.rolling_window) for col in self.columns_}\n",
    "        return filled\n",
    "\n",
    "    def _fit_seasonal_median(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        medians_lookup: Dict[Hashable, float],\n",
    "        calendar_series: pd.Series | None,\n",
    "    ):\n",
    "        calendar_dt = self._calendar_to_datetime(calendar_series)\n",
    "        if calendar_dt is None:\n",
    "            raise ValueError(f\"{self.policy} policy requires a datetime-like calendar column.\")\n",
    "        if self.policy == \"dow_median\":\n",
    "            keys = calendar_dt.dt.dayofweek\n",
    "        elif self.policy == \"dom_median\":\n",
    "            keys = calendar_dt.dt.day\n",
    "        else:  # month_median\n",
    "            keys = calendar_dt.dt.month\n",
    "        stats: Dict[str, Dict[int, float]] = {}\n",
    "        filled = data.copy()\n",
    "        for col in self.columns_:\n",
    "            frame = pd.DataFrame({\"key\": keys, \"value\": data[col]})\n",
    "            grouped = frame.groupby(\"key\")[\"value\"].agg([\"median\", \"count\"])\n",
    "            stats_for_col: Dict[int, float] = {}\n",
    "            fallback = float(medians_lookup.get(col, np.nan))\n",
    "            for idx, row in grouped.iterrows():\n",
    "                idx_key = int(cast(int, idx))\n",
    "                count = int(row[\"count\"])\n",
    "                median_raw = row[\"median\"]\n",
    "                if isinstance(median_raw, (int, float, np.floating)):\n",
    "                    median_val = float(median_raw)\n",
    "                else:\n",
    "                    median_val = fallback\n",
    "                if count >= 2 and not np.isnan(median_val):\n",
    "                    stats_for_col[idx_key] = median_val\n",
    "                else:\n",
    "                    stats_for_col[idx_key] = fallback\n",
    "            stats[col] = stats_for_col\n",
    "            fill_map = keys.map(stats_for_col)\n",
    "            col_series = data[col].copy()\n",
    "            col_series = col_series.fillna(fill_map)\n",
    "            col_series = col_series.fillna(medians_lookup.get(col, np.nan))\n",
    "            filled[col] = col_series.astype(float)\n",
    "        state = {\n",
    "            \"medians\": medians_lookup,\n",
    "            \"seasonal_stats\": stats,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_seasonal_median(self, data: pd.DataFrame, calendar_series: pd.Series | None):\n",
    "        calendar_dt = self._calendar_to_datetime(calendar_series)\n",
    "        if calendar_dt is None:\n",
    "            raise ValueError(f\"{self.policy} policy requires a datetime-like calendar column during transform.\")\n",
    "        if self.policy == \"dow_median\":\n",
    "            keys = calendar_dt.dt.dayofweek\n",
    "        elif self.policy == \"dom_median\":\n",
    "            keys = calendar_dt.dt.day\n",
    "        else:\n",
    "            keys = calendar_dt.dt.month\n",
    "        stats = cast(Dict[str, Dict[int, float]], self._state_.get(\"seasonal_stats\", {}))\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        filled = data.copy()\n",
    "        for col in self.columns_:\n",
    "            col_series = data[col].copy()\n",
    "            mapping = stats.get(col, {})\n",
    "            fill_map = keys.map(mapping)\n",
    "            col_series = col_series.fillna(fill_map)\n",
    "            col_series = col_series.fillna(medians_lookup.get(col, np.nan))\n",
    "            filled[col] = col_series.astype(float)\n",
    "        return filled\n",
    "\n",
    "    def _apply_holiday_bridge(\n",
    "        self,\n",
    "        frame: pd.DataFrame,\n",
    "        medians_lookup: Dict[Hashable, float],\n",
    "        window: int,\n",
    "        *,\n",
    "        online_start: int | None = None,\n",
    "    ):\n",
    "        bridged = frame.copy()\n",
    "        for col in self.columns_:\n",
    "            series = bridged[col].copy()\n",
    "            values = series.values\n",
    "            n = len(values)\n",
    "            i = 0\n",
    "            while i < n:\n",
    "                if not np.isnan(values[i]):\n",
    "                    i += 1\n",
    "                    continue\n",
    "                start = i\n",
    "                while i < n and np.isnan(values[i]):\n",
    "                    i += 1\n",
    "                end = i\n",
    "                length = end - start\n",
    "                prev_idx = start - 1\n",
    "                next_idx = end\n",
    "                prev_val = values[prev_idx] if prev_idx >= 0 and not np.isnan(values[prev_idx]) else None\n",
    "                next_val = None\n",
    "                if next_idx < n and not np.isnan(values[next_idx]):\n",
    "                    if online_start is None or next_idx < online_start:\n",
    "                        next_val = values[next_idx]\n",
    "                if prev_val is not None and next_val is not None and length <= window:\n",
    "                    fill_val = float((prev_val + next_val) / 2.0)\n",
    "                    values[start:end] = fill_val\n",
    "                else:\n",
    "                    segment = series.iloc[max(0, start - self.rolling_window) : start].dropna()\n",
    "                    if not segment.empty:\n",
    "                        fill_val = float(segment.median())\n",
    "                    elif prev_val is not None:\n",
    "                        fill_val = float(prev_val)\n",
    "                    elif next_val is not None:\n",
    "                        fill_val = float(next_val)\n",
    "                    else:\n",
    "                        fill_val = float(medians_lookup.get(col, np.nan))\n",
    "                    values[start:end] = fill_val\n",
    "            bridged[col] = values\n",
    "        return bridged\n",
    "\n",
    "    def _fit_holiday_bridge(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        medians_lookup: Dict[Hashable, float],\n",
    "        calendar_series: pd.Series | None,\n",
    "    ):\n",
    "        if self._calendar_to_datetime(calendar_series) is None:\n",
    "            raise ValueError(\"holiday_bridge policy requires a datetime-like calendar column.\")\n",
    "        window = int(self._get_policy_param(\"holiday_window\", 2))\n",
    "        requested_history = self._history_length()\n",
    "        bridged = self._apply_holiday_bridge(data, medians_lookup, window)\n",
    "        history_len, tail = self._effective_history(bridged, requested_history)\n",
    "        state = {\n",
    "            \"medians\": medians_lookup,\n",
    "            \"window\": window,\n",
    "            \"history_len\": history_len,\n",
    "            \"tail\": tail,\n",
    "        }\n",
    "        return bridged, state\n",
    "\n",
    "    def _transform_holiday_bridge(self, data: pd.DataFrame, calendar_series: pd.Series | None):\n",
    "        if self._calendar_to_datetime(calendar_series) is None:\n",
    "            raise ValueError(\"holiday_bridge policy requires a datetime-like calendar column during transform.\")\n",
    "        window = int(self._state_.get(\"window\", 2))\n",
    "        history_len = cast(int, self._state_.get(\"history_len\", self._history_length()))\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        tail = self._state_tail_frame()\n",
    "        if len(tail) != history_len:\n",
    "            raise RuntimeError(f\"history_len mismatch in holiday_bridge: tail={len(tail)} expected={history_len}\")\n",
    "        combined = pd.concat([tail, data], axis=0)\n",
    "        bridged = self._apply_holiday_bridge(combined, medians_lookup, window, online_start=len(tail))\n",
    "        result = bridged.iloc[history_len:].copy()\n",
    "        result.index = data.index\n",
    "        new_tail = bridged.tail(history_len).copy()\n",
    "        self._state_[\"tail\"] = new_tail\n",
    "        self._state_[\"history_len\"] = len(new_tail)\n",
    "        return result\n",
    "    # \u591a\u5909\u91cf\u88dc\u5b8c ----------------------------------------------------------\n",
    "\n",
    "    def _fit_knn(self, data: pd.DataFrame):\n",
    "        n_neighbors = int(self._get_policy_param(\"knn_neighbors\", min(5, len(data))))\n",
    "        n_neighbors = max(1, n_neighbors)\n",
    "        means, stds = self._compute_scaler_stats(data)\n",
    "        scaled = self._standardize_with_stats(data, means, stds)\n",
    "        available_cols: List[str] = []\n",
    "        for col in self.columns_:\n",
    "            notna_series = cast(pd.Series, data[col].notna())\n",
    "            if self._series_any(notna_series):\n",
    "                available_cols.append(col)\n",
    "        missing_cols = [col for col in self.columns_ if col not in available_cols]\n",
    "        if not available_cols:\n",
    "            # No informative columns => fall back to medians.\n",
    "            medians_series = pd.Series({col: self._medians_dict_.get(col, 0.0) for col in self.columns_})\n",
    "            filled = data.fillna(medians_series)\n",
    "            state = {\n",
    "                \"imputer\": None,\n",
    "                \"scaler_means\": means,\n",
    "                \"scaler_stds\": stds,\n",
    "                \"available_cols\": available_cols,\n",
    "                \"missing_cols\": missing_cols,\n",
    "                \"medians\": self._medians_dict_,\n",
    "            }\n",
    "            return filled, state\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors, weights=\"distance\")\n",
    "        scaled_subset = scaled.loc[:, available_cols]\n",
    "        filled_array = imputer.fit_transform(scaled_subset)\n",
    "        filled_scaled_subset = pd.DataFrame(\n",
    "            filled_array,\n",
    "            columns=pd.Index(available_cols),\n",
    "            index=data.index,\n",
    "        )\n",
    "        filled_scaled = scaled.copy()\n",
    "        filled_scaled.loc[:, available_cols] = filled_scaled_subset\n",
    "        filled = self._destandardize_with_stats(filled_scaled, means, stds)\n",
    "        medians_lookup = self._medians_dict_\n",
    "        for col in missing_cols:\n",
    "            median_val = float(medians_lookup.get(col, 0.0))\n",
    "            mask_missing = cast(pd.Series, filled[col].isna())\n",
    "            if self._series_any(mask_missing):\n",
    "                filled.loc[mask_missing, col] = median_val\n",
    "        for col in self.columns_:\n",
    "            mask_series = cast(pd.Series, data[col].isna()).astype(bool)\n",
    "            if self._series_any(~mask_series):\n",
    "                filled.loc[~mask_series, col] = data.loc[~mask_series, col].astype(float)\n",
    "        state = {\n",
    "            \"imputer\": imputer,\n",
    "            \"scaler_means\": means,\n",
    "            \"scaler_stds\": stds,\n",
    "            \"available_cols\": available_cols,\n",
    "            \"missing_cols\": missing_cols,\n",
    "            \"medians\": medians_lookup,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_knn(self, data: pd.DataFrame):\n",
    "        imputer = cast(KNNImputer, self._state_.get(\"imputer\"))\n",
    "        means = cast(Dict[str, float], self._state_.get(\"scaler_means\", {}))\n",
    "        stds = cast(Dict[str, float], self._state_.get(\"scaler_stds\", {}))\n",
    "        available_cols = cast(List[str], self._state_.get(\"available_cols\", []))\n",
    "        missing_cols = cast(List[str], self._state_.get(\"missing_cols\", []))\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        scaled = self._standardize_with_stats(data, means, stds)\n",
    "        if imputer is not None and available_cols:\n",
    "            scaled_subset = scaled.loc[:, available_cols]\n",
    "            filled_array = imputer.transform(scaled_subset)\n",
    "            filled_scaled_subset = pd.DataFrame(\n",
    "                filled_array,\n",
    "                columns=pd.Index(available_cols),\n",
    "                index=data.index,\n",
    "            )\n",
    "            scaled.loc[:, available_cols] = filled_scaled_subset\n",
    "        filled = self._destandardize_with_stats(scaled, means, stds)\n",
    "        for col in missing_cols:\n",
    "            median_val = float(medians_lookup.get(col, 0.0))\n",
    "            mask_missing = cast(pd.Series, filled[col].isna())\n",
    "            if self._series_any(mask_missing):\n",
    "                filled.loc[mask_missing, col] = median_val\n",
    "        for col in self.columns_:\n",
    "            mask_series = cast(pd.Series, data[col].isna()).astype(bool)\n",
    "            if self._series_any(~mask_series):\n",
    "                filled.loc[~mask_series, col] = data.loc[~mask_series, col].astype(float)\n",
    "        return filled\n",
    "\n",
    "    def _fit_pca_reconstruct(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float]):\n",
    "        n_features = max(1, len(self.columns_))\n",
    "        default_components = max(1, min(n_features - 1, n_features // 2)) if n_features > 1 else 1\n",
    "        components = int(self._get_policy_param(\"pca_components\", default_components))\n",
    "        components = max(1, min(components, n_features))\n",
    "        medians_series = pd.Series(medians_lookup)\n",
    "        filled_reference = data.fillna(medians_series)\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        pca = PCA(n_components=components, random_state=rng)\n",
    "        pca.fit(filled_reference.values)\n",
    "        reconstructed = pca.inverse_transform(pca.transform(filled_reference.values))\n",
    "        recon_df = pd.DataFrame(reconstructed, columns=data.columns, index=data.index)\n",
    "        filled = data.copy()\n",
    "        for col in self.columns_:\n",
    "            mask = data[col].isna()\n",
    "            filled.loc[mask, col] = recon_df.loc[mask, col]\n",
    "            filled.loc[~mask, col] = data.loc[~mask, col].astype(float)\n",
    "        state = {\n",
    "            \"pca\": pca,\n",
    "            \"medians\": medians_lookup,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_pca_reconstruct(self, data: pd.DataFrame):\n",
    "        pca = cast(PCA, self._state_.get(\"pca\"))\n",
    "        if pca is None:\n",
    "            raise RuntimeError(\"PCA model missing; ensure fit was called.\")\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        medians_series = pd.Series(medians_lookup)\n",
    "        filled_reference = data.fillna(medians_series)\n",
    "        reconstructed = pca.inverse_transform(pca.transform(filled_reference.values))\n",
    "        recon_df = pd.DataFrame(reconstructed, columns=data.columns, index=data.index)\n",
    "        filled = data.copy()\n",
    "        for col in self.columns_:\n",
    "            mask = data[col].isna()\n",
    "            filled.loc[mask, col] = recon_df.loc[mask, col]\n",
    "            filled.loc[~mask, col] = data.loc[~mask, col].astype(float)\n",
    "        return filled\n",
    "\n",
    "    def _fit_mice(self, data: pd.DataFrame):\n",
    "        max_iter = int(self._get_policy_param(\"mice_max_iter\", 10))\n",
    "        means, stds = self._compute_scaler_stats(data)\n",
    "        scaled = self._standardize_with_stats(data, means, stds)\n",
    "        available_cols: List[str] = []\n",
    "        for col in self.columns_:\n",
    "            notna_series = cast(pd.Series, data[col].notna())\n",
    "            if self._series_any(notna_series):\n",
    "                available_cols.append(col)\n",
    "        missing_cols = [col for col in self.columns_ if col not in available_cols]\n",
    "        if not available_cols:\n",
    "            medians_series = pd.Series({col: self._medians_dict_.get(col, 0.0) for col in self.columns_})\n",
    "            filled = data.fillna(medians_series)\n",
    "            state = {\n",
    "                \"imputer\": None,\n",
    "                \"scaler_means\": means,\n",
    "                \"scaler_stds\": stds,\n",
    "                \"available_cols\": available_cols,\n",
    "                \"missing_cols\": missing_cols,\n",
    "                \"medians\": self._medians_dict_,\n",
    "            }\n",
    "            return filled, state\n",
    "        scaled_subset = scaled.loc[:, available_cols]\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        imputer = IterativeImputer(random_state=rng, max_iter=max_iter, sample_posterior=False)\n",
    "        filled_array = imputer.fit_transform(scaled_subset)\n",
    "        filled_scaled_subset = pd.DataFrame(\n",
    "            filled_array,\n",
    "            columns=pd.Index(available_cols),\n",
    "            index=data.index,\n",
    "        )\n",
    "        filled_scaled = scaled.copy()\n",
    "        filled_scaled.loc[:, available_cols] = filled_scaled_subset\n",
    "        filled = self._destandardize_with_stats(filled_scaled, means, stds)\n",
    "        medians_lookup = self._medians_dict_\n",
    "        for col in missing_cols:\n",
    "            median_val = float(medians_lookup.get(col, 0.0))\n",
    "            mask_missing = cast(pd.Series, filled[col].isna())\n",
    "            if self._series_any(mask_missing):\n",
    "                filled.loc[mask_missing, col] = median_val\n",
    "        for col in self.columns_:\n",
    "            mask_series = cast(pd.Series, data[col].isna()).astype(bool)\n",
    "            if self._series_any(~mask_series):\n",
    "                filled.loc[~mask_series, col] = data.loc[~mask_series, col].astype(float)\n",
    "        state = {\n",
    "            \"imputer\": imputer,\n",
    "            \"scaler_means\": means,\n",
    "            \"scaler_stds\": stds,\n",
    "            \"available_cols\": available_cols,\n",
    "            \"missing_cols\": missing_cols,\n",
    "            \"medians\": medians_lookup,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_mice(self, data: pd.DataFrame):\n",
    "        imputer = cast(IterativeImputer, self._state_.get(\"imputer\"))\n",
    "        means = cast(Dict[str, float], self._state_.get(\"scaler_means\", {}))\n",
    "        stds = cast(Dict[str, float], self._state_.get(\"scaler_stds\", {}))\n",
    "        available_cols = cast(List[str], self._state_.get(\"available_cols\", []))\n",
    "        missing_cols = cast(List[str], self._state_.get(\"missing_cols\", []))\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        scaled = self._standardize_with_stats(data, means, stds)\n",
    "        if imputer is not None and available_cols:\n",
    "            scaled_subset = scaled.loc[:, available_cols]\n",
    "            filled_array = imputer.transform(scaled_subset)\n",
    "            filled_scaled_subset = pd.DataFrame(\n",
    "                filled_array,\n",
    "                columns=pd.Index(available_cols),\n",
    "                index=data.index,\n",
    "            )\n",
    "            scaled.loc[:, available_cols] = filled_scaled_subset\n",
    "        filled = self._destandardize_with_stats(scaled, means, stds)\n",
    "        for col in missing_cols:\n",
    "            median_val = float(medians_lookup.get(col, 0.0))\n",
    "            mask_missing = cast(pd.Series, filled[col].isna())\n",
    "            if self._series_any(mask_missing):\n",
    "                filled.loc[mask_missing, col] = median_val\n",
    "        for col in self.columns_:\n",
    "            mask_series = cast(pd.Series, data[col].isna()).astype(bool)\n",
    "            if self._series_any(~mask_series):\n",
    "                filled.loc[~mask_series, col] = data.loc[~mask_series, col].astype(float)\n",
    "        return filled\n",
    "\n",
    "    def _fit_missforest(self, data: pd.DataFrame):\n",
    "        max_iter = int(self._get_policy_param(\"missforest_max_iter\", 5))\n",
    "        n_estimators = int(self._get_policy_param(\"missforest_estimators\", 200))\n",
    "        means, stds = self._compute_scaler_stats(data)\n",
    "        scaled = self._standardize_with_stats(data, means, stds)\n",
    "        available_cols: List[str] = []\n",
    "        for col in self.columns_:\n",
    "            notna_series = cast(pd.Series, data[col].notna())\n",
    "            if self._series_any(notna_series):\n",
    "                available_cols.append(col)\n",
    "        missing_cols = [col for col in self.columns_ if col not in available_cols]\n",
    "        if not available_cols:\n",
    "            medians_series = pd.Series({col: self._medians_dict_.get(col, 0.0) for col in self.columns_})\n",
    "            filled = data.fillna(medians_series)\n",
    "            state = {\n",
    "                \"imputer\": None,\n",
    "                \"scaler_means\": means,\n",
    "                \"scaler_stds\": stds,\n",
    "                \"available_cols\": available_cols,\n",
    "                \"missing_cols\": missing_cols,\n",
    "                \"medians\": self._medians_dict_,\n",
    "            }\n",
    "            return filled, state\n",
    "        scaled_subset = scaled.loc[:, available_cols]\n",
    "        rng_estim = np.random.RandomState(self.random_state)\n",
    "        estimator = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=rng_estim,\n",
    "            n_jobs=-1,\n",
    "            max_depth=self._get_policy_param(\"missforest_max_depth\", None),\n",
    "        )\n",
    "        rng_imputer = np.random.RandomState(self.random_state)\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=estimator,\n",
    "            random_state=rng_imputer,\n",
    "            max_iter=max_iter,\n",
    "            sample_posterior=False,\n",
    "            initial_strategy=\"median\",\n",
    "        )\n",
    "        filled_array = imputer.fit_transform(scaled_subset)\n",
    "        filled_scaled_subset = pd.DataFrame(\n",
    "            filled_array,\n",
    "            columns=pd.Index(available_cols),\n",
    "            index=data.index,\n",
    "        )\n",
    "        filled_scaled = scaled.copy()\n",
    "        filled_scaled.loc[:, available_cols] = filled_scaled_subset\n",
    "        filled = self._destandardize_with_stats(filled_scaled, means, stds)\n",
    "        medians_lookup = self._medians_dict_\n",
    "        for col in missing_cols:\n",
    "            median_val = float(medians_lookup.get(col, 0.0))\n",
    "            mask_missing = cast(pd.Series, filled[col].isna())\n",
    "            if self._series_any(mask_missing):\n",
    "                filled.loc[mask_missing, col] = median_val\n",
    "        for col in self.columns_:\n",
    "            mask_series = cast(pd.Series, data[col].isna()).astype(bool)\n",
    "            if self._series_any(~mask_series):\n",
    "                filled.loc[~mask_series, col] = data.loc[~mask_series, col].astype(float)\n",
    "        state = {\n",
    "            \"imputer\": imputer,\n",
    "            \"scaler_means\": means,\n",
    "            \"scaler_stds\": stds,\n",
    "            \"available_cols\": available_cols,\n",
    "            \"missing_cols\": missing_cols,\n",
    "            \"medians\": medians_lookup,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_missforest(self, data: pd.DataFrame):\n",
    "        imputer = cast(IterativeImputer, self._state_.get(\"imputer\"))\n",
    "        means = cast(Dict[str, float], self._state_.get(\"scaler_means\", {}))\n",
    "        stds = cast(Dict[str, float], self._state_.get(\"scaler_stds\", {}))\n",
    "        available_cols = cast(List[str], self._state_.get(\"available_cols\", []))\n",
    "        missing_cols = cast(List[str], self._state_.get(\"missing_cols\", []))\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        scaled = self._standardize_with_stats(data, means, stds)\n",
    "        if imputer is not None and available_cols:\n",
    "            scaled_subset = scaled.loc[:, available_cols]\n",
    "            filled_array = imputer.transform(scaled_subset)\n",
    "            filled_scaled_subset = pd.DataFrame(\n",
    "                filled_array,\n",
    "                columns=pd.Index(available_cols),\n",
    "                index=data.index,\n",
    "            )\n",
    "            scaled.loc[:, available_cols] = filled_scaled_subset\n",
    "        filled = self._destandardize_with_stats(scaled, means, stds)\n",
    "        for col in missing_cols:\n",
    "            median_val = float(medians_lookup.get(col, 0.0))\n",
    "            mask_missing = cast(pd.Series, filled[col].isna())\n",
    "            if self._series_any(mask_missing):\n",
    "                filled.loc[mask_missing, col] = median_val\n",
    "        for col in self.columns_:\n",
    "            mask_series = cast(pd.Series, data[col].isna()).astype(bool)\n",
    "            if self._series_any(~mask_series):\n",
    "                filled.loc[~mask_series, col] = data.loc[~mask_series, col].astype(float)\n",
    "        return filled\n",
    "\n",
    "    def _fit_ridge_stack(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float]):\n",
    "        alpha = float(self._get_policy_param(\"ridge_alpha\", 1.0))\n",
    "        medians_series = pd.Series(medians_lookup)\n",
    "        filled_reference = data.fillna(medians_series)\n",
    "        models: Dict[str, Ridge] = {}\n",
    "        for col in self.columns_:\n",
    "            mask = cast(pd.Series, data[col].notna())\n",
    "            if int(mask.sum()) < 2:\n",
    "                continue\n",
    "            target = data.loc[mask, col].astype(float)\n",
    "            features = filled_reference.loc[mask, [c for c in self.columns_ if c != col]]\n",
    "            model = Ridge(alpha=alpha, random_state=np.random.RandomState(self.random_state))\n",
    "            model.fit(features, target)\n",
    "            models[col] = model\n",
    "        filled = filled_reference.copy()\n",
    "        for col in self.columns_:\n",
    "            mask_series = cast(pd.Series, data[col].isna()).astype(bool)\n",
    "            if not self._series_any(mask_series):\n",
    "                filled[col] = data[col].astype(float)\n",
    "                continue\n",
    "            model = models.get(col)\n",
    "            if model is None:\n",
    "                filled.loc[mask_series, col] = medians_lookup.get(col, np.nan)\n",
    "                continue\n",
    "            feature_cols = [c for c in self.columns_ if c != col]\n",
    "            preds = model.predict(filled.loc[mask_series, feature_cols])\n",
    "            filled.loc[mask_series, col] = preds\n",
    "        state = {\n",
    "            \"models\": models,\n",
    "            \"medians\": medians_lookup,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_ridge_stack(self, data: pd.DataFrame):\n",
    "        models = cast(Dict[str, Ridge], self._state_.get(\"models\", {}))\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        medians_series = pd.Series(medians_lookup)\n",
    "        filled = data.fillna(medians_series)\n",
    "        for col in self.columns_:\n",
    "            mask_series = cast(pd.Series, data[col].isna()).astype(bool)\n",
    "            if not self._series_any(mask_series):\n",
    "                filled[col] = data[col].astype(float)\n",
    "                continue\n",
    "            model = models.get(col)\n",
    "            if model is None:\n",
    "                filled.loc[mask_series, col] = medians_lookup.get(col, np.nan)\n",
    "                continue\n",
    "            feature_cols = [c for c in self.columns_ if c != col]\n",
    "            preds = model.predict(filled.loc[mask_series, feature_cols])\n",
    "            filled.loc[mask_series, col] = preds\n",
    "        return filled\n",
    "\n",
    "    def _fit_quantile_fill(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float]):\n",
    "        quantile = float(self._get_policy_param(\"quantile\", 0.5))\n",
    "        quantile = min(max(0.0, quantile), 1.0)\n",
    "        deques: Dict[str, Deque[float]] = {col: deque(maxlen=self.rolling_window) for col in self.columns_}\n",
    "        filled = data.copy()\n",
    "        for index_label in data.index:\n",
    "            for col in self.columns_:\n",
    "                original = data.at[index_label, col]\n",
    "                if pd.isna(original):\n",
    "                    dq = deques[col]\n",
    "                    if dq:\n",
    "                        fill_value = float(np.quantile(list(dq), quantile))\n",
    "                    else:\n",
    "                        fill_value = float(medians_lookup.get(col, np.nan))\n",
    "                    filled.at[index_label, col] = fill_value\n",
    "                    deques[col].append(fill_value)\n",
    "                else:\n",
    "                    valf = float(cast(float, original))\n",
    "                    filled.at[index_label, col] = valf\n",
    "                    deques[col].append(valf)\n",
    "        state = {\n",
    "            \"medians\": medians_lookup,\n",
    "            \"deques\": {col: deque(deques[col], maxlen=self.rolling_window) for col in self.columns_},\n",
    "            \"quantile\": quantile,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_quantile_fill(self, data: pd.DataFrame):\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        quantile = float(self._state_.get(\"quantile\", 0.5))\n",
    "        stored_deques = cast(Dict[str, Deque[float]], self._state_.get(\"deques\", {}))\n",
    "        deques = {col: deque(stored_deques.get(col, deque()), maxlen=self.rolling_window) for col in self.columns_}\n",
    "        filled = data.copy()\n",
    "        for index_label in data.index:\n",
    "            for col in self.columns_:\n",
    "                original = data.at[index_label, col]\n",
    "                if pd.isna(original):\n",
    "                    dq = deques[col]\n",
    "                    if dq:\n",
    "                        fill_value = float(np.quantile(list(dq), quantile))\n",
    "                    else:\n",
    "                        fill_value = float(medians_lookup.get(col, np.nan))\n",
    "                    filled.at[index_label, col] = fill_value\n",
    "                    deques[col].append(fill_value)\n",
    "                else:\n",
    "                    valf = float(cast(float, original))\n",
    "                    filled.at[index_label, col] = valf\n",
    "                    deques[col].append(valf)\n",
    "        self._state_[\"deques\"] = {col: deque(deques[col], maxlen=self.rolling_window) for col in self.columns_}\n",
    "        return filled\n",
    "\n",
    "    def _fit_kalman(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        medians_lookup: Dict[Hashable, float],\n",
    "        *,\n",
    "        level_only: bool,\n",
    "    ):\n",
    "        \"\"\"Fit UnobservedComponents models and backfill using filter (one-step-ahead) estimates.\"\"\"\n",
    "        if UnobservedComponents is None:\n",
    "            raise RuntimeError(\"kalman_* policies require the 'statsmodels' package.\")\n",
    "        models: Dict[str, Any] = {}\n",
    "        filled = data.copy()\n",
    "        requested_history = self._history_length()\n",
    "        warnings: List[str] = []\n",
    "        for col in self.columns_:\n",
    "            series = data[col].astype(float)\n",
    "            if series.notna().sum() < 3:\n",
    "                filled[col] = series.fillna(medians_lookup.get(col, np.nan))\n",
    "                continue\n",
    "            model = UnobservedComponents(series, level=\"local level\", trend=not level_only)  # type: ignore[arg-type]\n",
    "            try:\n",
    "                res = cast(Any, model.fit(disp=False))\n",
    "                fitted_series = pd.Series(cast(Any, res.fittedvalues), index=series.index)\n",
    "                if hasattr(res, \"remove_data\"):\n",
    "                    # Drop cached training arrays to keep serialized artifacts small while\n",
    "                    # retaining parameters required for forward filtering.\n",
    "                    res.remove_data()\n",
    "            except Exception as exc:  # pragma: no cover\n",
    "                res = None\n",
    "                fitted_series = series.fillna(method=\"ffill\")\n",
    "                warnings.append(f\"kalman_fit_fallback[{col}]: {type(exc).__name__}\")\n",
    "            col_filled = series.copy()\n",
    "            mask = series.isna()\n",
    "            col_filled.loc[mask] = fitted_series.loc[mask]\n",
    "            col_filled = col_filled.fillna(medians_lookup.get(col, np.nan))\n",
    "            filled[col] = col_filled\n",
    "            if res is not None:\n",
    "                models[col] = res\n",
    "        history_len, tail = self._effective_history(filled, requested_history)\n",
    "        state = {\n",
    "            \"models\": models,\n",
    "            \"medians\": medians_lookup,\n",
    "            \"history_len\": history_len,\n",
    "            \"tail\": tail,\n",
    "            \"warnings\": warnings,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_kalman(self, data: pd.DataFrame, calendar_series: pd.Series | None):\n",
    "        models = cast(Dict[str, Any], self._state_.get(\"models\", {}))\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        history_len = cast(int, self._state_.get(\"history_len\", self._history_length()))\n",
    "        tail = self._state_tail_frame()\n",
    "        if len(tail) != history_len:\n",
    "            raise RuntimeError(f\"history_len mismatch in kalman transform: tail={len(tail)} expected={history_len}\")\n",
    "        combined = pd.concat([tail, data], axis=0)\n",
    "        result = combined.copy()\n",
    "        for col in self.columns_:\n",
    "            series = combined[col].astype(float)\n",
    "            res = models.get(col)\n",
    "            if res is None:\n",
    "                filled_col = series.fillna(method=\"ffill\").fillna(medians_lookup.get(col, np.nan))\n",
    "                self._record_warning(f\"kalman_transform_fallback[{col}]: model_missing\")\n",
    "            else:\n",
    "                try:\n",
    "                    series_for_filter = series.copy()\n",
    "                    for idx in data.index:\n",
    "                        if idx in series_for_filter.index:\n",
    "                            series_for_filter.at[idx] = np.nan\n",
    "                    applied = res.apply(series_for_filter)\n",
    "                    forecasts_obj = getattr(applied, \"forecasts\", None)\n",
    "                    if forecasts_obj is None:\n",
    "                        filter_results = getattr(applied, \"filter_results\", None)\n",
    "                        forecasts_obj = getattr(filter_results, \"forecasts\", None)\n",
    "                    if forecasts_obj is not None:\n",
    "                        forecasts_arr = np.asarray(forecasts_obj)\n",
    "                        if forecasts_arr.ndim == 2:\n",
    "                            forecasts_arr = forecasts_arr.reshape(forecasts_arr.shape[0], -1)\n",
    "                            if forecasts_arr.shape[0] == 1:\n",
    "                                forecasts_arr = forecasts_arr[0]\n",
    "                            else:\n",
    "                                forecasts_arr = forecasts_arr.mean(axis=0)\n",
    "                        fitted_values = forecasts_arr.reshape(-1)\n",
    "                        if len(fitted_values) < len(series):\n",
    "                            fitted_values = np.pad(fitted_values, (len(series) - len(fitted_values), 0), mode=\"edge\")\n",
    "                    else:\n",
    "                        raise AttributeError(\"kalman_forecast_unavailable\")\n",
    "                except Exception as exc:  # pragma: no cover\n",
    "                    fitted_values = series.fillna(method=\"ffill\")\n",
    "                    self._record_warning(f\"kalman_transform_fallback[{col}]: {type(exc).__name__}\")\n",
    "                fitted_series = pd.Series(fitted_values, index=combined.index)\n",
    "                filled_col = series.copy()\n",
    "                mask = series.isna()\n",
    "                filled_col.loc[mask] = fitted_series.loc[mask]\n",
    "                filled_col = filled_col.fillna(medians_lookup.get(col, np.nan))\n",
    "            result[col] = filled_col\n",
    "        tail_updated = result.tail(history_len).copy()\n",
    "        self._state_[\"tail\"] = tail_updated\n",
    "        self._state_[\"history_len\"] = len(tail_updated)\n",
    "        return result.iloc[history_len:].copy()\n",
    "\n",
    "    def _fit_arima_auto(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float]):\n",
    "        \"\"\"Auto-select ARIMA orders and use filter outputs to keep the transform forward-only.\"\"\"\n",
    "        if ARIMA is None:\n",
    "            raise RuntimeError(\"arima_auto policy requires the 'statsmodels' package.\")\n",
    "        max_p = int(self._get_policy_param(\"arima_max_p\", 2))\n",
    "        max_d = int(self._get_policy_param(\"arima_max_d\", 1))\n",
    "        max_q = int(self._get_policy_param(\"arima_max_q\", 2))\n",
    "        models: Dict[str, Any] = {}\n",
    "        filled = data.copy()\n",
    "        requested_history = self._history_length()\n",
    "        warnings: List[str] = []\n",
    "        for col in self.columns_:\n",
    "            series = data[col].astype(float)\n",
    "            if series.notna().sum() < 5:\n",
    "                filled[col] = series.fillna(medians_lookup.get(col, np.nan))\n",
    "                continue\n",
    "            best_res = None\n",
    "            best_aic = np.inf\n",
    "            for p in range(max_p + 1):\n",
    "                for d in range(max_d + 1):\n",
    "                    for q in range(max_q + 1):\n",
    "                        if p == d == q == 0:\n",
    "                            continue\n",
    "                        try:\n",
    "                            res = ARIMA(\n",
    "                                series,\n",
    "                                order=(p, d, q),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False,\n",
    "                            ).fit(method_kwargs={\"warn_convergence\": False})\n",
    "                            if res.aic < best_aic:\n",
    "                                best_aic = float(res.aic)\n",
    "                                best_res = res\n",
    "                        except Exception:\n",
    "                            continue\n",
    "            if best_res is None:\n",
    "                filled[col] = series.fillna(method=\"ffill\").fillna(medians_lookup.get(col, np.nan))\n",
    "                warnings.append(f\"arima_fit_fallback[{col}]: no_model\")\n",
    "            else:\n",
    "                fitted_series = pd.Series(best_res.fittedvalues, index=series.index)  # filter (one-step) predictions\n",
    "                col_filled = series.copy()\n",
    "                mask = series.isna()\n",
    "                col_filled.loc[mask] = fitted_series.loc[mask]\n",
    "                col_filled = col_filled.fillna(medians_lookup.get(col, np.nan))\n",
    "                filled[col] = col_filled\n",
    "                models[col] = best_res\n",
    "        history_len, tail = self._effective_history(filled, requested_history)\n",
    "        state = {\n",
    "            \"models\": models,\n",
    "            \"medians\": medians_lookup,\n",
    "            \"history_len\": history_len,\n",
    "            \"tail\": tail,\n",
    "            \"warnings\": warnings,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_arima_auto(self, data: pd.DataFrame):\n",
    "        models = cast(Dict[str, Any], self._state_.get(\"models\", {}))\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        history_len = cast(int, self._state_.get(\"history_len\", self._history_length()))\n",
    "        tail = self._state_tail_frame()\n",
    "        if len(tail) != history_len:\n",
    "            raise RuntimeError(f\"history_len mismatch in arima transform: tail={len(tail)} expected={history_len}\")\n",
    "        combined = pd.concat([tail, data], axis=0)\n",
    "        result = combined.copy()\n",
    "        for col in self.columns_:\n",
    "            series = combined[col].astype(float)\n",
    "            res = models.get(col)\n",
    "            if res is None:\n",
    "                filled_col = series.fillna(method=\"ffill\").fillna(medians_lookup.get(col, np.nan))\n",
    "                self._record_warning(f\"arima_transform_fallback[{col}]: model_missing\")\n",
    "            else:\n",
    "                try:\n",
    "                    forecast_res = res.get_forecast(steps=len(data))\n",
    "                    forecast_mean = np.asarray(forecast_res.predicted_mean)\n",
    "                    forecast_series = pd.Series(forecast_mean, index=data.index)\n",
    "                except Exception as exc:  # pragma: no cover\n",
    "                    fitted_values = series.fillna(method=\"ffill\")\n",
    "                    self._record_warning(f\"arima_transform_fallback[{col}]: {type(exc).__name__}\")\n",
    "                    filled_col = series.copy()\n",
    "                    mask = series.isna()\n",
    "                    filled_col.loc[mask] = fitted_values.loc[mask] if isinstance(fitted_values, pd.Series) else fitted_values\n",
    "                    filled_col = filled_col.fillna(medians_lookup.get(col, np.nan))\n",
    "                else:\n",
    "                    filled_col = series.copy()\n",
    "                    mask_combined = series.isna()\n",
    "                    mask_new = mask_combined.loc[data.index]\n",
    "                    missing_indices = mask_new.index[mask_new]\n",
    "                    if not missing_indices.empty:\n",
    "                        filled_col.loc[missing_indices] = forecast_series.loc[missing_indices]\n",
    "                    filled_col = filled_col.fillna(medians_lookup.get(col, np.nan))\n",
    "            result[col] = filled_col\n",
    "        tail_updated = result.tail(history_len).copy()\n",
    "        self._state_[\"tail\"] = tail_updated\n",
    "        self._state_[\"history_len\"] = len(tail_updated)\n",
    "        return result.iloc[history_len:].copy()\n",
    "    # \u30de\u30b9\u30af\u7cfb\u30fb\u4e8c\u6bb5\u88dc\u5b8c --------------------------------------------------\n",
    "\n",
    "    def _fit_mask_plus_mean(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float]):\n",
    "        medians_series = pd.Series(medians_lookup)\n",
    "        filled = data.fillna(medians_series)\n",
    "        mask_map: Dict[str, str] = {}\n",
    "        for col in self.columns_:\n",
    "            mask_col = f\"{col}_missing_flag\"\n",
    "            mask_map[col] = mask_col\n",
    "            filled[mask_col] = data[col].isna().astype(float)\n",
    "        state = {\n",
    "            \"means\": medians_lookup,\n",
    "            \"mask_map\": mask_map,\n",
    "        }\n",
    "        return filled, state\n",
    "\n",
    "    def _transform_mask_plus_mean(self, data: pd.DataFrame):\n",
    "        means = cast(Dict[Hashable, float], self._state_.get(\"means\", {}))\n",
    "        mask_map = cast(Dict[str, str], self._state_.get(\"mask_map\", {}))\n",
    "        filled = data.fillna(pd.Series(means))\n",
    "        for col, mask_col in mask_map.items():\n",
    "            filled[mask_col] = data[col].isna().astype(float)\n",
    "        return filled\n",
    "\n",
    "    def _fit_two_stage(self, data: pd.DataFrame, medians_lookup: Dict[Hashable, float]):\n",
    "        requested_history = self._history_length()\n",
    "        first_pass = data.interpolate(method=\"linear\", limit_direction=\"forward\")\n",
    "        first_pass = first_pass.ffill()\n",
    "        first_pass = first_pass.fillna(pd.Series(medians_lookup))\n",
    "        deques: Dict[str, Deque[float]] = {col: deque(maxlen=self.rolling_window) for col in self.columns_}\n",
    "        second_pass = first_pass.copy()\n",
    "        for index_label in first_pass.index:\n",
    "            for col in self.columns_:\n",
    "                value = second_pass.at[index_label, col]\n",
    "                if pd.isna(data.at[index_label, col]):\n",
    "                    dq = deques[col]\n",
    "                    if dq:\n",
    "                        fill_value = self._deque_median(dq)\n",
    "                    else:\n",
    "                        fill_value = float(medians_lookup.get(col, np.nan))\n",
    "                    second_pass.at[index_label, col] = fill_value\n",
    "                    deques[col].append(fill_value)\n",
    "                else:\n",
    "                    valf = float(cast(float, value))\n",
    "                    second_pass.at[index_label, col] = valf\n",
    "                    deques[col].append(valf)\n",
    "        history_len, tail = self._effective_history(second_pass, requested_history)\n",
    "        state = {\n",
    "            \"medians\": medians_lookup,\n",
    "            \"history_len\": history_len,\n",
    "            \"tail\": tail,\n",
    "            \"deques\": {col: deque(deques[col], maxlen=self.rolling_window) for col in self.columns_},\n",
    "        }\n",
    "        return second_pass, state\n",
    "\n",
    "    def _transform_two_stage(self, data: pd.DataFrame):\n",
    "        medians_lookup = cast(Dict[Hashable, float], self._state_.get(\"medians\", {}))\n",
    "        history_len = cast(int, self._state_.get(\"history_len\", self._history_length()))\n",
    "        tail = self._state_tail_frame()\n",
    "        deques_state = cast(Dict[str, Deque[float]], self._state_.get(\"deques\", {}))\n",
    "        if len(tail) != history_len:\n",
    "            raise RuntimeError(f\"history_len mismatch in two_stage: tail={len(tail)} expected={history_len}\")\n",
    "        tail_base = tail.reindex(columns=self.columns_, fill_value=np.nan)\n",
    "        combined = pd.concat([tail_base, data], axis=0)\n",
    "        first_pass = combined.interpolate(method=\"linear\", limit_direction=\"forward\")\n",
    "        first_pass = first_pass.ffill()\n",
    "        first_pass = first_pass.fillna(pd.Series(medians_lookup))\n",
    "        deques = {col: deque(deques_state.get(col, deque()), maxlen=self.rolling_window) for col in self.columns_}\n",
    "        second_pass = first_pass.copy()\n",
    "        combined_original = pd.concat([tail_base, data], axis=0)\n",
    "        for index_label in second_pass.index:\n",
    "            for col in self.columns_:\n",
    "                original_val = combined_original.at[index_label, col]\n",
    "                if pd.isna(original_val):\n",
    "                    dq = deques[col]\n",
    "                    if dq:\n",
    "                        fill_value = self._deque_median(dq)\n",
    "                    else:\n",
    "                        fill_value = float(medians_lookup.get(col, np.nan))\n",
    "                    second_pass.at[index_label, col] = fill_value\n",
    "                    deques[col].append(fill_value)\n",
    "                else:\n",
    "                    valf = float(cast(float, second_pass.at[index_label, col]))\n",
    "                    second_pass.at[index_label, col] = valf\n",
    "                    deques[col].append(valf)\n",
    "        self._state_[\"deques\"] = {col: deque(deques[col], maxlen=self.rolling_window) for col in self.columns_}\n",
    "        new_tail = second_pass.tail(history_len).copy()\n",
    "        self._state_[\"tail\"] = new_tail\n",
    "        self._state_[\"history_len\"] = len(new_tail)\n",
    "        result = second_pass.iloc[history_len:].copy()\n",
    "        result.index = data.index\n",
    "        return result\n",
    "\n",
    "# === Module: preprocess.E_group.e_group ===\n",
    "import math\n",
    "from typing import Any, Hashable, Iterable, List, Mapping, cast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "_BaseImputer = MGroupImputer\n",
    "\n",
    "class EGroupImputer(_BaseImputer):\n",
    "    \"\"\"Imputer tailored for E-group features leveraging the M-group policies.\"\"\"\n",
    "    CALENDAR_REQUIRED_POLICIES = {\n",
    "        \"dow_median\",\n",
    "        \"dom_median\",\n",
    "        \"month_median\",\n",
    "        \"holiday_bridge\",\n",
    "        \"time_interp\",\n",
    "        \"kalman_local_level\",\n",
    "        \"state_space_custom\",\n",
    "        \"arima_auto\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        columns: Iterable[Hashable] | None = None,\n",
    "        policy: str = \"ffill_bfill\",\n",
    "        rolling_window: int = 5,\n",
    "        ema_alpha: float = 0.3,\n",
    "        calendar_column: str | None = None,\n",
    "        policy_params: Mapping[str, Any] | None = None,\n",
    "        random_state: int = 42,\n",
    "        all_nan_strategy: str = \"keep_nan\",\n",
    "        all_nan_fill: float = 0.0,\n",
    "    ) -> None:\n",
    "        self._user_calendar_column = calendar_column\n",
    "        self.all_nan_strategy = all_nan_strategy\n",
    "        strategy_choices = {\"keep_nan\", \"fill_zero\", \"fill_constant\"}\n",
    "        if self.all_nan_strategy not in strategy_choices:\n",
    "            raise ValueError(f\"all_nan_strategy must be one of {sorted(strategy_choices)}\")\n",
    "        self.all_nan_fill = float(all_nan_fill)\n",
    "        self.all_nan_fill_value_ = float(all_nan_fill)\n",
    "        self.all_nan_columns_: List[str] = []\n",
    "        self._prefit_warnings: List[str] = []\n",
    "        super().__init__(\n",
    "            columns=columns,\n",
    "            policy=policy,\n",
    "            rolling_window=rolling_window,\n",
    "            ema_alpha=ema_alpha,\n",
    "            calendar_column=calendar_column,\n",
    "            policy_params=policy_params,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        if self.policy in self.CALENDAR_REQUIRED_POLICIES and self._user_calendar_column is None:\n",
    "            raise ValueError(\n",
    "                f\"Policy '{self.policy_requested}' requires calendar_column to be provided explicitly.\"\n",
    "            )\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Any = None):  # type: ignore[override]\n",
    "        frame = self._ensure_dataframe(X).copy()\n",
    "        if self.columns is None:\n",
    "            selected = [c for c in frame.columns if isinstance(c, str) and c.startswith(\"E\")]\n",
    "            self.columns = selected\n",
    "        if self.columns is None:\n",
    "            self.columns = []\n",
    "        numeric_cols = []\n",
    "        for col in list(self.columns):\n",
    "            if col not in frame.columns:\n",
    "                continue\n",
    "            frame.loc[:, col] = pd.to_numeric(frame[col], errors=\"coerce\")\n",
    "            numeric_cols.append(col)\n",
    "        self.columns = numeric_cols\n",
    "        all_nan_columns: List[str] = []\n",
    "        for col in self.columns:\n",
    "            series = cast(pd.Series, frame[col])\n",
    "            if series.isna().all():\n",
    "                all_nan_columns.append(str(col))\n",
    "        self.all_nan_columns_ = all_nan_columns\n",
    "        self._prefit_warnings = []\n",
    "        if self.policy in self.CALENDAR_REQUIRED_POLICIES:\n",
    "            calendar_col = self._user_calendar_column or self.calendar_column\n",
    "            if calendar_col is None:\n",
    "                raise ValueError(\n",
    "                    f\"Policy '{self.policy_requested}' requires calendar_column but none was supplied.\"\n",
    "                )\n",
    "            if calendar_col not in frame.columns:\n",
    "                raise ValueError(f\"Calendar column '{calendar_col}' not found in training frame.\")\n",
    "            calendar_series = pd.to_datetime(frame[calendar_col], errors=\"coerce\")\n",
    "            if calendar_series.isna().any():\n",
    "                self._prefit_warnings.append(\"calendar_column_contains_non_parseable_values\")\n",
    "            if calendar_series.duplicated().any():\n",
    "                self._prefit_warnings.append(\"calendar_column_contains_duplicates\")\n",
    "        fitted = super().fit(frame, y)\n",
    "        if self._prefit_warnings:\n",
    "            for msg in self._prefit_warnings:\n",
    "                self._record_warning(msg)\n",
    "        extra_columns = getattr(self, \"extra_columns_\", None)\n",
    "        if isinstance(extra_columns, list) and extra_columns:\n",
    "            rename_map: dict[str, str] = {}\n",
    "            for col in extra_columns:\n",
    "                rename_map[col] = self._rename_generated_column(col)\n",
    "            if rename_map:\n",
    "                self._train_filled_ = self._train_filled_.rename(columns=rename_map)\n",
    "                renamed_extra: List[str] = [str(rename_map.get(col, col)) for col in extra_columns]\n",
    "                self.extra_columns_ = renamed_extra\n",
    "                output_cols = list(getattr(self, \"_output_columns_\", []))\n",
    "                self._output_columns_ = [str(rename_map.get(col, col)) for col in output_cols]\n",
    "                state = getattr(self, \"_state_\", {})\n",
    "                if isinstance(state, dict):\n",
    "                    for key, value in list(state.items()):\n",
    "                        if isinstance(value, pd.DataFrame):\n",
    "                            state[key] = value.rename(columns=rename_map)\n",
    "                    mask_map = state.get(\"mask_map\")\n",
    "                    if isinstance(mask_map, dict):\n",
    "                        state[\"mask_map\"] = {k: rename_map.get(v, v) for k, v in mask_map.items()}\n",
    "        if self.all_nan_columns_:\n",
    "            fill_value: float | None\n",
    "            if self.all_nan_strategy == \"keep_nan\":\n",
    "                fill_value = np.nan\n",
    "            elif self.all_nan_strategy == \"fill_zero\":\n",
    "                fill_value = 0.0\n",
    "            else:  # fill_constant\n",
    "                fill_value = self.all_nan_fill_value_\n",
    "            for col in self.all_nan_columns_:\n",
    "                if col in self._train_filled_.columns:\n",
    "                    if fill_value is None:\n",
    "                        self._train_filled_.loc[:, col] = np.nan\n",
    "                    elif isinstance(fill_value, float) and math.isnan(fill_value):\n",
    "                        self._train_filled_.loc[:, col] = np.nan\n",
    "                    else:\n",
    "                        self._train_filled_.loc[:, col] = float(fill_value)\n",
    "            state = getattr(self, \"_state_\", {})\n",
    "            if isinstance(state, dict):\n",
    "                state[\"all_nan_columns\"] = list(self.all_nan_columns_)\n",
    "                state[\"all_nan_strategy\"] = self.all_nan_strategy\n",
    "                state[\"all_nan_fill_value\"] = float(self.all_nan_fill_value_)\n",
    "        return fitted\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):  # type: ignore[override]\n",
    "        transformed = super().transform(X)\n",
    "        if self.all_nan_columns_:\n",
    "            if self.all_nan_strategy == \"keep_nan\":\n",
    "                fill_value = np.nan\n",
    "            elif self.all_nan_strategy == \"fill_zero\":\n",
    "                fill_value = 0.0\n",
    "            else:\n",
    "                fill_value = self.all_nan_fill_value_\n",
    "            for col in self.all_nan_columns_:\n",
    "                if col in transformed.columns:\n",
    "                    if fill_value is None:\n",
    "                        transformed.loc[:, col] = np.nan\n",
    "                    elif isinstance(fill_value, float) and math.isnan(fill_value):\n",
    "                        transformed.loc[:, col] = np.nan\n",
    "                    else:\n",
    "                        transformed.loc[:, col] = float(fill_value)\n",
    "        return transformed\n",
    "\n",
    "    def _rename_generated_column(self, name: str) -> str:\n",
    "        if name.endswith(\"_missing_flag\"):\n",
    "            base = name[: -len(\"_missing_flag\")]\n",
    "            return f\"Emask__{base}\"\n",
    "        if name.startswith(\"E__\") or name.startswith(\"Emask__\"):\n",
    "            return name\n",
    "        return f\"E__{name}\"\n",
    "\n",
    "# === Module: preprocess.I_group.i_group ===\n",
    "from typing import Any, Dict, Iterable, Mapping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "_BaseMGroupImputer = MGroupImputer\n",
    "\n",
    "class IGroupImputer(_BaseMGroupImputer):\n",
    "    \"\"\"Specialised imputer for I-group (inventory) features.\n",
    "    Extends :class:`MGroupImputer` with I-specific defaults:\n",
    "    - Automatically scopes to columns starting with ``\"I\"`` when no column list is provided.\n",
    "    - Renames generated helper columns so they remain I-namespaced (e.g. ``Imask__``).\n",
    "    - Optionally applies quantile clipping after imputation to cap extreme values.\n",
    "    \"\"\"\n",
    "    CALENDAR_REQUIRED_POLICIES = {\n",
    "        \"dow_median\",\n",
    "        \"dom_median\",\n",
    "        \"month_median\",\n",
    "        \"holiday_bridge\",\n",
    "        \"time_interp\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        columns: Iterable[str] | None = None,\n",
    "        policy: str = \"ffill_bfill\",\n",
    "        rolling_window: int = 5,\n",
    "        ema_alpha: float = 0.3,\n",
    "        calendar_column: str | None = None,\n",
    "        policy_params: Mapping[str, Any] | None = None,\n",
    "        random_state: int = 42,\n",
    "        *,\n",
    "        clip_quantile_low: float = 0.001,\n",
    "        clip_quantile_high: float = 0.999,\n",
    "        enable_quantile_clip: bool = True,\n",
    "    ) -> None:\n",
    "        self._user_calendar_column = calendar_column\n",
    "        self.clip_quantile_low = float(clip_quantile_low)\n",
    "        self.clip_quantile_high = float(clip_quantile_high)\n",
    "        self.enable_quantile_clip = bool(enable_quantile_clip)\n",
    "        self._clip_bounds_: Dict[str, tuple[float, float]] = {}\n",
    "        self._prefit_warnings: list[str] = []\n",
    "        super().__init__(\n",
    "            columns=columns,\n",
    "            policy=policy,\n",
    "            rolling_window=rolling_window,\n",
    "            ema_alpha=ema_alpha,\n",
    "            calendar_column=calendar_column,\n",
    "            policy_params=policy_params,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):  # type: ignore[override]\n",
    "        frame = self._ensure_dataframe(X).copy()\n",
    "        selected_columns = self._resolve_columns(frame)\n",
    "        numeric_columns: list[str] = []\n",
    "        for col in selected_columns:\n",
    "            if col not in frame.columns:\n",
    "                continue\n",
    "            frame.loc[:, col] = pd.to_numeric(frame[col], errors=\"coerce\")\n",
    "            numeric_columns.append(col)\n",
    "        self.columns = numeric_columns\n",
    "        calendar_column = self._resolve_calendar_column(frame)\n",
    "        if calendar_column is not None and calendar_column in frame.columns:\n",
    "            calendar_series = pd.to_datetime(frame[calendar_column], errors=\"coerce\")\n",
    "            if calendar_series.isna().any():\n",
    "                self._prefit_warnings.append(\"calendar_column_contains_non_parseable_values\")\n",
    "            if calendar_series.duplicated().any():\n",
    "                self._prefit_warnings.append(\"calendar_column_contains_duplicates\")\n",
    "        fitted = super().fit(frame, y)\n",
    "        self._relabel_generated_columns()\n",
    "        self._clip_bounds_ = self._compute_clip_bounds()\n",
    "        if hasattr(self, \"_state_\") and isinstance(self._state_, dict):\n",
    "            if self._prefit_warnings:\n",
    "                warnings = self._state_.setdefault(\"warnings\", [])\n",
    "                if isinstance(warnings, list):\n",
    "                    warnings.extend(self._prefit_warnings)\n",
    "            self._state_[\"clip_bounds\"] = dict(self._clip_bounds_)\n",
    "            self._state_[\"clip_quantile_low\"] = self.clip_quantile_low\n",
    "            self._state_[\"clip_quantile_high\"] = self.clip_quantile_high\n",
    "            self._state_[\"enable_quantile_clip\"] = self.enable_quantile_clip\n",
    "        return fitted\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):  # type: ignore[override]\n",
    "        frame = self._ensure_dataframe(X).copy()\n",
    "        for col in getattr(self, \"columns_\", []):\n",
    "            if col in frame.columns:\n",
    "                frame.loc[:, col] = pd.to_numeric(frame[col], errors=\"coerce\")\n",
    "        transformed = super().transform(frame)\n",
    "        if self.enable_quantile_clip and self._clip_bounds_:\n",
    "            for col, (low, high) in self._clip_bounds_.items():\n",
    "                if col in transformed.columns:\n",
    "                    transformed.loc[:, col] = transformed[col].clip(lower=low, upper=high)\n",
    "        return transformed\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _resolve_columns(self, frame: pd.DataFrame) -> list[str]:\n",
    "        if self.columns is None:\n",
    "            return [c for c in frame.columns if isinstance(c, str) and c.startswith(\"I\")]\n",
    "        return [c for c in self.columns if isinstance(c, str)]\n",
    "\n",
    "    def _resolve_calendar_column(self, frame: pd.DataFrame) -> str | None:\n",
    "        calendar_column = self._user_calendar_column or self.calendar_column\n",
    "        if self.policy in self.CALENDAR_REQUIRED_POLICIES and calendar_column is None:\n",
    "            raise ValueError(\n",
    "                f\"Policy '{self.policy}' requires a calendar column but none was provided.\"\n",
    "            )\n",
    "        if calendar_column is not None and calendar_column not in frame.columns:\n",
    "            raise KeyError(\n",
    "                f\"Calendar column '{calendar_column}' not found in input DataFrame.\"\n",
    "            )\n",
    "        return calendar_column\n",
    "\n",
    "    def _relabel_generated_columns(self) -> None:\n",
    "        rename_map: Dict[str, str] = {}\n",
    "        extra_columns = getattr(self, \"extra_columns_\", [])\n",
    "        if not extra_columns:\n",
    "            return\n",
    "        for col in extra_columns:\n",
    "            rename_map[col] = self._rename_generated_column(col)\n",
    "        if not rename_map:\n",
    "            return\n",
    "        if isinstance(getattr(self, \"_train_filled_\", None), pd.DataFrame):\n",
    "            self._train_filled_ = self._train_filled_.rename(columns=rename_map)\n",
    "        self.extra_columns_ = [str(rename_map.get(col, col)) for col in extra_columns]\n",
    "        if hasattr(self, \"_output_columns_\"):\n",
    "            self._output_columns_ = [str(rename_map.get(col, col)) for col in getattr(self, \"_output_columns_\", [])]\n",
    "        if hasattr(self, \"_state_\") and isinstance(self._state_, dict):\n",
    "            for key, value in list(self._state_.items()):\n",
    "                if isinstance(value, pd.DataFrame):\n",
    "                    self._state_[key] = value.rename(columns=rename_map)\n",
    "            mask_map = self._state_.get(\"mask_map\")\n",
    "            if isinstance(mask_map, dict):\n",
    "                self._state_[\"mask_map\"] = {k: rename_map.get(v, v) for k, v in mask_map.items()}\n",
    "\n",
    "    def _rename_generated_column(self, name: str) -> str:\n",
    "        if name.endswith(\"_missing_flag\"):\n",
    "            base_name = name[: -len(\"_missing_flag\")]\n",
    "            return f\"Imask__{base_name}\"\n",
    "        if name.startswith(\"I\"):\n",
    "            return name\n",
    "        return f\"Iextra__{name}\"\n",
    "\n",
    "    def _compute_clip_bounds(self) -> Dict[str, tuple[float, float]]:\n",
    "        if not self.enable_quantile_clip:\n",
    "            return {}\n",
    "        q_low = float(self.clip_quantile_low)\n",
    "        q_high = float(self.clip_quantile_high)\n",
    "        if q_low < 0.0 or q_high > 1.0 or q_low >= q_high:\n",
    "            return {}\n",
    "        filled = getattr(self, \"_train_filled_\", None)\n",
    "        if not isinstance(filled, pd.DataFrame):\n",
    "            return {}\n",
    "        bounds: Dict[str, tuple[float, float]] = {}\n",
    "        for col in getattr(self, \"columns_\", []):\n",
    "            if col not in filled.columns:\n",
    "                continue\n",
    "            numeric_values = pd.to_numeric(filled[col], errors=\"coerce\")\n",
    "            series = pd.Series(numeric_values).dropna()\n",
    "            if series.empty:\n",
    "                continue\n",
    "            low_value = float(series.quantile(q_low))\n",
    "            high_value = float(series.quantile(q_high))\n",
    "            if not np.isfinite(low_value) or not np.isfinite(high_value):\n",
    "                continue\n",
    "            if high_value <= low_value:\n",
    "                continue\n",
    "            bounds[col] = (low_value, high_value)\n",
    "        return bounds\n",
    "\n",
    "# === Module: preprocess.P_group.p_group ===\n",
    "from typing import Any, Dict, Iterable, Mapping, cast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "_BaseMGroupImputer = MGroupImputer\n",
    "\n",
    "class PGroupImputer(_BaseMGroupImputer):\n",
    "    \"\"\"Imputer tailored for valuation (P-group) features.\n",
    "    The implementation mirrors :class:`MGroupImputer` but adds P-specific behavior:\n",
    "    - Default column discovery targets columns beginning with ``\"P\"``.\n",
    "    - Helper columns generated by policies such as ``mask_plus_mean`` are renamed to\n",
    "      remain P-namespaced (``Pmask__`` / ``Pextra__``).\n",
    "    - After fitting, values are clipped by a robust median\u00b1MAD envelope (configurable)\n",
    "      to damp extreme valuation swings. A quantile fallback is used when MAD is zero\n",
    "      or insufficient samples are available.\n",
    "    \"\"\"\n",
    "    CALENDAR_REQUIRED_POLICIES = {\n",
    "        \"dow_median\",\n",
    "        \"dom_median\",\n",
    "        \"month_median\",\n",
    "        \"holiday_bridge\",\n",
    "        \"time_interp\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        columns: Iterable[str] | None = None,\n",
    "        policy: str = \"ffill_bfill\",\n",
    "        rolling_window: int = 5,\n",
    "        ema_alpha: float = 0.3,\n",
    "        calendar_column: str | None = None,\n",
    "        policy_params: Mapping[str, Any] | None = None,\n",
    "        random_state: int = 42,\n",
    "        *,\n",
    "        mad_clip_scale: float = 4.0,\n",
    "        mad_clip_min_samples: int = 25,\n",
    "        enable_mad_clip: bool = True,\n",
    "        fallback_quantile_low: float = 0.005,\n",
    "        fallback_quantile_high: float = 0.995,\n",
    "    ) -> None:\n",
    "        self._user_calendar_column = calendar_column\n",
    "        self.mad_clip_scale = float(mad_clip_scale)\n",
    "        self.mad_clip_min_samples = int(mad_clip_min_samples)\n",
    "        self.enable_mad_clip = bool(enable_mad_clip)\n",
    "        self.fallback_quantile_low = float(fallback_quantile_low)\n",
    "        self.fallback_quantile_high = float(fallback_quantile_high)\n",
    "        self._clip_bounds_: Dict[str, tuple[float, float]] = {}\n",
    "        self._prefit_warnings: list[str] = []\n",
    "        super().__init__(\n",
    "            columns=columns,\n",
    "            policy=policy,\n",
    "            rolling_window=rolling_window,\n",
    "            ema_alpha=ema_alpha,\n",
    "            calendar_column=calendar_column,\n",
    "            policy_params=policy_params,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Any = None):  # type: ignore[override]\n",
    "        frame = self._ensure_dataframe(X).copy()\n",
    "        selected_columns = self._resolve_columns(frame)\n",
    "        numeric_columns: list[str] = []\n",
    "        for col in selected_columns:\n",
    "            if col not in frame.columns:\n",
    "                continue\n",
    "            frame.loc[:, col] = pd.to_numeric(frame[col], errors=\"coerce\")\n",
    "            numeric_columns.append(col)\n",
    "        self.columns = numeric_columns\n",
    "        calendar_column = self._resolve_calendar_column(frame)\n",
    "        if calendar_column is not None and calendar_column in frame.columns:\n",
    "            calendar_series = pd.to_datetime(frame[calendar_column], errors=\"coerce\")\n",
    "            if calendar_series.isna().any():\n",
    "                self._prefit_warnings.append(\"calendar_column_contains_non_parseable_values\")\n",
    "            if calendar_series.duplicated().any():\n",
    "                self._prefit_warnings.append(\"calendar_column_contains_duplicates\")\n",
    "        fitted = super().fit(frame, y)\n",
    "        self._relabel_generated_columns()\n",
    "        self._clip_bounds_ = self._compute_clip_bounds()\n",
    "        if hasattr(self, \"_state_\") and isinstance(self._state_, dict):\n",
    "            if self._prefit_warnings:\n",
    "                warnings = self._state_.setdefault(\"warnings\", [])\n",
    "                if isinstance(warnings, list):\n",
    "                    warnings.extend(self._prefit_warnings)\n",
    "            self._state_[\"mad_clip_bounds\"] = dict(self._clip_bounds_)\n",
    "            self._state_[\"mad_clip_scale\"] = self.mad_clip_scale\n",
    "            self._state_[\"mad_clip_min_samples\"] = self.mad_clip_min_samples\n",
    "            self._state_[\"enable_mad_clip\"] = self.enable_mad_clip\n",
    "            self._state_[\"fallback_quantile_low\"] = self.fallback_quantile_low\n",
    "            self._state_[\"fallback_quantile_high\"] = self.fallback_quantile_high\n",
    "        return fitted\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):  # type: ignore[override]\n",
    "        frame = self._ensure_dataframe(X).copy()\n",
    "        for col in getattr(self, \"columns_\", []):\n",
    "            if col in frame.columns:\n",
    "                frame.loc[:, col] = pd.to_numeric(frame[col], errors=\"coerce\")\n",
    "        transformed = super().transform(frame)\n",
    "        if self.enable_mad_clip and self._clip_bounds_:\n",
    "            for col, (low, high) in self._clip_bounds_.items():\n",
    "                if col in transformed.columns:\n",
    "                    transformed.loc[:, col] = transformed[col].clip(lower=low, upper=high)\n",
    "        return transformed\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _resolve_columns(self, frame: pd.DataFrame) -> list[str]:\n",
    "        if self.columns is None:\n",
    "            return [c for c in frame.columns if isinstance(c, str) and c.startswith(\"P\")]\n",
    "        return [c for c in self.columns if isinstance(c, str)]\n",
    "\n",
    "    def _resolve_calendar_column(self, frame: pd.DataFrame) -> str | None:\n",
    "        calendar_column = self._user_calendar_column or self.calendar_column\n",
    "        if self.policy in self.CALENDAR_REQUIRED_POLICIES and calendar_column is None:\n",
    "            raise ValueError(\n",
    "                f\"Policy '{self.policy}' requires a calendar column but none was provided.\"\n",
    "            )\n",
    "        if calendar_column is not None and calendar_column not in frame.columns:\n",
    "            raise KeyError(\n",
    "                f\"Calendar column '{calendar_column}' not found in input DataFrame.\"\n",
    "            )\n",
    "        return calendar_column\n",
    "\n",
    "    def _relabel_generated_columns(self) -> None:\n",
    "        rename_map: Dict[str, str] = {}\n",
    "        extra_columns = getattr(self, \"extra_columns_\", [])\n",
    "        if not extra_columns:\n",
    "            return\n",
    "        for col in extra_columns:\n",
    "            rename_map[col] = self._rename_generated_column(col)\n",
    "        if not rename_map:\n",
    "            return\n",
    "        train_filled = getattr(self, \"_train_filled_\", None)\n",
    "        if isinstance(train_filled, pd.DataFrame):\n",
    "            self._train_filled_ = train_filled.rename(columns=rename_map)\n",
    "        self.extra_columns_ = [str(rename_map.get(col, col)) for col in extra_columns]\n",
    "        if hasattr(self, \"_output_columns_\"):\n",
    "            self._output_columns_ = [str(rename_map.get(col, col)) for col in getattr(self, \"_output_columns_\", [])]\n",
    "        state = getattr(self, \"_state_\", None)\n",
    "        if isinstance(state, dict):\n",
    "            for key, value in list(state.items()):\n",
    "                if isinstance(value, pd.DataFrame):\n",
    "                    state[key] = value.rename(columns=rename_map)\n",
    "            mask_map = state.get(\"mask_map\")\n",
    "            if isinstance(mask_map, dict):\n",
    "                state[\"mask_map\"] = {k: rename_map.get(v, v) for k, v in mask_map.items()}\n",
    "\n",
    "    def _rename_generated_column(self, name: str) -> str:\n",
    "        if name.endswith(\"_missing_flag\"):\n",
    "            base_name = name[: -len(\"_missing_flag\")]\n",
    "            return f\"Pmask__{base_name}\"\n",
    "        if name.startswith(\"P\"):\n",
    "            return name\n",
    "        return f\"Pextra__{name}\"\n",
    "\n",
    "    def _compute_clip_bounds(self) -> Dict[str, tuple[float, float]]:\n",
    "        if not self.enable_mad_clip:\n",
    "            return {}\n",
    "        filled = getattr(self, \"_train_filled_\", None)\n",
    "        if not isinstance(filled, pd.DataFrame):\n",
    "            return {}\n",
    "        bounds: Dict[str, tuple[float, float]] = {}\n",
    "        q_low = float(self.fallback_quantile_low)\n",
    "        q_high = float(self.fallback_quantile_high)\n",
    "        use_quantile = 0.0 <= q_low < q_high <= 1.0\n",
    "        for col in getattr(self, \"columns_\", []):\n",
    "            if col not in filled.columns:\n",
    "                continue\n",
    "            numeric_series = cast(pd.Series, pd.to_numeric(filled[col], errors=\"coerce\"))\n",
    "            numeric_series = numeric_series.dropna()\n",
    "            if numeric_series.empty:\n",
    "                continue\n",
    "            low = high = None\n",
    "            values = numeric_series.to_numpy(dtype=float, copy=False)\n",
    "            if values.size >= max(1, self.mad_clip_min_samples):\n",
    "                median = float(np.median(values))\n",
    "                mad = float(np.median(np.abs(values - median)))\n",
    "                if np.isfinite(mad) and mad > 0.0 and np.isfinite(median):\n",
    "                    spread = float(self.mad_clip_scale) * mad\n",
    "                    low = median - spread\n",
    "                    high = median + spread\n",
    "            if (low is None or high is None or not np.isfinite(low) or not np.isfinite(high) or high <= low) and use_quantile:\n",
    "                low_q = float(numeric_series.quantile(q_low))\n",
    "                high_q = float(numeric_series.quantile(q_high))\n",
    "                if np.isfinite(low_q) and np.isfinite(high_q) and high_q > low_q:\n",
    "                    low, high = low_q, high_q\n",
    "            if low is None or high is None or not np.isfinite(low) or not np.isfinite(high) or high <= low:\n",
    "                continue\n",
    "            bounds[col] = (float(low), float(high))\n",
    "        return bounds\n",
    "\n",
    "# === Module: preprocess.S_group.s_group ===\n",
    "from typing import Any, Dict, Iterable, Mapping, cast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "_BaseMGroupImputer = MGroupImputer\n",
    "\n",
    "class SGroupImputer(_BaseMGroupImputer):\n",
    "    \"\"\"Imputer tailored for sentiment (S-group) features.\n",
    "    The implementation mirrors :class:`MGroupImputer` but adds S-specific behavior:\n",
    "    - Default column discovery targets columns beginning with ``\"S\"``.\n",
    "    - Helper columns generated by policies such as ``mask_plus_mean`` are renamed to\n",
    "      remain S-namespaced (``Smask__`` / ``Sextra__``).\n",
    "    - After fitting, values are clipped by a robust median\u00b1MAD envelope (configurable)\n",
    "      to damp extreme sentiment swings. A quantile fallback is used when MAD is zero\n",
    "      or insufficient samples are available.\n",
    "    \"\"\"\n",
    "    CALENDAR_REQUIRED_POLICIES = {\n",
    "        \"dow_median\",\n",
    "        \"dom_median\",\n",
    "        \"month_median\",\n",
    "        \"holiday_bridge\",\n",
    "        \"time_interp\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        columns: Iterable[str] | None = None,\n",
    "        policy: str = \"ffill_bfill\",\n",
    "        rolling_window: int = 5,\n",
    "        ema_alpha: float = 0.3,\n",
    "        calendar_column: str | None = None,\n",
    "        policy_params: Mapping[str, Any] | None = None,\n",
    "        random_state: int = 42,\n",
    "        *,\n",
    "        mad_clip_scale: float = 4.0,\n",
    "        mad_clip_min_samples: int = 25,\n",
    "        enable_mad_clip: bool = True,\n",
    "        fallback_quantile_low: float = 0.005,\n",
    "        fallback_quantile_high: float = 0.995,\n",
    "    ) -> None:\n",
    "        self._user_calendar_column = calendar_column\n",
    "        self.mad_clip_scale = float(mad_clip_scale)\n",
    "        self.mad_clip_min_samples = int(mad_clip_min_samples)\n",
    "        self.enable_mad_clip = bool(enable_mad_clip)\n",
    "        self.fallback_quantile_low = float(fallback_quantile_low)\n",
    "        self.fallback_quantile_high = float(fallback_quantile_high)\n",
    "        self._clip_bounds_: Dict[str, tuple[float, float]] = {}\n",
    "        self._prefit_warnings: list[str] = []\n",
    "        super().__init__(\n",
    "            columns=columns,\n",
    "            policy=policy,\n",
    "            rolling_window=rolling_window,\n",
    "            ema_alpha=ema_alpha,\n",
    "            calendar_column=calendar_column,\n",
    "            policy_params=policy_params,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Any = None):  # type: ignore[override]\n",
    "        frame = self._ensure_dataframe(X).copy()\n",
    "        selected_columns = self._resolve_columns(frame)\n",
    "        numeric_columns: list[str] = []\n",
    "        for col in selected_columns:\n",
    "            if col not in frame.columns:\n",
    "                continue\n",
    "            frame.loc[:, col] = pd.to_numeric(frame[col], errors=\"coerce\")\n",
    "            numeric_columns.append(col)\n",
    "        self.columns = numeric_columns\n",
    "        calendar_column = self._resolve_calendar_column(frame)\n",
    "        if calendar_column is not None and calendar_column in frame.columns:\n",
    "            calendar_series = pd.to_datetime(frame[calendar_column], errors=\"coerce\")\n",
    "            if calendar_series.isna().any():\n",
    "                self._prefit_warnings.append(\"calendar_column_contains_non_parseable_values\")\n",
    "            if calendar_series.duplicated().any():\n",
    "                self._prefit_warnings.append(\"calendar_column_contains_duplicates\")\n",
    "        fitted = super().fit(frame, y)\n",
    "        self._relabel_generated_columns()\n",
    "        self._clip_bounds_ = self._compute_clip_bounds()\n",
    "        if hasattr(self, \"_state_\") and isinstance(self._state_, dict):\n",
    "            if self._prefit_warnings:\n",
    "                warnings = self._state_.setdefault(\"warnings\", [])\n",
    "                if isinstance(warnings, list):\n",
    "                    warnings.extend(self._prefit_warnings)\n",
    "            self._state_[\"mad_clip_bounds\"] = dict(self._clip_bounds_)\n",
    "            self._state_[\"mad_clip_scale\"] = self.mad_clip_scale\n",
    "            self._state_[\"mad_clip_min_samples\"] = self.mad_clip_min_samples\n",
    "            self._state_[\"enable_mad_clip\"] = self.enable_mad_clip\n",
    "            self._state_[\"fallback_quantile_low\"] = self.fallback_quantile_low\n",
    "            self._state_[\"fallback_quantile_high\"] = self.fallback_quantile_high\n",
    "        return fitted\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):  # type: ignore[override]\n",
    "        frame = self._ensure_dataframe(X).copy()\n",
    "        for col in getattr(self, \"columns_\", []):\n",
    "            if col in frame.columns:\n",
    "                frame.loc[:, col] = pd.to_numeric(frame[col], errors=\"coerce\")\n",
    "        transformed = super().transform(frame)\n",
    "        if self.enable_mad_clip and self._clip_bounds_:\n",
    "            for col, (low, high) in self._clip_bounds_.items():\n",
    "                if col in transformed.columns:\n",
    "                    transformed.loc[:, col] = transformed[col].clip(lower=low, upper=high)\n",
    "        return transformed\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _resolve_columns(self, frame: pd.DataFrame) -> list[str]:\n",
    "        if self.columns is None:\n",
    "            return [c for c in frame.columns if isinstance(c, str) and c.startswith(\"S\")]\n",
    "        return [c for c in self.columns if isinstance(c, str)]\n",
    "\n",
    "    def _resolve_calendar_column(self, frame: pd.DataFrame) -> str | None:\n",
    "        calendar_column = self._user_calendar_column or self.calendar_column\n",
    "        if self.policy in self.CALENDAR_REQUIRED_POLICIES and calendar_column is None:\n",
    "            raise ValueError(\n",
    "                f\"Policy '{self.policy}' requires a calendar column but none was provided.\"\n",
    "            )\n",
    "        if calendar_column is not None and calendar_column not in frame.columns:\n",
    "            raise KeyError(\n",
    "                f\"Calendar column '{calendar_column}' not found in input DataFrame.\"\n",
    "            )\n",
    "        return calendar_column\n",
    "\n",
    "    def _relabel_generated_columns(self) -> None:\n",
    "        rename_map: Dict[str, str] = {}\n",
    "        extra_columns = getattr(self, \"extra_columns_\", [])\n",
    "        if not extra_columns:\n",
    "            return\n",
    "        for col in extra_columns:\n",
    "            rename_map[col] = self._rename_generated_column(col)\n",
    "        if not rename_map:\n",
    "            return\n",
    "        train_filled = getattr(self, \"_train_filled_\", None)\n",
    "        if isinstance(train_filled, pd.DataFrame):\n",
    "            self._train_filled_ = train_filled.rename(columns=rename_map)\n",
    "        self.extra_columns_ = [str(rename_map.get(col, col)) for col in extra_columns]\n",
    "        if hasattr(self, \"_output_columns_\"):\n",
    "            self._output_columns_ = [str(rename_map.get(col, col)) for col in getattr(self, \"_output_columns_\", [])]\n",
    "        state = getattr(self, \"_state_\", None)\n",
    "        if isinstance(state, dict):\n",
    "            for key, value in list(state.items()):\n",
    "                if isinstance(value, pd.DataFrame):\n",
    "                    state[key] = value.rename(columns=rename_map)\n",
    "            mask_map = state.get(\"mask_map\")\n",
    "            if isinstance(mask_map, dict):\n",
    "                state[\"mask_map\"] = {k: rename_map.get(v, v) for k, v in mask_map.items()}\n",
    "\n",
    "    def _rename_generated_column(self, name: str) -> str:\n",
    "        if name.endswith(\"_missing_flag\"):\n",
    "            base_name = name[: -len(\"_missing_flag\")]\n",
    "            return f\"Smask__{base_name}\"\n",
    "        if name.startswith(\"S\"):\n",
    "            return name\n",
    "        return f\"Sextra__{name}\"\n",
    "\n",
    "    def _compute_clip_bounds(self) -> Dict[str, tuple[float, float]]:\n",
    "        if not self.enable_mad_clip:\n",
    "            return {}\n",
    "        filled = getattr(self, \"_train_filled_\", None)\n",
    "        if not isinstance(filled, pd.DataFrame):\n",
    "            return {}\n",
    "        bounds: Dict[str, tuple[float, float]] = {}\n",
    "        q_low = float(self.fallback_quantile_low)\n",
    "        q_high = float(self.fallback_quantile_high)\n",
    "        use_quantile = 0.0 <= q_low < q_high <= 1.0\n",
    "        for col in getattr(self, \"columns_\", []):\n",
    "            if col not in filled.columns:\n",
    "                continue\n",
    "            numeric_series = cast(pd.Series, pd.to_numeric(filled[col], errors=\"coerce\"))\n",
    "            numeric_series = numeric_series.dropna()\n",
    "            if numeric_series.empty:\n",
    "                continue\n",
    "            low = high = None\n",
    "            values = numeric_series.to_numpy(dtype=float, copy=False)\n",
    "            if values.size >= max(1, self.mad_clip_min_samples):\n",
    "                median = float(np.median(values))\n",
    "                mad = float(np.median(np.abs(values - median)))\n",
    "                if np.isfinite(mad) and mad > 0.0 and np.isfinite(median):\n",
    "                    spread = float(self.mad_clip_scale) * mad\n",
    "                    low = median - spread\n",
    "                    high = median + spread\n",
    "            if (low is None or high is None or not np.isfinite(low) or not np.isfinite(high) or high <= low) and use_quantile:\n",
    "                low_q = float(numeric_series.quantile(q_low))\n",
    "                high_q = float(numeric_series.quantile(q_high))\n",
    "                if np.isfinite(low_q) and np.isfinite(high_q) and high_q > low_q:\n",
    "                    low, high = low_q, high_q\n",
    "            if low is None or high is None or not np.isfinite(low) or not np.isfinite(high) or high <= low:\n",
    "                continue\n",
    "            bounds[col] = (float(low), float(high))\n",
    "        return bounds\n",
    "\n",
    "# === Module: src.feature_generation.su1.feature_su1 ===\n",
    "\"\"\"SU1\uff08\u6b20\u640d\u69cb\u9020\u30b3\u30a2\u7279\u5fb4\u91cf\uff09\u306e\u751f\u6210\u30ed\u30b8\u30c3\u30af\u3002\n",
    "\u672c\u30e2\u30b8\u30e5\u30fc\u30eb\u306f ``docs/feature_generation/SU1.md`` \u306b\u8a18\u8f09\u3055\u308c\u305f\u65b9\u91dd\u3092\u5b9f\u88c5\u3057\u3001\n",
    "scikit-learn \u4e92\u63db\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc ``SU1FeatureGenerator`` \u3068\u3001\u8a2d\u5b9a YAML \u3084\n",
    "\u751f\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u305f\u3081\u306e\u30d8\u30eb\u30d1\u30fc\u95a2\u6570\u3092\u63d0\u4f9b\u3059\u308b\u3002\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, Literal, Mapping, MutableMapping, Sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def _infer_group(column_name: str) -> str | None:\n",
    "    \"\"\"\u5217\u540d\u306e\u63a5\u982d\u8f9e\u304b\u3089\u7279\u5fb4\u30b0\u30eb\u30fc\u30d7\u3092\u63a8\u5b9a\u3059\u308b\u3002\"\"\"\n",
    "    prefix_chars: list[str] = []\n",
    "    for char in column_name:\n",
    "        if char.isalpha() and char.isupper():\n",
    "            prefix_chars.append(char)\n",
    "            continue\n",
    "        if char.isdigit():\n",
    "            break\n",
    "        # \u82f1\u6570\u5b57\u4ee5\u5916\u304c\u51fa\u73fe\u3057\u305f\u5834\u5408\u306f\u898f\u7d04\u5916\u5217\u3068\u307f\u306a\u3059\u3002\n",
    "        return None\n",
    "    if not prefix_chars:\n",
    "        return None\n",
    "    # \u8907\u6570\u6587\u5b57\u306e\u63a5\u982d\u8f9e\u3082\u305d\u306e\u307e\u307e\u30b0\u30eb\u30fc\u30d7\u8b58\u5225\u5b50\u3068\u3059\u308b\u3002\n",
    "    return \"\".join(prefix_chars)\n",
    "\n",
    "def _coerce_dtype(dtype_like: str) -> np.dtype:\n",
    "    \"\"\"dtype \u6307\u5b9a\u6587\u5b57\u5217\u3092 ``numpy.dtype`` \u306b\u5909\u63db\u3059\u308b\u3002\"\"\"\n",
    "    try:\n",
    "        dtype = np.dtype(dtype_like)\n",
    "    except TypeError as exc:  # pragma: no cover - \u9632\u5fa1\u7684\u5206\u5c90\n",
    "        raise ValueError(f\"Invalid dtype specification: {dtype_like!r}\") from exc\n",
    "    return dtype\n",
    "\n",
    "def _path_from_config(base_dir: Path, value: str) -> Path:\n",
    "    \"\"\"YAML \u8a2d\u5b9a\u5185\u3067\u6307\u5b9a\u3055\u308c\u305f\u30d1\u30b9\u3092\u89e3\u6c7a\u3059\u308b\u3002\"\"\"\n",
    "    raw_path = Path(value)\n",
    "    return raw_path if raw_path.is_absolute() else (base_dir / raw_path).resolve()\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "\n",
    "class SU1Config:\n",
    "    \"\"\"SU1 \u7279\u5fb4\u91cf\u751f\u6210\u306b\u5fc5\u8981\u306a\u8a2d\u5b9a\u3092\u4fdd\u6301\u3059\u308b\u30c7\u30fc\u30bf\u30af\u30e9\u30b9\u3002\"\"\"\n",
    "    id_column: str\n",
    "    exclude_columns: tuple[str, ...]\n",
    "    target_groups: tuple[str, ...]\n",
    "    gap_clip: int\n",
    "    run_clip: int\n",
    "    flag_dtype: np.dtype\n",
    "    run_dtype: np.dtype\n",
    "    include_avg_gap: bool\n",
    "    include_avg_run: bool\n",
    "    exclude_all_nan_for_means: bool\n",
    "    raw_dir: Path\n",
    "    train_filename: str\n",
    "    test_filename: str\n",
    "\n",
    "    @classmethod\n",
    "\n",
    "    def from_mapping(cls, mapping: Mapping[str, Any], *, base_dir: Path) -> \"SU1Config\":\n",
    "        data_section = mapping.get(\"data\", {})\n",
    "        exclude_columns = tuple(mapping.get(\"exclude_columns\", ()))\n",
    "        groups_mapping = mapping.get(\"groups\", {})\n",
    "        include_groups = set(groups_mapping.get(\"include\", []))\n",
    "        exclude_groups = set(groups_mapping.get(\"exclude\", []))\n",
    "        target_groups = tuple(sorted(include_groups.difference(exclude_groups)))\n",
    "        if not target_groups:\n",
    "            raise ValueError(\"SU1 configuration must specify at least one target group.\")\n",
    "        id_column = mapping.get(\"id_column\", \"date_id\")\n",
    "        gap_clip = int(mapping.get(\"gap_clip\", 60))\n",
    "        run_clip = int(mapping.get(\"run_clip\", gap_clip))\n",
    "        dtype_section = mapping.get(\"dtype\", {})\n",
    "        flag_dtype = _coerce_dtype(dtype_section.get(\"flag\", \"uint8\"))\n",
    "        run_dtype = _coerce_dtype(dtype_section.get(\"run\", \"int16\"))\n",
    "        include_group_means = mapping.get(\"include_group_means\", {})\n",
    "        include_avg_gap = bool(include_group_means.get(\"gap_ffill\", True))\n",
    "        include_avg_run = bool(include_group_means.get(\"run_na\", True))\n",
    "        exclude_all_nan_for_means = bool(include_group_means.get(\"exclude_all_nan\", False))\n",
    "        raw_dir = _path_from_config(base_dir, data_section.get(\"raw_dir\", \"data/raw\"))\n",
    "        train_filename = data_section.get(\"train_filename\", \"train.csv\")\n",
    "        test_filename = data_section.get(\"test_filename\", \"test.csv\")\n",
    "        return cls(\n",
    "            id_column=id_column,\n",
    "            exclude_columns=exclude_columns,\n",
    "            target_groups=target_groups,\n",
    "            gap_clip=gap_clip,\n",
    "            run_clip=run_clip,\n",
    "            flag_dtype=flag_dtype,\n",
    "            run_dtype=run_dtype,\n",
    "            include_avg_gap=include_avg_gap,\n",
    "            include_avg_run=include_avg_run,\n",
    "            exclude_all_nan_for_means=exclude_all_nan_for_means,\n",
    "            raw_dir=raw_dir,\n",
    "            train_filename=train_filename,\n",
    "            test_filename=test_filename,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "\n",
    "    def train_path(self) -> Path:\n",
    "        \"\"\"\u5b66\u7fd2\u30c7\u30fc\u30bf CSV \u3078\u306e\u7d76\u5bfe\u30d1\u30b9\u3092\u8fd4\u3059\u3002\"\"\"\n",
    "        return (self.raw_dir / self.train_filename).resolve()\n",
    "\n",
    "    @property\n",
    "\n",
    "    def test_path(self) -> Path:\n",
    "        \"\"\"\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf CSV \u3078\u306e\u7d76\u5bfe\u30d1\u30b9\u3092\u8fd4\u3059\u3002\"\"\"\n",
    "        return (self.raw_dir / self.test_filename).resolve()\n",
    "\n",
    "def load_su1_config(config_path: str | Path) -> SU1Config:\n",
    "    \"\"\"SU1 \u8a2d\u5b9a YAML \u3092\u8aad\u307f\u8fbc\u307f :class:`SU1Config` \u3092\u751f\u6210\u3059\u308b\u3002\"\"\"\n",
    "    path = Path(config_path).resolve()\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        full_cfg: Mapping[str, Any] = yaml.safe_load(fh) or {}\n",
    "    try:\n",
    "        su1_section = full_cfg[\"su1\"]\n",
    "    except KeyError as exc:  # pragma: no cover - \u9632\u5fa1\u7684\u5206\u5c90\n",
    "        raise KeyError(\"'su1' section is required in feature_generation.yaml\") from exc\n",
    "    return SU1Config.from_mapping(su1_section, base_dir=path.parent)\n",
    "\n",
    "def load_raw_dataset(config: SU1Config, *, dataset: Literal[\"train\", \"test\"] = \"train\") -> pd.DataFrame:\n",
    "    \"\"\"SU1 \u7279\u5fb4\u91cf\u751f\u6210\u7528\u306b\u751f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080\u3002\"\"\"\n",
    "    if dataset not in {\"train\", \"test\"}:  # pragma: no cover - \u9632\u5fa1\u7684\u5206\u5c90\n",
    "        raise ValueError(\"dataset must be 'train' or 'test'\")\n",
    "    csv_path = config.train_path if dataset == \"train\" else config.test_path\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Raw data file not found: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if config.id_column in df.columns:\n",
    "        df = df.set_index(config.id_column)\n",
    "    return df\n",
    "\n",
    "def _clip_array(values: np.ndarray, clip_value: int) -> np.ndarray:\n",
    "    \"\"\"\u914d\u5217\u3092\u4e0a\u9650\u5024\u3067\u30af\u30ea\u30c3\u30d7\u3057\u3001\u305d\u306e\u53c2\u7167\u3092\u8fd4\u3059\u3002\"\"\"\n",
    "    np.clip(values, None, clip_value, out=values)\n",
    "    return values\n",
    "\n",
    "def _distance_from_last_observation(mask: np.ndarray, clip: int, dtype: np.dtype) -> np.ndarray:\n",
    "    \"\"\"NaN \u30de\u30b9\u30af\u304b\u3089\u76f4\u8fd1\u89b3\u6e2c\u307e\u3067\u306e\u8ddd\u96e2\u3092\u7b97\u51fa\u3059\u308b\u3002\"\"\"\n",
    "    out = np.zeros(mask.shape[0], dtype=dtype)\n",
    "    last_obs_index = -1\n",
    "    seen_obs = False\n",
    "    for idx, is_missing in enumerate(mask):\n",
    "        if is_missing:\n",
    "            if not seen_obs:\n",
    "                out[idx] = clip\n",
    "            else:\n",
    "                distance = idx - last_obs_index\n",
    "                out[idx] = distance if distance <= clip else clip\n",
    "        else:\n",
    "            out[idx] = 0\n",
    "            last_obs_index = idx\n",
    "            seen_obs = True\n",
    "    if not seen_obs:\n",
    "        out.fill(0)\n",
    "    return out\n",
    "\n",
    "def _run_length(mask: np.ndarray, clip: int, dtype: np.dtype, *, target_missing: bool) -> np.ndarray:\n",
    "    \"\"\"NaN \u307e\u305f\u306f\u89b3\u6e2c\u304c\u9023\u7d9a\u3059\u308b\u9577\u3055\u3092\u7b97\u51fa\u3059\u308b\u3002\"\"\"\n",
    "    out = np.zeros(mask.shape[0], dtype=dtype)\n",
    "    counter = 0\n",
    "    for idx, is_missing in enumerate(mask):\n",
    "        condition = is_missing if target_missing else not is_missing\n",
    "        if condition:\n",
    "            counter = counter + 1 if counter < clip else clip\n",
    "            out[idx] = counter\n",
    "        else:\n",
    "            counter = 0\n",
    "            out[idx] = 0\n",
    "    return out\n",
    "\n",
    "class SU1FeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\u751f\u30c7\u30fc\u30bf\u304b\u3089 SU1 \u6b20\u640d\u69cb\u9020\u7279\u5fb4\u91cf\u3092\u751f\u6210\u3059\u308b\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3002\"\"\"\n",
    "\n",
    "    def __init__(self, config: SU1Config):\n",
    "        self.config = config\n",
    "        self.feature_columns_: list[str] | None = None\n",
    "        self.group_columns_: Dict[str, list[str]] | None = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Any = None) -> \"SU1FeatureGenerator\":\n",
    "        df = self._ensure_dataframe(X)\n",
    "        self.feature_columns_ = self._select_feature_columns(df.columns)\n",
    "        self.group_columns_ = self._build_group_columns(self.feature_columns_)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.feature_columns_ is None or self.group_columns_ is None:\n",
    "            raise RuntimeError(\"The transformer must be fitted before calling transform().\")\n",
    "        df = self._ensure_dataframe(X)\n",
    "        missing_columns = [col for col in self.feature_columns_ if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise KeyError(f\"Input dataframe is missing columns required for SU1: {missing_columns}\")\n",
    "        feature_df = self._generate_features(df)\n",
    "        feature_df.index = df.index\n",
    "        return feature_df\n",
    "    # ------------------------------------------------------------------\n",
    "    # \u5185\u90e8\u30d8\u30eb\u30d1\u30fc\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _ensure_dataframe(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not isinstance(X, pd.DataFrame):  # pragma: no cover - \u9632\u5fa1\u7684\u5206\u5c90\n",
    "            raise TypeError(\"SU1FeatureGenerator expects a pandas.DataFrame input\")\n",
    "        return X.copy()\n",
    "\n",
    "    def _select_feature_columns(self, columns: Iterable[str]) -> list[str]:\n",
    "        selected: list[str] = []\n",
    "        for column in columns:\n",
    "            if column in self.config.exclude_columns:\n",
    "                continue\n",
    "            group = _infer_group(column)\n",
    "            if group and group in self.config.target_groups:\n",
    "                selected.append(column)\n",
    "        if not selected:\n",
    "            raise ValueError(\"No columns matched the SU1 configuration criteria.\")\n",
    "        return selected\n",
    "\n",
    "    def _build_group_columns(self, columns: Sequence[str]) -> Dict[str, list[str]]:\n",
    "        group_map: Dict[str, list[str]] = {group: [] for group in self.config.target_groups}\n",
    "        for column in columns:\n",
    "            group = _infer_group(column)\n",
    "            if group in group_map:\n",
    "                group_map[group].append(column)\n",
    "        return group_map\n",
    "\n",
    "    def _generate_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        feature_columns = self.feature_columns_\n",
    "        group_columns = self.group_columns_\n",
    "        if feature_columns is None or group_columns is None:\n",
    "            raise RuntimeError(\"The transformer must be fitted before generating features.\")\n",
    "        data = df[feature_columns]\n",
    "        mask = data.isna()\n",
    "        all_nan_series = mask.all(axis=0)\n",
    "        all_nan_lookup = all_nan_series.to_dict()\n",
    "        flag_df = mask.astype(self.config.flag_dtype)\n",
    "        flag_df.columns = [f\"m/{col}\" for col in feature_columns]\n",
    "        gap_data: MutableMapping[str, np.ndarray] = {}\n",
    "        run_na_data: MutableMapping[str, np.ndarray] = {}\n",
    "        run_obs_data: MutableMapping[str, np.ndarray] = {}\n",
    "        for column in feature_columns:\n",
    "            column_mask = mask[column].to_numpy(dtype=bool)\n",
    "            gap_values = _distance_from_last_observation(\n",
    "                column_mask, self.config.gap_clip, self.config.run_dtype\n",
    "            )\n",
    "            run_na_values = _run_length(\n",
    "                column_mask,\n",
    "                self.config.run_clip,\n",
    "                self.config.run_dtype,\n",
    "                target_missing=True,\n",
    "            )\n",
    "            run_obs_values = _run_length(column_mask, self.config.run_clip, self.config.run_dtype, target_missing=False)\n",
    "            gap_data[f\"gap_ffill/{column}\"] = _clip_array(gap_values, self.config.gap_clip)\n",
    "            run_na_data[f\"run_na/{column}\"] = _clip_array(run_na_values, self.config.run_clip)\n",
    "            run_obs_data[f\"run_obs/{column}\"] = _clip_array(run_obs_values, self.config.run_clip)\n",
    "        gap_df = pd.DataFrame(gap_data, index=data.index)\n",
    "        run_na_df = pd.DataFrame(run_na_data, index=data.index)\n",
    "        run_obs_df = pd.DataFrame(run_obs_data, index=data.index)\n",
    "        m_any_day = flag_df.sum(axis=\"columns\").astype(self.config.run_dtype)\n",
    "        m_rate_day = (m_any_day / len(feature_columns)).astype(np.float32)\n",
    "        group_features: Dict[str, pd.Series] = {\n",
    "            \"m_any_day\": m_any_day,\n",
    "            \"m_rate_day\": m_rate_day,\n",
    "            \"m_cnt/ALL\": m_any_day,\n",
    "            \"m_rate/ALL\": m_rate_day,\n",
    "        }\n",
    "        for group, columns in group_columns.items():\n",
    "            if not columns:\n",
    "                continue\n",
    "            flag_cols = [f\"m/{col}\" for col in columns]\n",
    "            group_count = flag_df[flag_cols].sum(axis=\"columns\").astype(self.config.run_dtype)\n",
    "            group_rate = (group_count / len(columns)).astype(np.float32)\n",
    "            group_features[f\"m_cnt/{group}\"] = group_count\n",
    "            group_features[f\"m_rate/{group}\"] = group_rate\n",
    "            if self.config.include_avg_gap:\n",
    "                gap_cols = [f\"gap_ffill/{col}\" for col in columns]\n",
    "                gap_values = gap_df[gap_cols].copy()\n",
    "                if self.config.exclude_all_nan_for_means:\n",
    "                    for orig_col, gap_col in zip(columns, gap_cols):\n",
    "                        if all_nan_lookup.get(orig_col, False):\n",
    "                            gap_values[gap_col] = np.nan\n",
    "                group_features[f\"avg_gapff/{group}\"] = gap_values.mean(axis=1).astype(np.float32)\n",
    "            if self.config.include_avg_run:\n",
    "                run_cols = [f\"run_na/{col}\" for col in columns]\n",
    "                run_values = run_na_df[run_cols].copy()\n",
    "                if self.config.exclude_all_nan_for_means:\n",
    "                    for orig_col, run_col in zip(columns, run_cols):\n",
    "                        if all_nan_lookup.get(orig_col, False):\n",
    "                            run_values[run_col] = np.nan\n",
    "                group_features[f\"avg_run_na/{group}\"] = run_values.mean(axis=1).astype(np.float32)\n",
    "        aggregated_df = pd.DataFrame(group_features, index=data.index)\n",
    "        output_frames = [flag_df, gap_df, run_na_df, run_obs_df, aggregated_df]\n",
    "        return pd.concat(output_frames, axis=1)\n",
    "\n",
    "    def __init__(self, config: SU1Config):\n",
    "        self.config = config\n",
    "        self.feature_columns_: list[str] | None = None\n",
    "        self.group_columns_: Dict[str, list[str]] | None = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Any = None) -> \"SU1FeatureGenerator\":\n",
    "        df = self._ensure_dataframe(X)\n",
    "        self.feature_columns_ = self._select_feature_columns(df.columns)\n",
    "        self.group_columns_ = self._build_group_columns(self.feature_columns_)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.feature_columns_ is None or self.group_columns_ is None:\n",
    "            raise RuntimeError(\"The transformer must be fitted before calling transform().\")\n",
    "        df = self._ensure_dataframe(X)\n",
    "        missing_columns = [col for col in self.feature_columns_ if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise KeyError(f\"Input dataframe is missing columns required for SU1: {missing_columns}\")\n",
    "        feature_df = self._generate_features(df)\n",
    "        feature_df.index = df.index\n",
    "        return feature_df\n",
    "    # ------------------------------------------------------------------\n",
    "    # \u5185\u90e8\u30d8\u30eb\u30d1\u30fc\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _ensure_dataframe(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not isinstance(X, pd.DataFrame):  # pragma: no cover - \u9632\u5fa1\u7684\u5206\u5c90\n",
    "            raise TypeError(\"SU1FeatureGenerator expects a pandas.DataFrame input\")\n",
    "        return X.copy()\n",
    "\n",
    "    def _select_feature_columns(self, columns: Iterable[str]) -> list[str]:\n",
    "        selected: list[str] = []\n",
    "        for column in columns:\n",
    "            if column in self.config.exclude_columns:\n",
    "                continue\n",
    "            group = _infer_group(column)\n",
    "            if group and group in self.config.target_groups:\n",
    "                selected.append(column)\n",
    "        if not selected:\n",
    "            raise ValueError(\"No columns matched the SU1 configuration criteria.\")\n",
    "        return selected\n",
    "\n",
    "    def _build_group_columns(self, columns: Sequence[str]) -> Dict[str, list[str]]:\n",
    "        group_map: Dict[str, list[str]] = {group: [] for group in self.config.target_groups}\n",
    "        for column in columns:\n",
    "            group = _infer_group(column)\n",
    "            if group in group_map:\n",
    "                group_map[group].append(column)\n",
    "        return group_map\n",
    "\n",
    "    def _generate_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        feature_columns = self.feature_columns_\n",
    "        group_columns = self.group_columns_\n",
    "        if feature_columns is None or group_columns is None:\n",
    "            raise RuntimeError(\"The transformer must be fitted before generating features.\")\n",
    "        data = df[feature_columns]\n",
    "        mask = data.isna()\n",
    "        all_nan_series = mask.all(axis=0)\n",
    "        all_nan_lookup = all_nan_series.to_dict()\n",
    "        flag_df = mask.astype(self.config.flag_dtype)\n",
    "        flag_df.columns = [f\"m/{col}\" for col in feature_columns]\n",
    "        gap_data: MutableMapping[str, np.ndarray] = {}\n",
    "        run_na_data: MutableMapping[str, np.ndarray] = {}\n",
    "        run_obs_data: MutableMapping[str, np.ndarray] = {}\n",
    "        for column in feature_columns:\n",
    "            column_mask = mask[column].to_numpy(dtype=bool)\n",
    "            gap_values = _distance_from_last_observation(\n",
    "                column_mask, self.config.gap_clip, self.config.run_dtype\n",
    "            )\n",
    "            run_na_values = _run_length(\n",
    "                column_mask,\n",
    "                self.config.run_clip,\n",
    "                self.config.run_dtype,\n",
    "                target_missing=True,\n",
    "            )\n",
    "            run_obs_values = _run_length(column_mask, self.config.run_clip, self.config.run_dtype, target_missing=False)\n",
    "            gap_data[f\"gap_ffill/{column}\"] = _clip_array(gap_values, self.config.gap_clip)\n",
    "            run_na_data[f\"run_na/{column}\"] = _clip_array(run_na_values, self.config.run_clip)\n",
    "            run_obs_data[f\"run_obs/{column}\"] = _clip_array(run_obs_values, self.config.run_clip)\n",
    "        gap_df = pd.DataFrame(gap_data, index=data.index)\n",
    "        run_na_df = pd.DataFrame(run_na_data, index=data.index)\n",
    "        run_obs_df = pd.DataFrame(run_obs_data, index=data.index)\n",
    "        m_any_day = flag_df.sum(axis=\"columns\").astype(self.config.run_dtype)\n",
    "        m_rate_day = (m_any_day / len(feature_columns)).astype(np.float32)\n",
    "        group_features: Dict[str, pd.Series] = {\n",
    "            \"m_any_day\": m_any_day,\n",
    "            \"m_rate_day\": m_rate_day,\n",
    "            \"m_cnt/ALL\": m_any_day,\n",
    "            \"m_rate/ALL\": m_rate_day,\n",
    "        }\n",
    "        for group, columns in group_columns.items():\n",
    "            if not columns:\n",
    "                continue\n",
    "            flag_cols = [f\"m/{col}\" for col in columns]\n",
    "            group_count = flag_df[flag_cols].sum(axis=\"columns\").astype(self.config.run_dtype)\n",
    "            group_rate = (group_count / len(columns)).astype(np.float32)\n",
    "            group_features[f\"m_cnt/{group}\"] = group_count\n",
    "            group_features[f\"m_rate/{group}\"] = group_rate\n",
    "            if self.config.include_avg_gap:\n",
    "                gap_cols = [f\"gap_ffill/{col}\" for col in columns]\n",
    "                gap_values = gap_df[gap_cols].copy()\n",
    "                if self.config.exclude_all_nan_for_means:\n",
    "                    for orig_col, gap_col in zip(columns, gap_cols):\n",
    "                        if all_nan_lookup.get(orig_col, False):\n",
    "                            gap_values[gap_col] = np.nan\n",
    "                group_features[f\"avg_gapff/{group}\"] = gap_values.mean(axis=1).astype(np.float32)\n",
    "            if self.config.include_avg_run:\n",
    "                run_cols = [f\"run_na/{col}\" for col in columns]\n",
    "                run_values = run_na_df[run_cols].copy()\n",
    "                if self.config.exclude_all_nan_for_means:\n",
    "                    for orig_col, run_col in zip(columns, run_cols):\n",
    "                        if all_nan_lookup.get(orig_col, False):\n",
    "                            run_values[run_col] = np.nan\n",
    "                group_features[f\"avg_run_na/{group}\"] = run_values.mean(axis=1).astype(np.float32)\n",
    "        aggregated_df = pd.DataFrame(group_features, index=data.index)\n",
    "        output_frames = [flag_df, gap_df, run_na_df, run_obs_df, aggregated_df]\n",
    "        return pd.concat(output_frames, axis=1)\n",
    "\n",
    "# === Module: src.feature_generation.su5.feature_su5 ===\n",
    "\n",
    "def _infer_group_su5(column_name: str) -> str | None:\n",
    "    \"\"\"\u5217\u540d\u306e\u63a5\u982d\u8f9e\u304b\u3089\u7279\u5fb4\u30b0\u30eb\u30fc\u30d7\u3092\u63a8\u5b9a\u3059\u308b\u3002\"\"\"\n",
    "    prefix_chars: list[str] = []\n",
    "    for char in column_name:\n",
    "        if char.isalpha() and char.isupper():\n",
    "            prefix_chars.append(char)\n",
    "            continue\n",
    "        if char.isdigit():\n",
    "            break\n",
    "        return None\n",
    "    if not prefix_chars:\n",
    "        return None\n",
    "    return \"\".join(prefix_chars)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SU5Config:\n",
    "    \"\"\"SU5\uff08\u5171\u6b20\u640d\u69cb\u9020\uff09\u7279\u5fb4\u751f\u6210\u306e\u8a2d\u5b9a\u3092\u4fdd\u6301\u3059\u308b\u30c7\u30fc\u30bf\u30af\u30e9\u30b9\u3002\"\"\"\n",
    "    id_column: str\n",
    "    output_prefix: str\n",
    "    top_k_pairs: int\n",
    "    top_k_pairs_per_group: int | None\n",
    "    windows: Tuple[int, ...]\n",
    "    reset_each_fold: bool\n",
    "    dtype_flag: np.dtype\n",
    "    dtype_int: np.dtype\n",
    "    dtype_float: np.dtype\n",
    "\n",
    "\n",
    "class SU5FeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"SU5 \u5171\u6b20\u640d\u7279\u5fb4\u91cf\u751f\u6210\u5668\u3002\"\"\"\n",
    "    \n",
    "    def __init__(self, config: SU5Config):\n",
    "        self.config = config\n",
    "        self.m_columns_: list[str] | None = None\n",
    "        self.groups_: dict[str, list[str]] | None = None\n",
    "        self.top_pairs_: list[tuple[str, str]] | None = None\n",
    "        self.feature_names_: list[str] | None = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Any = None):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"SU5FeatureGenerator expects a pandas.DataFrame input\")\n",
    "        self.m_columns_ = sorted([c for c in X.columns if c.startswith(\"m/\")])\n",
    "        if not self.m_columns_:\n",
    "            raise ValueError(\"No 'm/' columns found in input. SU5 requires SU1 features as input.\")\n",
    "        self.groups_ = self._extract_groups()\n",
    "        self.top_pairs_ = self._select_top_k_pairs(X)\n",
    "        self.feature_names_ = self._build_feature_names()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, fold_indices: np.ndarray | None = None):\n",
    "        if self.m_columns_ is None or self.top_pairs_ is None:\n",
    "            raise RuntimeError(\"The transformer must be fitted before calling transform().\")\n",
    "        n = len(X)\n",
    "        features: dict[str, np.ndarray] = {}\n",
    "        fold_boundaries = self._compute_fold_boundaries(n, fold_indices)\n",
    "        co_now = self._compute_co_miss_now(X, fold_boundaries)\n",
    "        features.update(co_now)\n",
    "        co_roll = self._compute_co_miss_rollrate(features, fold_boundaries)\n",
    "        features.update(co_roll)\n",
    "        co_deg = self._compute_co_miss_degree(X)\n",
    "        features.update(co_deg)\n",
    "        return pd.DataFrame(features, index=X.index)\n",
    "\n",
    "    def _extract_groups(self):\n",
    "        groups: dict[str, list[str]] = {}\n",
    "        if self.m_columns_ is None:\n",
    "            return groups\n",
    "        for col in self.m_columns_:\n",
    "            base_col = col[2:]\n",
    "            group = _infer_group_su5(base_col)\n",
    "            if group:\n",
    "                if group not in groups:\n",
    "                    groups[group] = []\n",
    "                groups[group].append(base_col)\n",
    "        return groups\n",
    "\n",
    "    def _select_top_k_pairs(self, X: pd.DataFrame):\n",
    "        if self.m_columns_ is None:\n",
    "            return []\n",
    "        pair_scores: list[tuple[float, str, str]] = []\n",
    "        for i, col_a in enumerate(self.m_columns_):\n",
    "            for col_b in self.m_columns_[i + 1 :]:\n",
    "                m_a = np.asarray(X[col_a].values)\n",
    "                m_b = np.asarray(X[col_b].values)\n",
    "                both_na = int(np.sum((m_a == 1) & (m_b == 1)))\n",
    "                either_na = int(np.sum((m_a == 1) | (m_b == 1)))\n",
    "                if either_na > 0:\n",
    "                    score = float(both_na) / float(either_na)\n",
    "                else:\n",
    "                    score = 0.0\n",
    "                base_a = col_a[2:]\n",
    "                base_b = col_b[2:]\n",
    "                pair_scores.append((score, base_a, base_b))\n",
    "        pair_scores.sort(reverse=True)\n",
    "        if self.config.top_k_pairs_per_group is not None:\n",
    "            selected_pairs = self._select_top_k_per_group(pair_scores)\n",
    "        else:\n",
    "            selected_pairs = [(a, b) for _, a, b in pair_scores[: self.config.top_k_pairs]]\n",
    "        return selected_pairs\n",
    "\n",
    "    def _select_top_k_per_group(self, pair_scores):\n",
    "        if self.groups_ is None or self.config.top_k_pairs_per_group is None:\n",
    "            return []\n",
    "        selected: list[tuple[str, str]] = []\n",
    "        group_counts: dict[str, int] = {grp: 0 for grp in self.groups_.keys()}\n",
    "        for score, col_a, col_b in pair_scores:\n",
    "            group_a = _infer_group_su5(col_a)\n",
    "            group_b = _infer_group_su5(col_b)\n",
    "            if group_a == group_b and group_a is not None:\n",
    "                if group_counts[group_a] < self.config.top_k_pairs_per_group:\n",
    "                    selected.append((col_a, col_b))\n",
    "                    group_counts[group_a] += 1\n",
    "        return selected\n",
    "\n",
    "    def _build_feature_names(self):\n",
    "        names: list[str] = []\n",
    "        if self.top_pairs_ is None:\n",
    "            return names\n",
    "        for col_a, col_b in self.top_pairs_:\n",
    "            names.append(f\"co_miss_now/{col_a}__{col_b}\")\n",
    "        for window in self.config.windows:\n",
    "            for col_a, col_b in self.top_pairs_:\n",
    "                names.append(f\"co_miss_rollrate_{window}/{col_a}__{col_b}\")\n",
    "        if self.m_columns_ is not None:\n",
    "            for col in self.m_columns_:\n",
    "                base_col = col[2:]\n",
    "                names.append(f\"co_miss_deg/{base_col}\")\n",
    "        return names\n",
    "\n",
    "    def _compute_fold_boundaries(self, n_rows: int, fold_indices: np.ndarray | None):\n",
    "        if fold_indices is None or not self.config.reset_each_fold:\n",
    "            return [(0, n_rows)]\n",
    "        boundaries = []\n",
    "        unique_folds = np.unique(fold_indices)\n",
    "        for fold_id in unique_folds:\n",
    "            fold_mask = fold_indices == fold_id\n",
    "            indices = np.where(fold_mask)[0]\n",
    "            if len(indices) > 0:\n",
    "                boundaries.append((int(indices[0]), int(indices[-1]) + 1))\n",
    "        return boundaries if boundaries else [(0, n_rows)]\n",
    "\n",
    "    def _compute_co_miss_now(self, X: pd.DataFrame, fold_boundaries):\n",
    "        features: dict[str, np.ndarray] = {}\n",
    "        if self.top_pairs_ is None:\n",
    "            return features\n",
    "        for col_a, col_b in self.top_pairs_:\n",
    "            m_a = np.asarray(X[f\"m/{col_a}\"].values)\n",
    "            m_b = np.asarray(X[f\"m/{col_b}\"].values)\n",
    "            co_miss = ((m_a == 1) & (m_b == 1)).astype(self.config.dtype_flag)\n",
    "            features[f\"co_miss_now/{col_a}__{col_b}\"] = co_miss\n",
    "        return features\n",
    "\n",
    "    def _compute_co_miss_rollrate(self, features: dict[str, np.ndarray], fold_boundaries):\n",
    "        rollrate_features: dict[str, np.ndarray] = {}\n",
    "        if self.top_pairs_ is None:\n",
    "            return rollrate_features\n",
    "        for window in self.config.windows:\n",
    "            for col_a, col_b in self.top_pairs_:\n",
    "                co_miss_now = features[f\"co_miss_now/{col_a}__{col_b}\"]\n",
    "                n = len(co_miss_now)\n",
    "                rollrate = np.full(n, np.nan, dtype=self.config.dtype_float)\n",
    "                for start_idx, end_idx in fold_boundaries:\n",
    "                    for i in range(start_idx, end_idx):\n",
    "                        window_start = max(start_idx, i - window + 1)\n",
    "                        window_end = i + 1\n",
    "                        if window_end - window_start >= window:\n",
    "                            window_values = co_miss_now[window_start:window_end]\n",
    "                            rollrate[i] = np.mean(window_values)\n",
    "                rollrate_features[f\"co_miss_rollrate_{window}/{col_a}__{col_b}\"] = rollrate\n",
    "        return rollrate_features\n",
    "\n",
    "    def _compute_co_miss_degree(self, X: pd.DataFrame):\n",
    "        degree_features: dict[str, np.ndarray] = {}\n",
    "        if self.m_columns_ is None or self.top_pairs_ is None:\n",
    "            return degree_features\n",
    "        degree_counts: dict[str, int] = {col[2:]: 0 for col in self.m_columns_}\n",
    "        for col_a, col_b in self.top_pairs_:\n",
    "            degree_counts[col_a] += 1\n",
    "            degree_counts[col_b] += 1\n",
    "        n = len(X)\n",
    "        for col in self.m_columns_:\n",
    "            base_col = col[2:]\n",
    "            degree_value = degree_counts[base_col]\n",
    "            degree_features[f\"co_miss_deg/{base_col}\"] = np.full(\n",
    "                n, degree_value, dtype=self.config.dtype_int\n",
    "            )\n",
    "        return degree_features\n",
    "\n",
    "\n",
    "class SU5FeatureAugmenter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"SU5\u7279\u5fb4\u91cf\u3092SU1\u306e\u4e0a\u306b\u8ffd\u52a0\u3059\u308b\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3002\"\"\"\n",
    "    \n",
    "    def __init__(self, su1_config: SU1Config, su5_config: SU5Config, fill_value: float | None = 0.0) -> None:\n",
    "        self.su1_config = su1_config\n",
    "        self.su5_config = su5_config\n",
    "        self.fill_value = fill_value\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Any = None):\n",
    "        frame = self._ensure_dataframe(X)\n",
    "        # SU1 fit\n",
    "        self.su1_generator_ = SU1FeatureGenerator(self.su1_config)\n",
    "        self.su1_generator_.fit(frame)\n",
    "        su1_features = self.su1_generator_.transform(frame)\n",
    "        # SU5 fit (using SU1 features)\n",
    "        self.su5_generator_ = SU5FeatureGenerator(self.su5_config)\n",
    "        self.su5_generator_.fit(su1_features)\n",
    "        su5_features = self.su5_generator_.transform(su1_features)\n",
    "        # Store feature names\n",
    "        self.su1_feature_names_ = list(su1_features.columns)\n",
    "        self.su5_feature_names_ = list(su5_features.columns)\n",
    "        self.input_columns_ = list(frame.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, fold_indices: np.ndarray | None = None):\n",
    "        if not hasattr(self, \"su1_generator_\"):\n",
    "            raise RuntimeError(\"SU5FeatureAugmenter must be fitted before transform().\")\n",
    "        frame = self._ensure_dataframe(X)\n",
    "        # Generate SU1 features\n",
    "        su1_features = self.su1_generator_.transform(frame)\n",
    "        su1_features = su1_features.reindex(columns=self.su1_feature_names_, copy=True)\n",
    "        if self.fill_value is not None:\n",
    "            su1_features = su1_features.fillna(self.fill_value)\n",
    "        # Generate SU5 features\n",
    "        su5_features = self.su5_generator_.transform(su1_features, fold_indices=fold_indices)\n",
    "        su5_features = su5_features.reindex(columns=self.su5_feature_names_, copy=True)\n",
    "        if self.fill_value is not None:\n",
    "            su5_features = su5_features.fillna(self.fill_value)\n",
    "        # Concatenate: original + SU1 + SU5\n",
    "        augmented = pd.concat([frame, su1_features, su5_features], axis=1)\n",
    "        augmented.index = frame.index\n",
    "        return augmented\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dataframe(X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"SU5FeatureAugmenter expects a pandas.DataFrame input\")\n",
    "        return X.copy()\n",
    "\n",
    "\n",
    "    def _select_feature_columns(self, columns: Iterable[str]) -> list[str]:\n",
    "        selected: list[str] = []\n",
    "        for column in columns:\n",
    "            if column in self.config.exclude_columns:\n",
    "                continue\n",
    "            group = _infer_group(column)\n",
    "            if group and group in self.config.target_groups:\n",
    "                selected.append(column)\n",
    "        if not selected:\n",
    "            raise ValueError(\"No columns matched the SU1 configuration criteria.\")\n",
    "        return selected\n",
    "\n",
    "    def _build_group_columns(self, columns: Sequence[str]) -> Dict[str, list[str]]:\n",
    "        group_map: Dict[str, list[str]] = {group: [] for group in self.config.target_groups}\n",
    "        for column in columns:\n",
    "            group = _infer_group(column)\n",
    "            if group in group_map:\n",
    "                group_map[group].append(column)\n",
    "        return group_map\n",
    "\n",
    "    def _generate_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        feature_columns = self.feature_columns_\n",
    "        group_columns = self.group_columns_\n",
    "        if feature_columns is None or group_columns is None:\n",
    "            raise RuntimeError(\"The transformer must be fitted before generating features.\")\n",
    "        data = df[feature_columns]\n",
    "        mask = data.isna()\n",
    "        all_nan_series = mask.all(axis=0)\n",
    "        all_nan_lookup = all_nan_series.to_dict()\n",
    "        flag_df = mask.astype(self.config.flag_dtype)\n",
    "        flag_df.columns = [f\"m/{col}\" for col in feature_columns]\n",
    "        gap_data: MutableMapping[str, np.ndarray] = {}\n",
    "        run_na_data: MutableMapping[str, np.ndarray] = {}\n",
    "        run_obs_data: MutableMapping[str, np.ndarray] = {}\n",
    "        for column in feature_columns:\n",
    "            column_mask = mask[column].to_numpy(dtype=bool)\n",
    "            gap_values = _distance_from_last_observation(\n",
    "                column_mask, self.config.gap_clip, self.config.run_dtype\n",
    "            )\n",
    "            run_na_values = _run_length(\n",
    "                column_mask,\n",
    "                self.config.run_clip,\n",
    "                self.config.run_dtype,\n",
    "                target_missing=True,\n",
    "            )\n",
    "            run_obs_values = _run_length(column_mask, self.config.run_clip, self.config.run_dtype, target_missing=False)\n",
    "            gap_data[f\"gap_ffill/{column}\"] = _clip_array(gap_values, self.config.gap_clip)\n",
    "            run_na_data[f\"run_na/{column}\"] = _clip_array(run_na_values, self.config.run_clip)\n",
    "            run_obs_data[f\"run_obs/{column}\"] = _clip_array(run_obs_values, self.config.run_clip)\n",
    "        gap_df = pd.DataFrame(gap_data, index=data.index)\n",
    "        run_na_df = pd.DataFrame(run_na_data, index=data.index)\n",
    "        run_obs_df = pd.DataFrame(run_obs_data, index=data.index)\n",
    "        m_any_day = flag_df.sum(axis=\"columns\").astype(self.config.run_dtype)\n",
    "        m_rate_day = (m_any_day / len(feature_columns)).astype(np.float32)\n",
    "        group_features: Dict[str, pd.Series] = {\n",
    "            \"m_any_day\": m_any_day,\n",
    "            \"m_rate_day\": m_rate_day,\n",
    "            \"m_cnt/ALL\": m_any_day,\n",
    "            \"m_rate/ALL\": m_rate_day,\n",
    "        }\n",
    "        for group, columns in group_columns.items():\n",
    "            if not columns:\n",
    "                continue\n",
    "            flag_cols = [f\"m/{col}\" for col in columns]\n",
    "            group_count = flag_df[flag_cols].sum(axis=\"columns\").astype(self.config.run_dtype)\n",
    "            group_rate = (group_count / len(columns)).astype(np.float32)\n",
    "            group_features[f\"m_cnt/{group}\"] = group_count\n",
    "            group_features[f\"m_rate/{group}\"] = group_rate\n",
    "            if self.config.include_avg_gap:\n",
    "                gap_cols = [f\"gap_ffill/{col}\" for col in columns]\n",
    "                gap_values = gap_df[gap_cols].copy()\n",
    "                if self.config.exclude_all_nan_for_means:\n",
    "                    for orig_col, gap_col in zip(columns, gap_cols):\n",
    "                        if all_nan_lookup.get(orig_col, False):\n",
    "                            gap_values[gap_col] = np.nan\n",
    "                group_features[f\"avg_gapff/{group}\"] = gap_values.mean(axis=1).astype(np.float32)\n",
    "            if self.config.include_avg_run:\n",
    "                run_cols = [f\"run_na/{col}\" for col in columns]\n",
    "                run_values = run_na_df[run_cols].copy()\n",
    "                if self.config.exclude_all_nan_for_means:\n",
    "                    for orig_col, run_col in zip(columns, run_cols):\n",
    "                        if all_nan_lookup.get(orig_col, False):\n",
    "                            run_values[run_col] = np.nan\n",
    "                group_features[f\"avg_run_na/{group}\"] = run_values.mean(axis=1).astype(np.float32)\n",
    "        aggregated_df = pd.DataFrame(group_features, index=data.index)\n",
    "        output_frames = [flag_df, gap_df, run_na_df, run_obs_df, aggregated_df]\n",
    "        return pd.concat(output_frames, axis=1)\n",
    "\n",
    "def generate_su1_features(\n",
    "    config_path: str | Path,\n",
    "    *,\n",
    "    dataset: Literal[\"train\", \"test\"] = \"train\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"High-level helper to produce SU1 features from raw data.\"\"\"\n",
    "    config = load_su1_config(config_path)\n",
    "    raw_df = load_raw_dataset(config, dataset=dataset)\n",
    "    generator = SU1FeatureGenerator(config)\n",
    "    generator.fit(raw_df)\n",
    "    return generator.transform(raw_df)\n",
    "__all__ = [\n",
    "    \"SU1Config\",\n",
    "    \"SU1FeatureGenerator\",\n",
    "    \"generate_su1_features\",\n",
    "    \"load_raw_dataset\",\n",
    "    \"load_su1_config\",\n",
    "]\n",
    "\n",
    "# === Module: src.feature_generation.su1.train_su1 (subset) ===\n",
    "\n",
    "class SU1FeatureAugmenter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\u4e0b\u6d41\u524d\u51e6\u7406\u306e\u524d\u306b SU1 \u7279\u5fb4\u91cf\u3092\u5165\u529b\u30d5\u30ec\u30fc\u30e0\u3078\u8ffd\u52a0\u3059\u308b\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3002\n",
    "    :class:`SU1FeatureGenerator` \u306e\u6319\u52d5\u3092\u4fdd\u3063\u305f\u307e\u307e\u5217\u9806\u5236\u5fa1\u306a\u3069\u306e\u5229\u4fbf\u6027\u3092\u88dc\u3044\u3001``fit`` \u6642\u306b\n",
    "    \u5fc5\u8981\u306a\u751f\u5217\u304a\u3088\u3073\u751f\u6210\u5217\u306e\u9806\u5e8f\u3092\u8a18\u9332\u3057\u3066 ``transform`` \u3067\u6c7a\u5b9a\u7684\u306a\u914d\u7f6e\u3092\u518d\u73fe\u3059\u308b\u3002\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: SU1Config, fill_value: float | None = 0.0) -> None:\n",
    "        self.config = config\n",
    "        self.fill_value = fill_value\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Any = None) -> \"SU1FeatureAugmenter\":\n",
    "        frame = self._ensure_dataframe(X)\n",
    "        generator = SU1FeatureGenerator(self.config)\n",
    "        generator.fit(frame)\n",
    "        features = generator.transform(frame)\n",
    "        if self.fill_value is not None:\n",
    "            features = features.fillna(self.fill_value)\n",
    "        # transform \u6642\u306b\u518d\u5229\u7528\u3059\u308b\u5185\u90e8\u72b6\u614b\u3092\u4fdd\u6301\u3059\u308b\u3002\n",
    "        self.generator_ = generator\n",
    "        self.su1_feature_names_ = list(features.columns)\n",
    "        self.input_columns_ = list(frame.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not hasattr(self, \"generator_\"):\n",
    "            raise RuntimeError(\"SU1FeatureAugmenter must be fitted before transform().\")\n",
    "        frame = self._ensure_dataframe(X)\n",
    "        features = self.generator_.transform(frame)\n",
    "        # fit/transform \u306e\u5883\u754c\u3092\u8de8\u3044\u3067\u3082\u5217\u9806\u304c\u5909\u308f\u3089\u306a\u3044\u3088\u3046\u306b\u63c3\u3048\u308b\u3002\n",
    "        features = features.reindex(columns=self.su1_feature_names_, copy=True)\n",
    "        if self.fill_value is not None:\n",
    "            features = features.fillna(self.fill_value)\n",
    "        augmented = pd.concat([frame, features], axis=1)\n",
    "        augmented.index = frame.index\n",
    "        return augmented\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def _ensure_dataframe(X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not isinstance(X, pd.DataFrame):  # pragma: no cover - \u9632\u5fa1\u7684\u306a\u5206\u5c90\n",
    "            raise TypeError(\"SU1FeatureAugmenter expects a pandas.DataFrame input\")\n",
    "        return X.copy()\n",
    "\n",
    "def _ensure_package(name: str) -> types.ModuleType:\n",
    "    if name in sys.modules:\n",
    "        return sys.modules[name]\n",
    "    module = types.ModuleType(name)\n",
    "    module.__path__ = []\n",
    "    sys.modules[name] = module\n",
    "    parent, _, child = name.rpartition(\".\")\n",
    "    if parent:\n",
    "        parent_pkg = _ensure_package(parent)\n",
    "        setattr(parent_pkg, child, module)\n",
    "    return module\n",
    "preprocess_pkg = _ensure_package(\"preprocess\")\n",
    "M_pkg = _ensure_package(\"preprocess.M_group\")\n",
    "E_pkg = _ensure_package(\"preprocess.E_group\")\n",
    "I_pkg = _ensure_package(\"preprocess.I_group\")\n",
    "P_pkg = _ensure_package(\"preprocess.P_group\")\n",
    "S_pkg = _ensure_package(\"preprocess.S_group\")\n",
    "m_module = types.ModuleType(\"preprocess.M_group.m_group\")\n",
    "m_module.MGroupImputer = MGroupImputer\n",
    "sys.modules[\"preprocess.M_group.m_group\"] = m_module\n",
    "M_pkg.m_group = m_module\n",
    "e_module = types.ModuleType(\"preprocess.E_group.e_group\")\n",
    "e_module.EGroupImputer = EGroupImputer\n",
    "sys.modules[\"preprocess.E_group.e_group\"] = e_module\n",
    "E_pkg.e_group = e_module\n",
    "i_module = types.ModuleType(\"preprocess.I_group.i_group\")\n",
    "i_module.IGroupImputer = IGroupImputer\n",
    "sys.modules[\"preprocess.I_group.i_group\"] = i_module\n",
    "I_pkg.i_group = i_module\n",
    "p_module = types.ModuleType(\"preprocess.P_group.p_group\")\n",
    "p_module.PGroupImputer = PGroupImputer\n",
    "sys.modules[\"preprocess.P_group.p_group\"] = p_module\n",
    "P_pkg.p_group = p_module\n",
    "s_module = types.ModuleType(\"preprocess.S_group.s_group\")\n",
    "s_module.SGroupImputer = SGroupImputer\n",
    "sys.modules[\"preprocess.S_group.s_group\"] = s_module\n",
    "S_pkg.s_group = s_module\n",
    "preprocess_pkg.M_group = M_pkg\n",
    "preprocess_pkg.E_group = E_pkg\n",
    "preprocess_pkg.I_group = I_pkg\n",
    "preprocess_pkg.P_group = P_pkg\n",
    "preprocess_pkg.S_group = S_pkg\n",
    "htmpre_pkg = _ensure_package(\"htmpre\")\n",
    "htmpre_m = types.ModuleType(\"htmpre.m_group\")\n",
    "htmpre_m.MGroupImputer = MGroupImputer\n",
    "sys.modules[\"htmpre.m_group\"] = htmpre_m\n",
    "htmpre_pkg.m_group = htmpre_m\n",
    "htmpre_e = types.ModuleType(\"htmpre.e_group\")\n",
    "htmpre_e.EGroupImputer = EGroupImputer\n",
    "sys.modules[\"htmpre.e_group\"] = htmpre_e\n",
    "htmpre_pkg.e_group = htmpre_e\n",
    "htmpre_i = types.ModuleType(\"htmpre.i_group\")\n",
    "htmpre_i.IGroupImputer = IGroupImputer\n",
    "sys.modules[\"htmpre.i_group\"] = htmpre_i\n",
    "htmpre_pkg.i_group = htmpre_i\n",
    "htmpre_p = types.ModuleType(\"htmpre.p_group\")\n",
    "htmpre_p.PGroupImputer = PGroupImputer\n",
    "sys.modules[\"htmpre.p_group\"] = htmpre_p\n",
    "htmpre_pkg.p_group = htmpre_p\n",
    "htmpre_s = types.ModuleType(\"htmpre.s_group\")\n",
    "htmpre_s.SGroupImputer = SGroupImputer\n",
    "sys.modules[\"htmpre.s_group\"] = htmpre_s\n",
    "htmpre_pkg.s_group = htmpre_s\n",
    "src_pkg = _ensure_package(\"src\")\n",
    "feature_pkg = _ensure_package(\"src.feature_generation\")\n",
    "su1_pkg = _ensure_package(\"src.feature_generation.su1\")\n",
    "su1_feature_module = types.ModuleType(\"src.feature_generation.su1.feature_su1\")\n",
    "su1_feature_module.SU1Config = SU1Config\n",
    "su1_feature_module.SU1FeatureGenerator = SU1FeatureGenerator\n",
    "su1_feature_module.load_su1_config = load_su1_config\n",
    "su1_feature_module.load_raw_dataset = load_raw_dataset\n",
    "sys.modules[\"src.feature_generation.su1.feature_su1\"] = su1_feature_module\n",
    "su1_pkg.feature_su1 = su1_feature_module\n",
    "su1_train_module = types.ModuleType(\"src.feature_generation.su1.train_su1\")\n",
    "su1_train_module.SU1FeatureAugmenter = SU1FeatureAugmenter\n",
    "sys.modules[\"src.feature_generation.su1.train_su1\"] = su1_train_module\n",
    "\n",
    "# Register SU5 module\n",
    "su5_pkg = _ensure_package(\"src.feature_generation.su5\")\n",
    "su5_feature_module = types.ModuleType(\"src.feature_generation.su5.feature_su5\")\n",
    "su5_feature_module.SU5Config = SU5Config\n",
    "su5_feature_module.SU5FeatureGenerator = SU5FeatureGenerator\n",
    "su5_feature_module.SU5FeatureAugmenter = SU5FeatureAugmenter\n",
    "su5_feature_module._infer_group_su5 = _infer_group_su5\n",
    "sys.modules[\"src.feature_generation.su5.feature_su5\"] = su5_feature_module\n",
    "sys.modules[\"src.feature_generation.su5\"] = su5_pkg\n",
    "su5_pkg.feature_su5 = su5_feature_module\n",
    "feature_pkg.su5 = su5_pkg\n",
    "\n",
    "# Register SU5 train_su5 module (for evaluate_baseline.py bundles)\n",
    "su5_train_module = types.ModuleType(\"src.feature_generation.su5.train_su5\")\n",
    "su5_train_module.SU5FeatureAugmenter = SU5FeatureAugmenter\n",
    "su5_train_module.SU1FeatureGenerator = SU1FeatureGenerator\n",
    "su5_train_module.SU5FeatureGenerator = SU5FeatureGenerator\n",
    "su5_train_module.SU1Config = SU1Config\n",
    "su5_train_module.SU5Config = SU5Config\n",
    "sys.modules[\"src.feature_generation.su5.train_su5\"] = su5_train_module\n",
    "su5_pkg.train_su5 = su5_train_module\n",
    "\n",
    "su1_pkg.train_su1 = su1_train_module\n",
    "BUNDLE_PATH = ARTIFACT_ROOT / \"inference_bundle.pkl\"\n",
    "META_PATH = ARTIFACT_ROOT / \"model_meta.json\"\n",
    "FEATURE_LIST_PATH = ARTIFACT_ROOT / \"feature_list.json\"\n",
    "for required_path in (BUNDLE_PATH, META_PATH, FEATURE_LIST_PATH):\n",
    "    if not required_path.exists():\n",
    "        raise FileNotFoundError(f\"Required artifact not found: {required_path}\")\n",
    "with META_PATH.open(\"r\", encoding=\"utf-8\") as fp:\n",
    "    META = json.load(fp)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TwoHeadPositionConfig:\n",
    "    \"\"\"Configuration for two-head position mapping.\"\"\"\n",
    "    x: float\n",
    "    clip_min: float = 0.0\n",
    "    clip_max: float = 2.0\n",
    "    epsilon: float = 1e-8\n",
    "\n",
    "\n",
    "def map_positions_from_forward_rf(\n",
    "    forward_pred: np.ndarray,\n",
    "    rf_pred: np.ndarray,\n",
    "    x: float,\n",
    "    clip_min: float = 0.0,\n",
    "    clip_max: float = 2.0,\n",
    "    epsilon: float = 1e-8,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Map predictions to positions using two-head formula.\n",
    "    \n",
    "    Formula: position = clip((x - rf_pred) / (forward_pred - rf_pred), clip_min, clip_max)\n",
    "    \"\"\"\n",
    "    forward_pred = np.asarray(forward_pred, dtype=float)\n",
    "    rf_pred = np.asarray(rf_pred, dtype=float)\n",
    "    \n",
    "    denominator = forward_pred - rf_pred\n",
    "    denominator = np.where(\n",
    "        np.abs(denominator) < epsilon,\n",
    "        np.sign(denominator) * epsilon,\n",
    "        denominator\n",
    "    )\n",
    "    denominator = np.where(denominator == 0, epsilon, denominator)\n",
    "    \n",
    "    raw_position = (x - rf_pred) / denominator\n",
    "    return np.clip(raw_position, clip_min, clip_max)\n",
    "\n",
    "\n",
    "def _resolve_position_config(meta: Mapping[str, Any]) -> TwoHeadPositionConfig:\n",
    "    \"\"\"Extract position config from metadata.\"\"\"\n",
    "    x = float(meta.get(\"best_x\", 0.0))\n",
    "    return TwoHeadPositionConfig(x=x, clip_min=0.0, clip_max=2.0, epsilon=1e-8)\n",
    "\n",
    "\n",
    "def _hash_file(path: Path) -> str:\n",
    "    hasher = hashlib.sha256()\n",
    "    with path.open(\"rb\") as fh:\n",
    "        for chunk in iter(lambda: fh.read(8192), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "\n",
    "def _check_bundle_compat(meta: Mapping[str, Any]) -> None:\n",
    "    lib_versions = meta.get(\"library_versions\") if isinstance(meta, Mapping) else None\n",
    "    if isinstance(lib_versions, Mapping):\n",
    "        for lib_name, expected_version in lib_versions.items():\n",
    "            if not isinstance(lib_name, str):\n",
    "                continue\n",
    "            try:\n",
    "                module = importlib.import_module(lib_name)\n",
    "                actual_version = getattr(module, \"__version__\", None)\n",
    "            except Exception:\n",
    "                actual_version = None\n",
    "            if expected_version is None or actual_version is None:\n",
    "                print(\n",
    "                    f\"[warn] unable to verify version for {lib_name}: expected={expected_version} actual={actual_version}\"\n",
    "                )\n",
    "                continue\n",
    "            if str(actual_version) != str(expected_version):\n",
    "                print(\n",
    "                    f\"[warn] library version mismatch for {lib_name}: expected {expected_version}, running {actual_version}\"\n",
    "                )\n",
    "    for key in (\"config_hash\", \"config_digest\"):\n",
    "        expected_hash = meta.get(key)\n",
    "        if not isinstance(expected_hash, str):\n",
    "            continue\n",
    "        config_path_str = meta.get(\"config_path\")\n",
    "        if not isinstance(config_path_str, str):\n",
    "            continue\n",
    "        config_path = Path(config_path_str)\n",
    "        if not config_path.exists():\n",
    "            print(f\"[warn] unable to verify config hash; file not found: {config_path}\")\n",
    "            continue\n",
    "        actual_hash = _hash_file(config_path)\n",
    "        if actual_hash != expected_hash:\n",
    "            print(\n",
    "                f\"[warn] config hash mismatch: expected {expected_hash}, current {actual_hash} (path={config_path})\"\n",
    "            )\n",
    "    preprocess_hash = meta.get(\"preprocess_config_hash\")\n",
    "    preprocess_path_str = meta.get(\"preprocess_config_path\")\n",
    "    if isinstance(preprocess_hash, str) and isinstance(preprocess_path_str, str):\n",
    "        preprocess_path = Path(preprocess_path_str)\n",
    "        if preprocess_path.exists():\n",
    "            actual_pp_hash = _hash_file(preprocess_path)\n",
    "            if actual_pp_hash != preprocess_hash:\n",
    "                print(\n",
    "                    f\"[warn] preprocess config hash mismatch: expected {preprocess_hash}, current {actual_pp_hash}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\n",
    "                f\"[warn] unable to verify preprocess config hash; file not found: {preprocess_path}\"\n",
    "            )\n",
    "\n",
    "\n",
    "POSITION_CONFIG = _resolve_position_config(META)\n",
    "print(\"Two-head position config:\", POSITION_CONFIG)\n",
    "print(\"Hull Sharpe (CV):\", META.get(\"hull_sharpe\"))\n",
    "print(\"Best x:\", META.get(\"best_x\"))\n",
    "\n",
    "with FEATURE_LIST_PATH.open(\"r\", encoding=\"utf-8\") as fp:\n",
    "    FEATURE_MANIFEST: Dict[str, Any] = json.load(fp)\n",
    "PIPELINE_INPUT_COLUMNS = list(FEATURE_MANIFEST.get(\"pipeline_input_columns\") or [])\n",
    "if not PIPELINE_INPUT_COLUMNS:\n",
    "    raise ValueError(\"pipeline_input_columns missing in feature_list.json\")\n",
    "SU1_GENERATED_COLUMNS = FEATURE_MANIFEST.get(\"su1_generated_columns\", [])\n",
    "ID_COL = str(META.get(\"id_col\", \"date_id\"))\n",
    "TARGET_COL = str(META.get(\"target_col\", \"market_forward_excess_returns\"))\n",
    "DROP_NON_FEATURES = {col for col in (\"row_id\", TARGET_COL, \"is_scored\") if col and col != ID_COL}\n",
    "\n",
    "def _ensure_columns(frame: pd.DataFrame, columns: Sequence[str]) -> pd.DataFrame:\n",
    "    missing = [col for col in columns if col not in frame.columns]\n",
    "    if missing:\n",
    "        preview = \", \".join(str(col) for col in missing[:5])\n",
    "        print(f\"[info] adding {len(missing)} missing columns (preview: {preview})\")\n",
    "        for col in missing:\n",
    "            frame[col] = np.nan\n",
    "    return frame.reindex(columns=list(columns))\n",
    "\n",
    "def _coerce_numeric_like_columns(frame: pd.DataFrame) -> None:\n",
    "    object_cols = frame.select_dtypes(include=\"object\").columns\n",
    "    for col in object_cols:\n",
    "        converted = pd.to_numeric(frame[col], errors=\"coerce\")\n",
    "        if converted.notna().any() or frame[col].notna().sum() == 0:\n",
    "            frame[col] = converted\n",
    "\n",
    "def _extract_required_calendar_columns(meta: Mapping[str, Any]) -> set[str]:\n",
    "    required: set[str] = set()\n",
    "    imputer_meta = meta.get(\"imputer_metadata\")\n",
    "    if isinstance(imputer_meta, Mapping):\n",
    "        for info in imputer_meta.values():\n",
    "            if not isinstance(info, Mapping):\n",
    "                continue\n",
    "            calendar_value = info.get(\"calendar_column\")\n",
    "            if isinstance(calendar_value, str) and calendar_value.strip():\n",
    "                required.add(calendar_value.strip())\n",
    "    return required\n",
    "REQUIRED_CALENDAR_COLUMNS = _extract_required_calendar_columns(META)\n",
    "_check_bundle_compat(META)\n",
    "BUNDLE = joblib.load(BUNDLE_PATH)\n",
    "# Two-head bundle format: dict with forward_model, rf_model, augmenter, excluded_features, position_config\n",
    "if not isinstance(BUNDLE, dict):\n",
    "    raise ValueError(\"Two-head bundle must be a dict with forward_model and rf_model\")\n",
    "\n",
    "FORWARD_MODEL = BUNDLE.get(\"forward_model\")\n",
    "RF_MODEL = BUNDLE.get(\"rf_model\")\n",
    "AUGMENTER = BUNDLE.get(\"augmenter\")\n",
    "EXCLUDED_FEATURES: list[str] = BUNDLE.get(\"excluded_features\", [])\n",
    "BUNDLE_POSITION_CONFIG = BUNDLE.get(\"position_config\", {})\n",
    "\n",
    "if FORWARD_MODEL is None or RF_MODEL is None:\n",
    "    raise ValueError(\"Two-head bundle must contain both forward_model and rf_model\")\n",
    "\n",
    "# Update position config from bundle if available\n",
    "if BUNDLE_POSITION_CONFIG:\n",
    "    POSITION_CONFIG = TwoHeadPositionConfig(\n",
    "        x=float(BUNDLE_POSITION_CONFIG.get(\"x\", POSITION_CONFIG.x)),\n",
    "        clip_min=float(BUNDLE_POSITION_CONFIG.get(\"clip_min\", POSITION_CONFIG.clip_min)),\n",
    "        clip_max=float(BUNDLE_POSITION_CONFIG.get(\"clip_max\", POSITION_CONFIG.clip_max)),\n",
    "        epsilon=float(BUNDLE_POSITION_CONFIG.get(\"epsilon\", POSITION_CONFIG.epsilon)),\n",
    "    )\n",
    "    print(\"Updated position config from bundle:\", POSITION_CONFIG)\n",
    "\n",
    "print(\"Loaded two-head bundle:\", BUNDLE_PATH)\n",
    "print(f\"Forward model: {type(FORWARD_MODEL).__name__}\")\n",
    "print(f\"RF model: {type(RF_MODEL).__name__}\")\n",
    "print(f\"Augmenter: {type(AUGMENTER).__name__ if AUGMENTER else 'None'}\")\n",
    "print(f\"Excluded features: {len(EXCLUDED_FEATURES)}\")\n",
    "print(\"Pipeline input columns:\", len(PIPELINE_INPUT_COLUMNS))\n",
    "print(\"Total feature count:\", FEATURE_MANIFEST.get(\"total_feature_count\", len(PIPELINE_INPUT_COLUMNS)))\n",
    "DATA_DIR = Path(\"/kaggle/input/hull-tactical-market-prediction\")\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Competition dataset not mounted at /kaggle/input/hull-tactical-market-prediction\"\n",
    "    )\n",
    "\n",
    "def align_features(df: pd.DataFrame, *, sort_by_id: bool) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    working = df.reset_index(drop=True).copy()\n",
    "    drop_cols = [col for col in DROP_NON_FEATURES if col in working.columns]\n",
    "    if drop_cols:\n",
    "        working = working.drop(columns=drop_cols)\n",
    "    working[\"__original_order__\"] = np.arange(len(working))\n",
    "    if sort_by_id and ID_COL in working.columns:\n",
    "        working_sorted = working.sort_values(ID_COL).reset_index(drop=True)\n",
    "    else:\n",
    "        working_sorted = working\n",
    "    feature_frame = working_sorted.drop(columns=[\"__original_order__\"])\n",
    "    feature_frame = _ensure_columns(feature_frame, PIPELINE_INPUT_COLUMNS)\n",
    "    _coerce_numeric_like_columns(feature_frame)\n",
    "    return working_sorted, feature_frame\n",
    "\n",
    "def run_pipeline(X: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Run two-head pipeline and return positions.\n",
    "    \n",
    "    1. Apply augmenter (SU1 + SU5 feature generation)\n",
    "    2. Apply feature exclusion (tier3)\n",
    "    3. Predict forward_returns and risk_free_rate\n",
    "    4. Apply two-head formula to compute positions\n",
    "    \"\"\"\n",
    "    # Apply augmenter if present\n",
    "    if AUGMENTER is not None:\n",
    "        X_augmented = AUGMENTER.transform(X)\n",
    "        # Apply feature exclusion if specified\n",
    "        if EXCLUDED_FEATURES:\n",
    "            cols_to_drop = [c for c in X_augmented.columns if c in EXCLUDED_FEATURES]\n",
    "            X_augmented = X_augmented.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    else:\n",
    "        X_augmented = X\n",
    "    \n",
    "    # Predict with both heads\n",
    "    forward_pred = np.asarray(FORWARD_MODEL.predict(X_augmented), dtype=float).ravel()\n",
    "    rf_pred = np.asarray(RF_MODEL.predict(X_augmented), dtype=float).ravel()\n",
    "    \n",
    "    # Apply two-head formula: position = clip((x - rf) / (forward - rf), 0, 2)\n",
    "    positions = map_positions_from_forward_rf(\n",
    "        forward_pred=forward_pred,\n",
    "        rf_pred=rf_pred,\n",
    "        x=POSITION_CONFIG.x,\n",
    "        clip_min=POSITION_CONFIG.clip_min,\n",
    "        clip_max=POSITION_CONFIG.clip_max,\n",
    "        epsilon=POSITION_CONFIG.epsilon,\n",
    "    )\n",
    "    return positions\n",
    "\n",
    "def predict_bulk() -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if (DATA_DIR / \"test.parquet\").exists():\n",
    "        test_df = pd.read_parquet(DATA_DIR / \"test.parquet\")\n",
    "    else:\n",
    "        test_df = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "    missing_calendar_cols = [col for col in REQUIRED_CALENDAR_COLUMNS if col not in test_df.columns]\n",
    "    if missing_calendar_cols:\n",
    "        raise KeyError(\n",
    "            \"Missing calendar columns required by preprocessing: \"\n",
    "            + \", \".join(sorted(map(str, missing_calendar_cols)))\n",
    "        )\n",
    "    sorted_frame, X = align_features(test_df, sort_by_id=True)\n",
    "    positions = run_pipeline(X)  # Already in [0, 2] range from two-head formula\n",
    "    sorted_frame = sorted_frame.assign(\n",
    "        prediction=positions.astype(np.float32, copy=False),\n",
    "    )\n",
    "    sorted_frame = sorted_frame.sort_values(\"__original_order__\").reset_index(drop=True)\n",
    "    predictions = sorted_frame[\"prediction\"].to_numpy()\n",
    "    if \"is_scored\" not in test_df.columns:\n",
    "        raise KeyError(\"Expected 'is_scored' column in test data for submission filtering.\")\n",
    "    scored_mask = test_df[\"is_scored\"].to_numpy(dtype=bool)\n",
    "    scored_ids = test_df.loc[scored_mask, ID_COL].to_numpy()\n",
    "    scored_predictions = predictions[scored_mask]\n",
    "    if scored_ids.size != scored_predictions.size:\n",
    "        raise RuntimeError(\"Mismatch between scored ids and predictions lengths.\")\n",
    "    submission = pd.DataFrame(\n",
    "        {\n",
    "            ID_COL: scored_ids.astype(np.int64, copy=False),\n",
    "            \"prediction\": scored_predictions.astype(np.float32, copy=False),\n",
    "        }\n",
    "    )\n",
    "    submission = submission.sort_values(ID_COL).reset_index(drop=True)\n",
    "    if list(submission.columns) != [ID_COL, \"prediction\"]:\n",
    "        submission = submission[[ID_COL, \"prediction\"]]\n",
    "    if not np.isfinite(submission[\"prediction\"]).all():\n",
    "        raise ValueError(\"Submission contains non-finite predictions.\")\n",
    "    if not submission[ID_COL].is_unique:\n",
    "        raise ValueError(\"Submission date_id values must be unique.\")\n",
    "    expected_count = int(scored_mask.sum())\n",
    "    if len(submission) != expected_count:\n",
    "        raise ValueError(\n",
    "            f\"submission length {len(submission)} differs from scored rows {expected_count}\"\n",
    "        )\n",
    "    submission[ID_COL] = submission[ID_COL].astype(\"int64\", copy=False)\n",
    "    submission[\"prediction\"] = submission[\"prediction\"].astype(\"float32\", copy=False)\n",
    "    submission_path = Path(\"/kaggle/working/submission.csv\")\n",
    "    submission_parquet_path = submission_path.with_suffix(\".parquet\")\n",
    "    submission.to_parquet(submission_parquet_path, index=False)\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(\"Saved submission.csv:\", submission_path)\n",
    "    print(\"Saved submission.parquet:\", submission_parquet_path)\n",
    "    return test_df, submission\n",
    "\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"Single-row prediction for Kaggle inference server.\"\"\"\n",
    "    pdf = test.to_pandas()\n",
    "    _, X = align_features(pdf, sort_by_id=False)\n",
    "    positions = run_pipeline(X)  # Already in [0, 2] range\n",
    "    return float(np.asarray(positions, dtype=np.float32)[-1])\n",
    "TEST_DF, SUBMISSION_DF = predict_bulk()\n",
    "print(\"submission preview:\")\n",
    "print(SUBMISSION_DF.head())\n",
    "inference_server = kies.DefaultInferenceServer(predict)\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway((\"/kaggle/input/hull-tactical-market-prediction/\",))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}